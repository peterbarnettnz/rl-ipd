{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "88039f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "from gym import spaces\n",
    "\n",
    "import players\n",
    "import envs\n",
    "from evaluation import generate_history, is_t4t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "372561dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 17:28:56,436\tINFO worker.py:745 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.examples.env.multi_agent import MultiAgentCartPole\n",
    "from ray.rllib.examples.models.shared_weights_model import \\\n",
    "    SharedWeightsModel1, SharedWeightsModel2, TF2SharedWeightsModel, \\\n",
    "    TorchSharedWeightsModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "# from ray.rllib.policy import PolicySpec\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, PPOTorchPolicy\n",
    "from ray.rllib.agents.ppo import DEFAULT_CONFIG as DEFAULT_CONFIG_PPO\n",
    "\n",
    "from ray.rllib.agents.dqn import DQNTrainer, DQNTorchPolicy\n",
    "from ray.rllib.agents.dqn import  DEFAULT_CONFIG as DEFAULT_CONFIG_DQN\n",
    "\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6c1691b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env('two_agent_MG_env', lambda c: envs.TwoAgentMatrixGameEnv(c))\n",
    "register_env('MG_t4tTD_env', lambda c: envs.MatrixGameEnv(player2=players.TitForTatThenDefect()))\n",
    "# register_env('MG_t4t_env', lambda c: envs.MatrixGameEnv(player2=players.TitForTat()))\n",
    "register_env('MG_t4t_env', lambda c: envs.MatrixGameEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9b584d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_dqn0 = DEFAULT_CONFIG_DQN.copy()\n",
    "# trainer_config_dqn0['num_workers'] = 3\n",
    "trainer_config_dqn0['n_step'] = 3\n",
    "trainer_config_dqn0['noisy'] = True\n",
    "trainer_config_dqn0['v_min'] = -10.\n",
    "trainer_config_dqn0['v_max'] = 10.\n",
    "trainer_config_dqn0['model']['fcnet_hiddens'] = [256,32,8]\n",
    "trainer_config_dqn0['framework'] = 'torch'\n",
    "\n",
    "trainer_config_dqn0['env'] = 'MG_t4t_env'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bf39b94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/2.72 GiB heap, 0.0/1.36 GiB objects<br>Result logdir: /home/peter/ray_results/test_pretrain<br>Number of trials: 1/1 (1 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 17:29:06,949\tINFO tune.py:549 -- Total run time: 9.29 seconds (9.04 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# ray.init(ignore_reinit_error=True)\n",
    "results = tune.run(\n",
    "    \"DQN\",\n",
    "    stop={\"training_iteration\":2},\n",
    "    name='test_pretrain',\n",
    "    config=trainer_config_dqn0,\n",
    "#     config={\n",
    "# #         \"num_gpus\": 0,\n",
    "# #         \"num_workers\": 1,\n",
    "# #         \"lr\": tune.grid_search([0.01, 0.001, 0.0001]),\n",
    "#         \"config\":trainer_config_ppo\n",
    "#     },\n",
    "    verbose=1,\n",
    "    checkpoint_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3b9f2e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment_id': 'b8c51c90ee0642ce8c0ddc8510d0bbf1',\n",
       " 'iteration': 2,\n",
       " 'timesteps_total': None,\n",
       " 'time_total': 7.438932657241821,\n",
       " 'episodes_total': 20,\n",
       " 'ray_version': '1.4.1'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent0.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de20a710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:52:13,743\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-08-10 16:52:13,840\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "agent0 = DQNTrainer(config=trainer_config_dqn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "334b10b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:52:16,882\tINFO trainable.py:377 -- Restored on 192.168.1.21 from checkpoint: /home/peter/ray_results/test_pretrain/DQN_MG_t4t_env_b0c4e_00000_0_2021-08-10_16-51-57/checkpoint_000002/checkpoint-2\n",
      "2021-08-10 16:52:16,883\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 7.438932657241821, '_episodes_total': 20}\n"
     ]
    }
   ],
   "source": [
    "agent0.restore(results.get_last_checkpoint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429352e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent0.get_policy().get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf8267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, stop_criteria):\n",
    "    \"\"\"\n",
    "    Train an RLlib PPO agent using tune until any of the configured stopping criteria is met.\n",
    "    :param stop_criteria: Dict with stopping criteria.\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run\n",
    "    :return: Return the path to the saved agent (checkpoint) and tune's ExperimentAnalysis object\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/analysis.html#experimentanalysis-tune-experimentanalysis\n",
    "    \"\"\"\n",
    "    analysis = ray.tune.run(ppo.PPOTrainer, config=self.config, local_dir=self.save_dir, stop=stop_criteria,\n",
    "                            checkpoint_at_end=True)\n",
    "    # list of lists: one list per checkpoint; each checkpoint list contains 1st the path, 2nd the metric value\n",
    "    checkpoints = analysis.get_trial_checkpoints_paths(trial=analysis.get_best_trial('episode_reward_mean'),\n",
    "                                                       metric='episode_reward_mean')\n",
    "    # retriev the checkpoint path; we only have a single checkpoint, so take the first one\n",
    "    checkpoint_path = checkpoints[0][0]\n",
    "    return checkpoint_path, analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "40d76276",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_dqn0 = DEFAULT_CONFIG_DQN.copy()\n",
    "# trainer_config_dqn0['num_workers'] = 3\n",
    "trainer_config_dqn0['n_step'] = 3\n",
    "trainer_config_dqn0['noisy'] = True\n",
    "trainer_config_dqn0['v_min'] = -10.\n",
    "trainer_config_dqn0['v_max'] = 10.\n",
    "trainer_config_dqn0['model']['fcnet_hiddens'] = [256,32,8]\n",
    "trainer_config_dqn1['framework'] = 'torch'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "52ac55d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_dqn1 = DEFAULT_CONFIG_DQN.copy()\n",
    "# trainer_config_dqn1['num_workers'] = 3\n",
    "trainer_config_dqn1['n_step'] = 3\n",
    "trainer_config_dqn1['noisy'] = True\n",
    "trainer_config_dqn1['v_min'] = -10.\n",
    "trainer_config_dqn1['v_max'] = 10.\n",
    "trainer_config_dqn1['model']['fcnet_hiddens'] = [256,32,8]\n",
    "trainer_config_dqn1['framework'] = 'torch'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c2705616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0faacb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def env_creator(_):\n",
    "    return envs.TwoAgentMatrixGameEnv()\n",
    "single_env = envs.TwoAgentMatrixGameEnv()\n",
    "env_name = \"two_agent_MG_env\"\n",
    "register_env(env_name, env_creator)\n",
    "\n",
    "\n",
    "obs_space = single_env.observation_space\n",
    "act_space = single_env.action_space\n",
    "num_agents = single_env.num_agents\n",
    "# DQNTorchPolicy0 = DQNTorchPolicy.copy()\n",
    "\n",
    "def gen_policy(i):\n",
    "#     return (PPOTorchPolicy, obs_space, act_space, {})\n",
    "    if i == 0:\n",
    "        conf = trainer_config_dqn0\n",
    "#         return (DQNTorchPolicy0, obs_space, act_space, conf)\n",
    "#         policy0 = copy.copy(agent0.get_policy())\n",
    "#         policy0 = DQNTorchPolicy(obs_space, act_space, conf)\n",
    "        policy0 = DQNTorchPolicy\n",
    "#         policy0.set_weights(agent0.get_weights()['default_policy'])\n",
    "#         policy0 = lambda c: policy0\n",
    "        return (policy0, obs_space, act_space, conf)\n",
    "\n",
    "    elif i == 1:\n",
    "        conf = trainer_config_dqn1\n",
    "        return (DQNTorchPolicy, obs_space, act_space, conf)\n",
    "    else:\n",
    "        print('NO CONF!')\n",
    "    \n",
    "    return (DQNTorchPolicy, obs_space, act_space, conf)\n",
    "\n",
    "policy_graphs = {}\n",
    "\n",
    "\n",
    "\n",
    "for i in range(num_agents):\n",
    "    policy_graphs['agent-' + str(i)] = gen_policy(i)\n",
    "def policy_mapping_fn(agent_id):\n",
    "        return 'agent-' + str(agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f8fcc793",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_dqn1['model']['fcnet_hiddens'] = [3]\n",
    "policy0 = DQNTorchPolicy(obs_space, act_space, trainer_config_dqn0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "14e24547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment_id': 'b8c51c90ee0642ce8c0ddc8510d0bbf1',\n",
       " 'iteration': 2,\n",
       " 'timesteps_total': None,\n",
       " 'time_total': 7.438932657241821,\n",
       " 'episodes_total': 20,\n",
       " 'ray_version': '1.4.1'}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent0.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5b80654e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ray.rllib.policy.policy_template.DQNTorchPolicy,\n",
       " Box(0.0, 1.0, (400,), float32),\n",
       " Discrete(2),\n",
       " {'num_workers': 0,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'create_env_on_driver': False,\n",
       "  'rollout_fragment_length': 4,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'train_batch_size': 32,\n",
       "  'model': {'_use_default_native_models': False,\n",
       "   'fcnet_hiddens': [3],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': True,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'num_framestacks': 0,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'lstm_use_prev_action_reward': -1,\n",
       "   'framestack': True},\n",
       "  'optimizer': {},\n",
       "  'gamma': 0.99,\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env': 'MG_t4t_env',\n",
       "  'env_config': {},\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'record_env': False,\n",
       "  'normalize_actions': False,\n",
       "  'clip_rewards': None,\n",
       "  'clip_actions': True,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'lr': 0.0005,\n",
       "  'log_level': 'WARN',\n",
       "  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'framework': 'torch',\n",
       "  'eager_tracing': False,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'EpsilonGreedy',\n",
       "   'initial_epsilon': 1.0,\n",
       "   'final_epsilon': 0.02,\n",
       "   'epsilon_timesteps': 10000},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_num_episodes': 10,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {'explore': False},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'sample_async': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'compress_observations': False,\n",
       "  'collect_metrics_timeout': 180,\n",
       "  'metrics_smoothing_episodes': 100,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'min_iter_time_s': 1,\n",
       "  'timesteps_per_iteration': 1000,\n",
       "  'seed': None,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'placement_strategy': 'PACK',\n",
       "  'input': 'sampler',\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'multiagent': {'policies': {},\n",
       "   'policy_mapping_fn': None,\n",
       "   'policies_to_train': None,\n",
       "   'observation_fn': None,\n",
       "   'replay_mode': 'independent',\n",
       "   'count_steps_by': 'env_steps'},\n",
       "  'logger_config': None,\n",
       "  'simple_optimizer': -1,\n",
       "  'monitor': -1,\n",
       "  'num_atoms': 1,\n",
       "  'v_min': -10.0,\n",
       "  'v_max': 10.0,\n",
       "  'noisy': True,\n",
       "  'sigma0': 0.5,\n",
       "  'dueling': True,\n",
       "  'hiddens': [256],\n",
       "  'double_q': True,\n",
       "  'n_step': 3,\n",
       "  'target_network_update_freq': 500,\n",
       "  'buffer_size': 50000,\n",
       "  'replay_sequence_length': 1,\n",
       "  'prioritized_replay': True,\n",
       "  'prioritized_replay_alpha': 0.6,\n",
       "  'prioritized_replay_beta': 0.4,\n",
       "  'final_prioritized_replay_beta': 0.4,\n",
       "  'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       "  'prioritized_replay_eps': 1e-06,\n",
       "  'before_learn_on_batch': None,\n",
       "  'training_intensity': None,\n",
       "  'lr_schedule': None,\n",
       "  'adam_epsilon': 1e-08,\n",
       "  'grad_clip': 40,\n",
       "  'learning_starts': 1000,\n",
       "  'worker_side_prioritization': False})"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_graphs['agent-0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "07fb38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"log_level\": \"WARN\",\n",
    "#     \"num_workers\": 3,\n",
    "#     \"num_cpus_for_driver\": 1,\n",
    "#     \"num_cpus_per_worker\": 1,\n",
    "#     \"lr\": 5e-3,\n",
    "#     \"model\":{\"fcnet_hiddens\": [1024, 512,256,32,8]},\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policy_graphs,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "    },\n",
    "    \"env\": \"two_agent_MG_env\",\n",
    "    'framework': 'torch'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fa11f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'TA_TEST2'\n",
    "exp_dict = {\n",
    "        'name': exp_name,\n",
    "        'run_or_experiment': 'DQN',\n",
    "        \"stop\": {\n",
    "            \"training_iteration\": 200\n",
    "        },\n",
    "        'checkpoint_freq': 0,\n",
    "        'checkpoint_at_end': True,\n",
    "        \"config\": config,\n",
    "}\n",
    "# ray.init()\n",
    "tune.run(**exp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dd64c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy0 = DQNTorchPolicy(obs_space, act_space, trainer_config_dqn0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4a7cd6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy0.set_weights(agent0.get_weights()['default_policy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6481cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
