{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88039f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/.local/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "from gym import spaces\n",
    "\n",
    "import players\n",
    "import envs\n",
    "from evaluation import generate_history, is_t4t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "372561dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-12 14:57:23,598\tINFO services.py:1272 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.21',\n",
       " 'raylet_ip_address': '192.168.1.21',\n",
       " 'redis_address': '192.168.1.21:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-08-12_14-57-22_149583_113661/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-08-12_14-57-22_149583_113661/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-08-12_14-57-22_149583_113661',\n",
       " 'metrics_export_port': 50589,\n",
       " 'node_id': '8ee327bc8a28be62a453be14507154a8a803f94a3fd98defd6be4d33'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.examples.env.multi_agent import MultiAgentCartPole\n",
    "from ray.rllib.examples.models.shared_weights_model import \\\n",
    "    SharedWeightsModel1, SharedWeightsModel2, TF2SharedWeightsModel, \\\n",
    "    TorchSharedWeightsModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "# from ray.rllib.policy import PolicySpec\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, PPOTorchPolicy, PPOTFPolicy\n",
    "from ray.rllib.agents.ppo import DEFAULT_CONFIG as DEFAULT_CONFIG_PPO\n",
    "\n",
    "from ray.rllib.agents.dqn import DQNTrainer, DQNTorchPolicy, DQNTFPolicy\n",
    "from ray.rllib.agents.dqn import  DEFAULT_CONFIG as DEFAULT_CONFIG_DQN\n",
    "\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c1691b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env('two_agent_MG_env', lambda c: envs.TwoAgentMatrixGameEnv(c))\n",
    "\n",
    "register_env('two_agent_t4t_MG_env', lambda c: envs.TwoAgentSeparateMatrixGameEnv(c))\n",
    "\n",
    "register_env('MG_t4tTD_env', lambda c: envs.MatrixGameEnv(player2=players.TitForTatThenDefect()))\n",
    "# register_env('MG_t4t_env', lambda c: envs.MatrixGameEnv(player2=players.TitForTat()))\n",
    "register_env('MG_t4t_env', lambda c: envs.MatrixGameEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9b584d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_dqn0 = DEFAULT_CONFIG_DQN.copy()\n",
    "# trainer_config_dqn0['num_workers'] = 3\n",
    "trainer_config_dqn0['n_step'] = 3\n",
    "trainer_config_dqn0['noisy'] = True\n",
    "trainer_config_dqn0['v_min'] = -10.\n",
    "trainer_config_dqn0['v_max'] = 10.\n",
    "trainer_config_dqn0['model']['fcnet_hiddens'] = [256,32,8]\n",
    "trainer_config_dqn0['framework'] = 'torch'\n",
    "# trainer_config_dqn0['framework'] = 'tf'\n",
    "\n",
    "trainer_config_dqn0['env'] = 'MG_t4t_env'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bf39b94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 6.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/6.55 GiB heap, 0.0/3.27 GiB objects<br>Result logdir: /home/peter/ray_results/test_pretrain<br>Number of trials: 1/1 (1 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-12 12:14:05,017\tINFO tune.py:549 -- Total run time: 8.56 seconds (8.06 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# ray.init(ignore_reinit_error=True)\n",
    "results = tune.run(\n",
    "    \"DQN\",\n",
    "    stop={\"training_iteration\":2},\n",
    "    name='test_pretrain',\n",
    "    config=trainer_config_dqn0,\n",
    "#     config={\n",
    "# #         \"num_gpus\": 0,\n",
    "# #         \"num_workers\": 1,\n",
    "# #         \"lr\": tune.grid_search([0.01, 0.001, 0.0001]),\n",
    "#         \"config\":trainer_config_ppo\n",
    "#     },\n",
    "    verbose=1,\n",
    "    checkpoint_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "de20a710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-12 12:14:07,953\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "agent0 = DQNTrainer(config=trainer_config_dqn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "334b10b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-12 12:14:08,111\tINFO trainable.py:377 -- Restored on 192.168.1.21 from checkpoint: /home/peter/ray_results/test_pretrain/DQN_MG_t4t_env_2f287_00000_0_2021-08-12_12-13-56/checkpoint_000002/checkpoint-2\n",
      "2021-08-12 12:14:08,113\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 5.200430631637573, '_episodes_total': 20}\n"
     ]
    }
   ],
   "source": [
    "agent0.restore(results.get_last_checkpoint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2c4066fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/peter/ray_results/test_pretrain/DQN_MG_t4t_env_2f287_00000_0_2021-08-12_12-13-56/checkpoint_000002/checkpoint-2'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.get_last_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f429352e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_hidden_layers.0._model.0.weight': array([[ 0.04890376, -0.00558209,  0.01287127, ..., -0.07449174,\n",
       "         -0.00684128, -0.03332094],\n",
       "        [-0.07871381,  0.03723612, -0.01275583, ..., -0.03392028,\n",
       "         -0.11493096, -0.03749057],\n",
       "        [-0.01244541,  0.02239128, -0.00389348, ..., -0.01874868,\n",
       "         -0.0041142 , -0.02274424],\n",
       "        ...,\n",
       "        [-0.05733058,  0.01309578, -0.10551378, ..., -0.0299821 ,\n",
       "         -0.01164537, -0.0390831 ],\n",
       "        [ 0.03285411, -0.00725919,  0.06943528, ..., -0.00349141,\n",
       "         -0.02176144,  0.17167993],\n",
       "        [-0.0195437 , -0.051215  ,  0.0139996 , ..., -0.01509678,\n",
       "         -0.0730854 , -0.04187715]], dtype=float32),\n",
       " '_hidden_layers.0._model.0.bias': array([ 0.02052973,  0.00589235,  0.00153296, -0.02188277,  0.04248617,\n",
       "        -0.01027183, -0.0207677 , -0.00118991, -0.02367345, -0.01769985,\n",
       "        -0.01216752,  0.00134712,  0.03695444, -0.02303964,  0.02971013,\n",
       "         0.00341944, -0.00409666,  0.03229688, -0.0179458 ,  0.01861962,\n",
       "         0.03506381, -0.02170533, -0.00677455,  0.02239835,  0.01765032,\n",
       "         0.02403176,  0.00974598, -0.0149071 ,  0.02548569, -0.01588533,\n",
       "        -0.02305359,  0.00214301, -0.03456329, -0.00707483,  0.03153357,\n",
       "        -0.02699989, -0.0008681 ,  0.02545101, -0.00201799,  0.03363357,\n",
       "        -0.00759832, -0.01080264, -0.03039746,  0.03023081, -0.02260409,\n",
       "         0.02723953,  0.00731938,  0.01953933, -0.02614036, -0.0314283 ,\n",
       "        -0.00623304, -0.03458372,  0.01410635,  0.01792203,  0.01768667,\n",
       "        -0.02979833,  0.03159744, -0.03228466,  0.01474291, -0.01968233,\n",
       "         0.00756408, -0.01985792,  0.03206934,  0.01324647, -0.00454111,\n",
       "         0.02629266, -0.01085804, -0.01797189,  0.02066143, -0.01340234,\n",
       "        -0.02157016,  0.01680673, -0.00476155, -0.02040781, -0.04307198,\n",
       "        -0.02783334,  0.00815925, -0.00327614, -0.02642166, -0.0161976 ,\n",
       "         0.02915881, -0.00854166,  0.0073722 ,  0.0149712 , -0.02454765,\n",
       "         0.02468698,  0.00020779,  0.01734531,  0.01947275, -0.02071202,\n",
       "        -0.00672228, -0.01502968,  0.0131521 , -0.00303584,  0.03524546,\n",
       "        -0.02145712,  0.03052085,  0.01723795,  0.02437894,  0.01790596,\n",
       "         0.02040855, -0.00362763,  0.02423355, -0.00711354,  0.01082732,\n",
       "        -0.00633428,  0.02430526, -0.01342094, -0.01254185, -0.009961  ,\n",
       "         0.00491665,  0.02939369,  0.02688636,  0.02969124, -0.01918382,\n",
       "        -0.02884066,  0.01401441, -0.00170469, -0.0269725 ,  0.00569717,\n",
       "         0.00472233,  0.01192234, -0.01247657,  0.04011726, -0.00190098,\n",
       "        -0.02809516, -0.0040752 , -0.000566  ,  0.00297868,  0.01842348,\n",
       "         0.01015464, -0.01871319, -0.01610675, -0.01911479,  0.01887531,\n",
       "         0.02261558, -0.00825374,  0.00603714, -0.02928375, -0.01481386,\n",
       "         0.00967977,  0.02995668, -0.00545447, -0.02567384, -0.03129587,\n",
       "        -0.0267162 , -0.02677382,  0.0127033 ,  0.03047964,  0.01103421,\n",
       "         0.0111445 , -0.00634328,  0.02183776, -0.02172588, -0.0052032 ,\n",
       "         0.03242305, -0.00234969,  0.03517637,  0.02089844, -0.02429423,\n",
       "        -0.02575981, -0.01666231,  0.00893178, -0.01077855, -0.01154441,\n",
       "        -0.0139743 ,  0.00111166,  0.00827442,  0.01887088, -0.01822953,\n",
       "        -0.01209686,  0.01066969,  0.00011494, -0.02007419,  0.01537027,\n",
       "        -0.00278592, -0.01667609,  0.02889458, -0.02874419, -0.01922096,\n",
       "        -0.01092735, -0.00753054,  0.03277298, -0.03155687,  0.01655359,\n",
       "        -0.00235409,  0.02046199, -0.0222816 ,  0.00173122,  0.00092213,\n",
       "        -0.01544353, -0.02043633, -0.01974853,  0.02403229,  0.02614719,\n",
       "         0.00939223, -0.02164176, -0.00442015, -0.03331094,  0.011976  ,\n",
       "         0.00743539,  0.02793857,  0.02117058, -0.00412293, -0.00352107,\n",
       "         0.02184861, -0.00786477,  0.02144958,  0.02758171,  0.02661756,\n",
       "        -0.00129872,  0.01661186,  0.00573516, -0.01663768, -0.02245006,\n",
       "        -0.02582175, -0.02525391,  0.01998046, -0.01844609,  0.00709344,\n",
       "         0.03206794,  0.00597691, -0.02611275,  0.02800522, -0.01662892,\n",
       "         0.02751839,  0.02140821, -0.00330567,  0.01001121,  0.0009177 ,\n",
       "         0.03107261,  0.02725987, -0.02224852,  0.00192184, -0.00298021,\n",
       "        -0.00034441,  0.01793236, -0.01982616,  0.00342933, -0.01673325,\n",
       "        -0.01293237, -0.01855325, -0.03600231, -0.00626462,  0.02827457,\n",
       "        -0.00339996,  0.0287412 ,  0.02564957, -0.02301035,  0.005365  ,\n",
       "         0.02474268,  0.01291542, -0.01467123,  0.01858461,  0.02563862,\n",
       "         0.01516672], dtype=float32),\n",
       " '_hidden_layers.1._model.0.weight': array([[ 0.10409377,  0.02117277,  0.09679156, ...,  0.16215305,\n",
       "          0.11420508,  0.03135535],\n",
       "        [ 0.04808111, -0.01536947, -0.01344304, ...,  0.04539439,\n",
       "          0.03281635, -0.082572  ],\n",
       "        [-0.04365534, -0.10596761,  0.06076981, ..., -0.10614017,\n",
       "         -0.07086757,  0.01375408],\n",
       "        ...,\n",
       "        [ 0.02800373, -0.00489485, -0.03533736, ...,  0.03095819,\n",
       "         -0.04571285,  0.0690527 ],\n",
       "        [ 0.04690844, -0.06891369,  0.04463483, ...,  0.01573198,\n",
       "         -0.01656282, -0.07379875],\n",
       "        [-0.06034461,  0.1056838 ,  0.15139282, ..., -0.05492966,\n",
       "          0.00520892, -0.03183815]], dtype=float32),\n",
       " '_hidden_layers.1._model.0.bias': array([ 0.01485413,  0.01363737, -0.00033576,  0.02982499,  0.01492514,\n",
       "         0.00846242, -0.02462595,  0.01656981, -0.01576598, -0.01251944,\n",
       "        -0.00196092, -0.02712908, -0.0025533 , -0.02189682, -0.01666064,\n",
       "         0.001703  ,  0.01571492,  0.01696566, -0.01101146, -0.02405705,\n",
       "         0.02090551,  0.01034829,  0.01060915, -0.01562172, -0.01778388,\n",
       "        -0.0222785 , -0.01768386, -0.00963381, -0.02622214,  0.00467726,\n",
       "         0.01828884,  0.01813258], dtype=float32),\n",
       " '_hidden_layers.2._model.0.weight': array([[ 0.07255462, -0.16590767, -0.06650951,  0.0467401 ,  0.09275027,\n",
       "         -0.18488604,  0.02792497,  0.27383268, -0.24472706, -0.11855014,\n",
       "         -0.12973477,  0.12839632, -0.00994899, -0.11906866, -0.40314293,\n",
       "          0.20841359, -0.0115771 , -0.03653247, -0.10602832, -0.20200804,\n",
       "          0.01499094, -0.19675566,  0.20169812,  0.08792493, -0.20799755,\n",
       "         -0.11930178,  0.143661  ,  0.24247816, -0.25907022,  0.23694383,\n",
       "         -0.18851857,  0.315718  ],\n",
       "        [ 0.25500014,  0.04255449, -0.21709299, -0.03318493, -0.05400387,\n",
       "         -0.02536392, -0.19500913,  0.16044532,  0.14299285, -0.00556089,\n",
       "         -0.05786914, -0.45294794,  0.04904446, -0.14293066,  0.14915526,\n",
       "         -0.1390924 ,  0.1148208 , -0.01722402,  0.19549775, -0.32566696,\n",
       "          0.2673933 ,  0.28901422, -0.1431559 , -0.09291583, -0.11429109,\n",
       "         -0.10349569,  0.37212688, -0.08684225, -0.12133465,  0.16181447,\n",
       "          0.05954955,  0.08499782],\n",
       "        [ 0.25931895, -0.03298737,  0.23328944,  0.44885698, -0.03995378,\n",
       "          0.32512394, -0.09809592,  0.17388815, -0.06018373, -0.14333564,\n",
       "          0.0423746 , -0.26328573,  0.15324277, -0.11637024, -0.01237811,\n",
       "         -0.1429243 ,  0.11486456,  0.19080049, -0.3562665 ,  0.01694694,\n",
       "         -0.11057106, -0.00186623, -0.00967029, -0.19480391, -0.3121819 ,\n",
       "         -0.17388606, -0.0051513 ,  0.09623106, -0.06073672, -0.01512279,\n",
       "          0.11444197, -0.18333593],\n",
       "        [-0.03255503,  0.01707797, -0.12919344,  0.03974972,  0.04329725,\n",
       "          0.32739297,  0.02958426, -0.1217528 , -0.3618068 , -0.1995514 ,\n",
       "         -0.06913884, -0.325524  , -0.24448408,  0.01945183,  0.20368339,\n",
       "          0.24962206, -0.14579949,  0.38228798,  0.00674725,  0.1916307 ,\n",
       "         -0.01962836, -0.18685444,  0.01212542, -0.14448601, -0.03026056,\n",
       "         -0.0863229 , -0.32767323, -0.08559774, -0.09461487, -0.05292746,\n",
       "          0.14034107,  0.1411095 ],\n",
       "        [-0.10186353,  0.23576738, -0.13265163,  0.2973971 ,  0.0716656 ,\n",
       "         -0.20175867,  0.00989415, -0.160478  , -0.01357495, -0.02150247,\n",
       "         -0.25179932, -0.14549598,  0.38807714, -0.2731763 ,  0.09850376,\n",
       "         -0.26238954, -0.02823836,  0.1729915 , -0.10491985,  0.0587838 ,\n",
       "          0.18202431,  0.27084306, -0.1736394 , -0.0005093 ,  0.11996531,\n",
       "         -0.08570904, -0.19759509, -0.17912109,  0.0179514 ,  0.21822652,\n",
       "          0.09549138, -0.2105771 ],\n",
       "        [ 0.16758491, -0.05248798,  0.09950393, -0.00797627,  0.40866607,\n",
       "         -0.16482444, -0.19560269,  0.06513894,  0.25955704, -0.03133976,\n",
       "          0.26565027,  0.01767511, -0.33227232,  0.0881965 , -0.34342864,\n",
       "          0.15822893,  0.12632892,  0.13157044,  0.08165096, -0.13135886,\n",
       "          0.1547204 ,  0.19815955,  0.207617  , -0.2422165 ,  0.04407857,\n",
       "         -0.1201624 , -0.05013742, -0.06050981, -0.03123945, -0.29402322,\n",
       "          0.00766855, -0.10858095],\n",
       "        [-0.25930542,  0.41469607, -0.00550724,  0.22968878, -0.04049763,\n",
       "          0.22169507, -0.19364735, -0.09315629, -0.17834888,  0.08759729,\n",
       "          0.16365547, -0.13146642, -0.05851791,  0.03718768, -0.13625695,\n",
       "         -0.1597141 ,  0.09046221, -0.26532298, -0.02345167, -0.05952808,\n",
       "          0.28516954,  0.19720595,  0.23459338,  0.09942888, -0.05939079,\n",
       "          0.08737489, -0.26919198, -0.03903911, -0.2619906 ,  0.15816249,\n",
       "          0.04152898,  0.26919866],\n",
       "        [ 0.11533669, -0.0224929 ,  0.34577566, -0.20200711, -0.06369222,\n",
       "          0.10465196, -0.07612184, -0.04173649,  0.01528845,  0.38881716,\n",
       "         -0.01901597,  0.30313155,  0.06603062,  0.01430656,  0.02552048,\n",
       "         -0.02003981, -0.07674456, -0.09028816,  0.3262583 ,  0.09117191,\n",
       "         -0.06464868, -0.155301  , -0.3255412 ,  0.07937011, -0.04356874,\n",
       "          0.01980551, -0.03838844, -0.18574506,  0.07147357, -0.38986665,\n",
       "         -0.01010632,  0.21116526]], dtype=float32),\n",
       " '_hidden_layers.2._model.0.bias': array([0.01806764, 0.0152277 , 0.01609238, 0.0160063 , 0.01535865,\n",
       "        0.01412854, 0.01941656, 0.01205911], dtype=float32),\n",
       " '_value_branch._model.0.weight': array([[-0.00040089,  0.00122378,  0.00581361,  0.00285687, -0.00454617,\n",
       "          0.00209565,  0.00236446, -0.00507265]], dtype=float32),\n",
       " '_value_branch._model.0.bias': array([0.], dtype=float32),\n",
       " 'advantage_module.dueling_A_0.sigma_w': array([[ 0.32567823, -0.22377267,  0.31942773, ..., -0.34104368,\n",
       "         -0.06339259,  0.19830285],\n",
       "        [ 0.12418847, -0.08500414,  0.31094608, ..., -0.2896157 ,\n",
       "         -0.03471352, -0.06137766],\n",
       "        [-0.1018812 , -0.15759775,  0.15633081, ..., -0.00471529,\n",
       "          0.31128466, -0.18493296],\n",
       "        ...,\n",
       "        [ 0.03673758, -0.06614786,  0.32745904, ..., -0.17644104,\n",
       "          0.18513803,  0.2685394 ],\n",
       "        [ 0.15754335,  0.26110667,  0.17345718, ...,  0.10296805,\n",
       "          0.28884503,  0.3127378 ],\n",
       "        [ 0.1285754 ,  0.13113587, -0.23232403, ..., -0.11196075,\n",
       "         -0.24840762, -0.1269191 ]], dtype=float32),\n",
       " 'advantage_module.dueling_A_0.sigma_b': array([0.17343435, 0.17497738, 0.16653208, 0.18421237, 0.19358997,\n",
       "        0.16829307, 0.18705472, 0.18330945, 0.18695228, 0.16964461,\n",
       "        0.18586403, 0.16994476, 0.16703956, 0.17552222, 0.17536023,\n",
       "        0.17454883, 0.1761654 , 0.18250747, 0.18146524, 0.18111193,\n",
       "        0.19050556, 0.17826971, 0.188175  , 0.17767236, 0.17248866,\n",
       "        0.17832033, 0.17471059, 0.17247169, 0.17859426, 0.18454595,\n",
       "        0.17762756, 0.17248201, 0.17982312, 0.17139967, 0.17895661,\n",
       "        0.16784976, 0.1784014 , 0.17040494, 0.17154787, 0.16813062,\n",
       "        0.17259917, 0.17370792, 0.19039781, 0.17376973, 0.17320849,\n",
       "        0.16558263, 0.18165293, 0.18307298, 0.18146774, 0.17054741,\n",
       "        0.18542229, 0.17222774, 0.18127337, 0.17507671, 0.1749847 ,\n",
       "        0.16605857, 0.16610442, 0.1762995 , 0.16223355, 0.18925913,\n",
       "        0.1683341 , 0.17785387, 0.16973911, 0.1809893 , 0.17161548,\n",
       "        0.16596046, 0.17107038, 0.1695599 , 0.16250096, 0.17562488,\n",
       "        0.18871467, 0.17426257, 0.17182575, 0.17058621, 0.17931795,\n",
       "        0.17740622, 0.17899129, 0.1714828 , 0.17225498, 0.17082487,\n",
       "        0.17915973, 0.17970757, 0.181835  , 0.17250651, 0.18350334,\n",
       "        0.17756741, 0.17313063, 0.16859834, 0.16867343, 0.17954803,\n",
       "        0.17407566, 0.18194097, 0.17364669, 0.18109865, 0.17513521,\n",
       "        0.17587325, 0.17352001, 0.1903022 , 0.18133768, 0.16250703,\n",
       "        0.17612234, 0.17811027, 0.17086948, 0.1771591 , 0.18318413,\n",
       "        0.17572123, 0.17303267, 0.17895621, 0.17016801, 0.16955847,\n",
       "        0.17984954, 0.1856098 , 0.18006599, 0.17123254, 0.18237907,\n",
       "        0.18051761, 0.19446006, 0.18302965, 0.17513442, 0.17451636,\n",
       "        0.17887758, 0.16979662, 0.1764519 , 0.17834947, 0.18523537,\n",
       "        0.18889207, 0.17493665, 0.17782933, 0.17912737, 0.18089278,\n",
       "        0.17573072, 0.18481898, 0.16794078, 0.17155887, 0.18198061,\n",
       "        0.17736539, 0.17324746, 0.16704534, 0.17659904, 0.16835833,\n",
       "        0.17325506, 0.18022908, 0.17611286, 0.18698215, 0.17560636,\n",
       "        0.17208235, 0.16371396, 0.16983089, 0.17185465, 0.1754701 ,\n",
       "        0.17301449, 0.17220533, 0.16309142, 0.17639527, 0.18637742,\n",
       "        0.16990724, 0.1694282 , 0.18985435, 0.18148476, 0.17388761,\n",
       "        0.18704182, 0.1735112 , 0.18181211, 0.1692295 , 0.18064591,\n",
       "        0.17463587, 0.18067577, 0.17876604, 0.17939663, 0.18534611,\n",
       "        0.18756233, 0.17225417, 0.17460708, 0.17989951, 0.18190439,\n",
       "        0.17467423, 0.18218414, 0.18542397, 0.17521915, 0.1751946 ,\n",
       "        0.1739674 , 0.18661144, 0.17830583, 0.17615433, 0.18720582,\n",
       "        0.18869083, 0.1789208 , 0.16927351, 0.17637637, 0.17408767,\n",
       "        0.17659019, 0.17438358, 0.1719342 , 0.18368955, 0.1651383 ,\n",
       "        0.17589863, 0.16717605, 0.17550257, 0.17365359, 0.18356304,\n",
       "        0.17967078, 0.17884743, 0.17500278, 0.18210728, 0.18248266,\n",
       "        0.17905366, 0.17850384, 0.17959495, 0.17684524, 0.16926256,\n",
       "        0.180854  , 0.18661948, 0.1718951 , 0.16462725, 0.17755316,\n",
       "        0.17572956, 0.1709356 , 0.17745273, 0.18897794, 0.17126243,\n",
       "        0.17854074, 0.17724895, 0.18263789, 0.18712276, 0.18080415,\n",
       "        0.17394684, 0.17327975, 0.17991018, 0.16958725, 0.17718646,\n",
       "        0.18322748, 0.169354  , 0.1733532 , 0.18298188, 0.17975788,\n",
       "        0.17093758, 0.17590378, 0.17441782, 0.17790693, 0.17693558,\n",
       "        0.17597185, 0.17627648, 0.17383115, 0.17185424, 0.17805201,\n",
       "        0.17953882, 0.17805931, 0.17811644, 0.1736126 , 0.18913552,\n",
       "        0.17605968, 0.18258907, 0.18056642, 0.18240637, 0.16758473,\n",
       "        0.1828949 ], dtype=float32),\n",
       " 'advantage_module.dueling_A_0.w': array([[0.36289316, 0.3600223 , 0.3683541 , ..., 0.36950657, 0.36467963,\n",
       "         0.37108168],\n",
       "        [0.36261076, 0.35803783, 0.36331558, ..., 0.36282617, 0.3722109 ,\n",
       "         0.36923474],\n",
       "        [0.36272693, 0.3570891 , 0.3657146 , ..., 0.36981422, 0.36706778,\n",
       "         0.3705603 ],\n",
       "        ...,\n",
       "        [0.3630188 , 0.35942957, 0.36754236, ..., 0.367564  , 0.3674238 ,\n",
       "         0.36935613],\n",
       "        [0.36197925, 0.3583523 , 0.3676551 , ..., 0.3679677 , 0.36867467,\n",
       "         0.36812076],\n",
       "        [0.37500978, 0.3807706 , 0.37196547, ..., 0.37429884, 0.36822072,\n",
       "         0.3666719 ]], dtype=float32),\n",
       " 'advantage_module.dueling_A_0.b': array([-6.29342068e-03, -8.92140903e-03,  5.97422768e-04, -2.66284129e-04,\n",
       "         1.44822989e-04, -1.25471281e-03, -6.20957511e-03, -1.71349421e-02,\n",
       "         5.42799756e-03,  1.66865881e-04, -7.01518217e-03, -2.90142302e-03,\n",
       "        -7.50953564e-03, -8.98591708e-04, -7.14848144e-03, -8.72220378e-03,\n",
       "         7.40421144e-03, -7.56029226e-03, -4.47130250e-03, -5.95741905e-03,\n",
       "        -6.31349348e-03,  2.24930653e-03, -1.32650444e-02, -7.46749854e-03,\n",
       "        -6.47121528e-03, -4.80989506e-03, -9.67114232e-03, -7.92359002e-03,\n",
       "        -5.06192818e-03, -4.21658251e-03, -2.78877770e-03, -1.45861332e-03,\n",
       "        -8.98569636e-03, -9.14275739e-03,  5.62047912e-03,  2.58105225e-03,\n",
       "        -9.95015167e-03, -4.82604792e-03,  1.38900802e-03, -6.05823984e-03,\n",
       "        -2.70423922e-03, -1.22974906e-02,  7.30413012e-03,  4.58247541e-03,\n",
       "         3.31736082e-04, -7.53613748e-03,  6.29879581e-03, -5.85926045e-03,\n",
       "        -3.14306584e-03, -1.55369136e-02, -4.56390483e-03, -6.32104711e-05,\n",
       "        -9.59235337e-03,  7.56261405e-03, -1.25900842e-02,  6.99774968e-03,\n",
       "        -8.85513518e-03, -8.22111126e-03, -1.02071045e-02, -1.23842165e-03,\n",
       "         9.00072686e-04, -2.24283361e-03, -2.42213882e-03, -9.17605590e-03,\n",
       "        -4.73570405e-03, -1.55979721e-02, -6.29891269e-03,  3.43990163e-03,\n",
       "        -2.59801513e-03, -1.11178011e-02,  1.22140360e-03,  3.45124980e-03,\n",
       "        -3.80037143e-03,  1.19921956e-02, -5.17452974e-03, -7.38390815e-03,\n",
       "        -2.93849129e-03,  9.51972697e-03, -7.20881950e-03, -4.59881313e-03,\n",
       "        -9.65578575e-03, -8.87472462e-03, -4.51401854e-03, -2.50144280e-04,\n",
       "        -7.71535840e-03, -2.54734245e-04, -4.77316324e-03, -1.27142705e-02,\n",
       "        -4.92973858e-03,  5.69447130e-03,  9.55877709e-04,  3.03323427e-03,\n",
       "        -3.57892527e-03, -2.60634720e-03,  1.80315808e-04, -5.04133105e-03,\n",
       "        -2.43941508e-03, -9.58169531e-03, -1.90102856e-03, -1.36785926e-02,\n",
       "         2.11201934e-03,  4.69174888e-03, -1.40105104e-02,  5.00800274e-03,\n",
       "        -8.30076821e-03, -7.32954964e-03, -5.47841378e-03,  1.43281580e-03,\n",
       "        -2.59775226e-03, -4.28327126e-03, -3.17233079e-03, -9.79133882e-03,\n",
       "         3.69479414e-04, -1.19850575e-03, -6.55516842e-03,  8.26995447e-03,\n",
       "         3.88090103e-03, -4.31038300e-03, -1.24996458e-03, -5.07274130e-03,\n",
       "        -9.02468048e-04, -9.33952630e-04, -2.04381696e-03,  9.34003736e-04,\n",
       "        -1.31358695e-03, -5.95952524e-03, -6.19705860e-03, -1.00340182e-02,\n",
       "        -7.79595401e-04,  1.85738769e-04, -8.75063520e-03, -1.23198926e-02,\n",
       "        -4.09862539e-03, -5.40326117e-03, -1.19330653e-04, -4.76512592e-03,\n",
       "         2.64070835e-03, -4.49453853e-03, -1.83735823e-03, -6.36255462e-03,\n",
       "        -7.55370641e-03, -6.25394192e-03,  6.31494774e-03, -1.24483844e-02,\n",
       "        -1.28827933e-02, -7.37464661e-03,  3.88140143e-05,  5.00568282e-03,\n",
       "         3.92475311e-04, -2.83930032e-03, -1.04047509e-03, -6.13376265e-03,\n",
       "        -5.24483761e-03,  9.56700766e-04, -2.37553869e-03, -1.36262747e-02,\n",
       "        -3.27411690e-03,  2.01911060e-03,  2.79977312e-03, -5.95582183e-03,\n",
       "        -8.87111202e-03, -6.37435867e-03,  5.82721736e-03,  5.89868613e-03,\n",
       "        -2.61153560e-04,  6.40728092e-03, -6.93175104e-03,  6.92964066e-03,\n",
       "         3.40897590e-04,  2.64149555e-03, -2.08605174e-03, -8.36602412e-03,\n",
       "        -1.11182472e-02,  1.88321143e-03,  7.45907542e-04, -1.72313524e-03,\n",
       "        -2.42637517e-03, -1.49415936e-02,  3.64944688e-04,  2.31667934e-03,\n",
       "        -2.93200882e-03, -5.49574476e-03, -4.28155763e-03, -9.80104599e-03,\n",
       "         4.29828186e-03, -3.28108249e-03, -6.66023139e-03, -8.62252805e-03,\n",
       "        -7.40691228e-03, -2.10519112e-03, -1.22601418e-02, -7.09058112e-03,\n",
       "        -8.16186424e-03, -2.93672364e-03, -1.69191812e-03, -1.05696833e-02,\n",
       "        -6.23349054e-03, -4.16360283e-03,  1.26535550e-03, -7.50002312e-03,\n",
       "         8.80200765e-04, -1.97171909e-03, -3.18595930e-03, -7.37296138e-03,\n",
       "        -1.30888419e-02,  1.61345850e-03, -3.05143371e-03, -8.08802713e-03,\n",
       "         8.46362847e-04, -4.06982983e-03, -3.60990432e-03,  1.45238161e-03,\n",
       "        -1.04147964e-03,  3.60688008e-03, -3.26577039e-03, -1.34023214e-02,\n",
       "        -5.52461389e-03, -3.96670541e-03, -6.69797650e-03, -6.88809762e-03,\n",
       "        -1.48820197e-02, -1.11583965e-02, -8.65086634e-03, -4.90494538e-03,\n",
       "        -3.58826946e-04,  1.14922749e-03, -1.49445620e-03,  1.33731263e-03,\n",
       "        -7.75581785e-03, -1.44957704e-03, -9.38748824e-04, -3.93929426e-03,\n",
       "        -9.17763729e-03, -1.26172081e-02, -5.54123288e-03, -3.71435052e-03,\n",
       "        -8.77625775e-03, -3.23484186e-03, -1.36442892e-02, -3.88798350e-03,\n",
       "         3.04653333e-03, -1.20497961e-02,  2.24813214e-03, -8.27944186e-03,\n",
       "        -6.17111521e-03, -2.50518066e-03, -4.95055830e-03,  1.25213398e-03,\n",
       "         2.51619262e-03,  1.84094580e-03, -7.56347319e-03,  6.39371632e-04,\n",
       "        -5.48938988e-03,  6.75593270e-04, -9.06209229e-04, -2.11471692e-03],\n",
       "       dtype=float32),\n",
       " 'advantage_module.A.sigma_w': array([[-4.65564355e-02, -2.35272851e-02],\n",
       "        [-4.86597717e-02, -3.96110071e-03],\n",
       "        [-5.38379475e-02, -4.43796329e-02],\n",
       "        [-5.22150472e-02,  4.77960184e-02],\n",
       "        [ 4.42502983e-02, -2.46805651e-03],\n",
       "        [ 3.63206230e-02,  3.27315852e-02],\n",
       "        [-3.86016034e-02, -1.33034652e-02],\n",
       "        [-3.13429050e-02,  3.17368358e-02],\n",
       "        [-3.21955159e-02,  3.70616913e-02],\n",
       "        [-1.41921565e-02, -1.55929429e-02],\n",
       "        [-2.48377770e-02, -3.00060422e-03],\n",
       "        [-5.62897604e-03,  5.84398396e-02],\n",
       "        [ 1.16966246e-02,  1.28711769e-02],\n",
       "        [ 7.11780507e-04, -5.27634919e-02],\n",
       "        [ 4.00123978e-03, -4.01319042e-02],\n",
       "        [-1.90982390e-02, -3.02083995e-02],\n",
       "        [-3.61466454e-03, -2.87939217e-02],\n",
       "        [ 3.67101207e-02, -4.52157147e-02],\n",
       "        [-2.42700614e-02,  8.76458827e-04],\n",
       "        [-7.68788299e-03,  9.91631579e-03],\n",
       "        [ 4.52582650e-02,  7.29178591e-03],\n",
       "        [-1.69811565e-02,  5.26292771e-02],\n",
       "        [-3.92678715e-02,  1.70550719e-02],\n",
       "        [ 5.28906398e-02,  3.30419205e-02],\n",
       "        [ 5.01201162e-03, -3.41374353e-02],\n",
       "        [-8.20203125e-03,  6.89121662e-03],\n",
       "        [-3.34544219e-02, -1.41528342e-02],\n",
       "        [-3.68387625e-02, -3.76297124e-02],\n",
       "        [ 1.19291048e-03,  2.06092279e-02],\n",
       "        [ 4.63925637e-02,  2.60203751e-03],\n",
       "        [ 3.21054645e-02, -1.50374910e-02],\n",
       "        [ 5.47918417e-02, -4.70102578e-02],\n",
       "        [ 3.15754227e-02,  2.46200617e-02],\n",
       "        [ 2.05317624e-02, -2.86380891e-02],\n",
       "        [-1.93555765e-02, -1.29801929e-02],\n",
       "        [-4.93240058e-02, -1.96516849e-02],\n",
       "        [ 1.99612346e-03, -7.62277236e-03],\n",
       "        [ 5.80922812e-02,  4.58574817e-02],\n",
       "        [-3.19791585e-02,  5.44296801e-02],\n",
       "        [ 3.82888541e-02,  9.13434254e-04],\n",
       "        [-2.92500127e-02,  4.80535924e-02],\n",
       "        [-2.59362310e-02, -3.96488197e-02],\n",
       "        [ 1.96685758e-03,  3.56744304e-02],\n",
       "        [-5.55505715e-02,  3.50544155e-02],\n",
       "        [-4.21675108e-02, -3.70650105e-02],\n",
       "        [ 4.85640168e-02, -4.20672260e-02],\n",
       "        [ 3.24339829e-02,  3.93771417e-02],\n",
       "        [-3.95020992e-02, -5.18573187e-02],\n",
       "        [-1.22474087e-02, -5.76886758e-02],\n",
       "        [ 2.05223616e-02, -4.55615856e-02],\n",
       "        [ 3.41945654e-03,  1.29021127e-02],\n",
       "        [-5.22360355e-02, -4.26736809e-02],\n",
       "        [-5.12882136e-02, -3.74849588e-02],\n",
       "        [-2.57921033e-02, -2.22648792e-02],\n",
       "        [ 3.00719682e-03,  3.49263079e-03],\n",
       "        [-3.52680087e-02, -3.32821645e-02],\n",
       "        [-1.32630104e-02,  4.13782299e-02],\n",
       "        [-2.10299646e-03, -2.51760334e-02],\n",
       "        [ 2.48930510e-02, -3.33241746e-02],\n",
       "        [ 1.69951599e-02,  5.21126762e-03],\n",
       "        [ 4.33080532e-02, -5.48400655e-02],\n",
       "        [ 3.26909199e-02, -4.21686284e-03],\n",
       "        [-5.18637784e-02,  2.13579144e-02],\n",
       "        [ 5.69014053e-04, -3.85057293e-02],\n",
       "        [ 1.48805995e-02,  4.10860293e-02],\n",
       "        [-7.52686709e-03, -7.45899044e-03],\n",
       "        [-3.05939410e-02,  4.84690517e-02],\n",
       "        [-1.59532353e-02,  4.86347415e-02],\n",
       "        [ 3.18062417e-02, -1.86893661e-02],\n",
       "        [-3.65429595e-02,  3.23225558e-02],\n",
       "        [-4.00060900e-02,  4.73521501e-02],\n",
       "        [ 1.11854207e-02,  5.62664345e-02],\n",
       "        [-1.61669236e-02,  1.48977377e-02],\n",
       "        [ 1.27996532e-02, -7.36833811e-02],\n",
       "        [ 4.52270210e-02,  5.15942723e-02],\n",
       "        [ 4.34044860e-02, -2.44098119e-02],\n",
       "        [-4.58662212e-03,  2.46314127e-02],\n",
       "        [-1.21003194e-02,  3.29959430e-02],\n",
       "        [ 4.73498181e-02, -7.92880263e-03],\n",
       "        [ 1.99918747e-02, -3.15085799e-02],\n",
       "        [ 9.01663501e-04,  5.02001774e-03],\n",
       "        [ 2.31369138e-02,  4.18744087e-02],\n",
       "        [ 1.57792866e-02, -4.15408909e-02],\n",
       "        [ 6.76163379e-03,  6.38480559e-02],\n",
       "        [ 3.39568146e-02,  2.99479649e-03],\n",
       "        [-2.23090965e-02, -2.68070288e-02],\n",
       "        [-3.11723780e-02, -8.11647531e-03],\n",
       "        [ 3.62013951e-02, -2.38600112e-02],\n",
       "        [-1.20452382e-02,  1.76500361e-02],\n",
       "        [ 3.41440812e-02,  2.98871007e-02],\n",
       "        [ 5.01261465e-02, -5.42905442e-02],\n",
       "        [-2.65537674e-04, -5.56597635e-02],\n",
       "        [-1.28990859e-02, -1.70583185e-02],\n",
       "        [-2.53664553e-02, -4.92581762e-02],\n",
       "        [-4.38716412e-02,  1.86217688e-02],\n",
       "        [-4.18553390e-02, -2.83988696e-02],\n",
       "        [-3.23264785e-02, -4.24901880e-02],\n",
       "        [ 3.79745616e-03,  3.69425304e-02],\n",
       "        [-6.69507217e-03, -9.34486277e-03],\n",
       "        [ 3.24194431e-02, -4.81047183e-02],\n",
       "        [-2.76043136e-02, -3.10177412e-02],\n",
       "        [ 2.89400201e-02,  4.85447980e-03],\n",
       "        [ 4.72153649e-02, -9.91489924e-03],\n",
       "        [-3.00633535e-02,  1.34234568e-02],\n",
       "        [ 2.15536393e-02,  1.02943629e-02],\n",
       "        [ 1.12444803e-03, -2.21435819e-02],\n",
       "        [ 3.73169370e-02,  1.81333404e-02],\n",
       "        [ 4.19452228e-02, -6.33536233e-03],\n",
       "        [ 5.03162853e-02, -2.32317913e-02],\n",
       "        [ 5.11543937e-02,  3.83347683e-02],\n",
       "        [ 1.44423107e-02,  4.09188755e-02],\n",
       "        [-1.73278563e-02,  3.92540991e-02],\n",
       "        [-4.08993885e-02,  6.82041571e-02],\n",
       "        [ 1.65296160e-02, -1.07811531e-02],\n",
       "        [ 4.56776693e-02, -1.80103537e-02],\n",
       "        [ 4.16007526e-02,  9.22857970e-03],\n",
       "        [ 1.00889057e-03,  1.68922059e-02],\n",
       "        [ 4.51082215e-02, -5.21105491e-02],\n",
       "        [-5.43990843e-02, -2.61582807e-02],\n",
       "        [ 4.27066796e-02, -3.61127332e-02],\n",
       "        [-2.61353888e-02, -4.59290221e-02],\n",
       "        [-3.65161821e-02,  1.44467168e-02],\n",
       "        [ 5.96662015e-02, -2.83175614e-02],\n",
       "        [-1.41368927e-02, -3.84555645e-02],\n",
       "        [ 4.14545909e-02,  1.47693856e-02],\n",
       "        [-3.85095403e-02,  2.32083052e-02],\n",
       "        [ 5.52285761e-02,  3.90099660e-02],\n",
       "        [ 2.15675551e-02, -3.31576951e-02],\n",
       "        [-9.12699476e-03,  6.16926774e-02],\n",
       "        [ 1.85405295e-02, -3.18474360e-02],\n",
       "        [ 1.27783045e-02,  2.35343594e-02],\n",
       "        [-3.09573151e-02, -5.01989573e-02],\n",
       "        [ 3.94691452e-02, -3.04715447e-02],\n",
       "        [ 5.30775562e-02,  1.82552915e-02],\n",
       "        [-4.80561368e-02, -1.65341794e-02],\n",
       "        [-4.21376005e-02,  1.14435907e-02],\n",
       "        [-5.84043711e-02,  1.12468684e-02],\n",
       "        [-2.38215271e-02, -5.06408289e-02],\n",
       "        [ 4.18836810e-02,  4.90431637e-02],\n",
       "        [ 2.00155675e-02,  4.50414084e-02],\n",
       "        [-4.75636880e-05,  2.20356304e-02],\n",
       "        [-4.92423214e-02,  4.68328707e-02],\n",
       "        [ 4.37664725e-02, -6.44709021e-02],\n",
       "        [ 4.31568697e-02, -5.04610799e-02],\n",
       "        [ 1.67094637e-02,  3.46864872e-02],\n",
       "        [-4.63459119e-02,  1.64337102e-02],\n",
       "        [-1.62165333e-02, -1.70621425e-02],\n",
       "        [-1.00886810e-03,  3.54994461e-02],\n",
       "        [ 1.61536653e-02, -3.64508629e-02],\n",
       "        [-6.29891977e-02,  4.64020483e-03],\n",
       "        [-4.32639867e-02,  1.30931903e-02],\n",
       "        [ 5.22230379e-02,  2.49865819e-02],\n",
       "        [ 4.50991467e-02, -1.94651689e-02],\n",
       "        [-4.47980501e-02,  5.55067919e-02],\n",
       "        [-3.98291387e-02, -3.22214067e-02],\n",
       "        [-4.02349345e-02,  4.15403657e-02],\n",
       "        [ 3.76504846e-02, -2.80702095e-02],\n",
       "        [-4.99007776e-02,  4.13357392e-02],\n",
       "        [-4.18281965e-02,  4.24483195e-02],\n",
       "        [ 1.33423992e-02, -4.90703387e-03],\n",
       "        [-8.33492260e-03, -2.11671498e-02],\n",
       "        [ 5.59400171e-02, -2.07839105e-02],\n",
       "        [-4.19953205e-02, -5.60958497e-02],\n",
       "        [ 4.93817553e-02, -3.03659607e-02],\n",
       "        [ 4.49957931e-03,  4.69415523e-02],\n",
       "        [ 5.50386496e-02, -2.79948618e-02],\n",
       "        [-1.45382797e-02,  2.93226428e-02],\n",
       "        [ 1.90477800e-02,  5.94439879e-02],\n",
       "        [ 9.64607019e-03,  4.49372604e-02],\n",
       "        [-6.50369674e-02, -1.97876357e-02],\n",
       "        [ 1.92648526e-02, -2.14533880e-02],\n",
       "        [-1.91641841e-02, -4.76051308e-02],\n",
       "        [-5.08857742e-02,  1.66146681e-02],\n",
       "        [-6.23228326e-02, -1.74514670e-02],\n",
       "        [ 4.05495651e-02,  6.94576949e-02],\n",
       "        [ 5.47081865e-02,  2.05394309e-02],\n",
       "        [ 4.24225703e-02, -1.55819729e-02],\n",
       "        [ 3.24320160e-02, -1.80987604e-02],\n",
       "        [ 5.93424104e-02, -3.62513363e-02],\n",
       "        [ 4.58063632e-02, -1.79099813e-02],\n",
       "        [-3.13170962e-02,  2.69560013e-02],\n",
       "        [-1.70382410e-02,  3.54730487e-02],\n",
       "        [ 2.32100897e-02,  5.43722976e-03],\n",
       "        [ 1.32375713e-02, -2.54706247e-03],\n",
       "        [ 3.94062288e-02,  1.08146323e-02],\n",
       "        [-4.76552248e-02,  1.55977979e-02],\n",
       "        [-2.62484048e-02, -3.32639478e-02],\n",
       "        [ 1.38656492e-03, -4.29550707e-02],\n",
       "        [ 6.98532397e-03,  2.94656828e-02],\n",
       "        [ 1.47058489e-02,  5.68260113e-03],\n",
       "        [-2.89466325e-03, -3.65464948e-02],\n",
       "        [ 2.67638778e-03,  2.16626581e-02],\n",
       "        [ 4.17772010e-02, -1.44479917e-02],\n",
       "        [-2.05141865e-02, -3.86981554e-02],\n",
       "        [-5.60723580e-02,  2.89958958e-02],\n",
       "        [ 8.78988858e-03,  1.43129528e-02],\n",
       "        [ 5.04014604e-02,  2.21433789e-02],\n",
       "        [-1.23679433e-02,  6.22982644e-02],\n",
       "        [ 1.97341461e-02, -2.54502278e-02],\n",
       "        [-2.62438674e-02,  8.78039841e-03],\n",
       "        [-5.64714782e-02, -2.99331211e-02],\n",
       "        [-7.31955096e-03, -4.86180186e-02],\n",
       "        [-4.24592346e-02, -6.21336214e-02],\n",
       "        [-4.93843853e-02,  2.88025141e-02],\n",
       "        [-4.98670749e-02,  8.62411875e-03],\n",
       "        [-1.81394722e-02,  5.98972403e-02],\n",
       "        [ 3.92697230e-02, -4.69473340e-02],\n",
       "        [ 1.84812695e-02,  7.84677733e-03],\n",
       "        [-6.16865344e-02,  2.74951868e-02],\n",
       "        [ 3.74193676e-03, -2.24210713e-02],\n",
       "        [ 4.58684526e-02, -1.79756957e-03],\n",
       "        [-9.09473002e-03,  2.31668931e-02],\n",
       "        [-3.65817212e-02, -3.43653224e-02],\n",
       "        [-4.75291461e-02, -8.83625168e-03],\n",
       "        [ 4.54932731e-03, -1.76830739e-02],\n",
       "        [ 8.55442788e-03, -2.69481149e-02],\n",
       "        [ 5.31827807e-02, -3.62157188e-02],\n",
       "        [ 1.30676581e-02,  3.92792150e-02],\n",
       "        [ 3.57760750e-02, -5.27946046e-03],\n",
       "        [ 1.94923077e-02,  2.02159900e-02],\n",
       "        [ 2.69350521e-02, -1.10453153e-02],\n",
       "        [ 4.64842506e-02, -2.47167777e-02],\n",
       "        [ 4.78876606e-02,  4.57125232e-02],\n",
       "        [ 1.93688590e-02,  9.86767840e-03],\n",
       "        [-4.65373229e-03, -3.52969989e-02],\n",
       "        [-4.04792987e-02,  3.56952399e-02],\n",
       "        [ 5.69636896e-02, -3.21401246e-02],\n",
       "        [ 2.00873055e-02,  5.06835431e-02],\n",
       "        [ 2.38526165e-02,  2.38874517e-02],\n",
       "        [-1.22949518e-02, -5.78157119e-02],\n",
       "        [ 4.64615179e-03,  3.86685804e-02],\n",
       "        [ 3.23390327e-02, -9.06895846e-03],\n",
       "        [-3.41504104e-02, -4.42143604e-02],\n",
       "        [-3.69102857e-03,  6.53297175e-03],\n",
       "        [-4.47204523e-02,  3.83740589e-02],\n",
       "        [-4.12763916e-02, -3.50218304e-02],\n",
       "        [-4.62554581e-02,  1.21965995e-02],\n",
       "        [ 2.95588113e-02,  6.98879501e-03],\n",
       "        [-3.34900841e-02,  1.03244849e-03],\n",
       "        [-4.78798933e-02,  5.13274558e-02],\n",
       "        [-6.18390553e-02, -1.61490701e-02],\n",
       "        [-4.23702262e-02, -4.27217782e-02],\n",
       "        [ 6.43107593e-02, -3.46106775e-02],\n",
       "        [ 4.36474681e-02, -5.06553575e-02],\n",
       "        [ 3.39059229e-03,  1.30220829e-02],\n",
       "        [ 5.06914593e-02,  3.43837328e-02],\n",
       "        [-3.59179042e-02,  2.76220758e-02],\n",
       "        [ 4.53976206e-02, -2.63119605e-03],\n",
       "        [ 2.53654867e-02, -5.31363152e-02],\n",
       "        [ 2.92869993e-02, -6.28531277e-02],\n",
       "        [-3.45395319e-02, -7.81413459e-04],\n",
       "        [ 2.65766438e-02, -1.80817712e-02],\n",
       "        [-5.05754352e-02, -1.33238984e-02],\n",
       "        [ 2.75556147e-02,  9.57334181e-04],\n",
       "        [-5.71101196e-02,  1.24336714e-02],\n",
       "        [-4.59907502e-02, -1.63474157e-02]], dtype=float32),\n",
       " 'advantage_module.A.sigma_b': array([0.03476992, 0.02634908], dtype=float32),\n",
       " 'advantage_module.A.w': array([[0.37273517, 0.3743522 ],\n",
       "        [0.3747198 , 0.37236756],\n",
       "        [0.37396827, 0.3731191 ],\n",
       "        [0.37656385, 0.3705235 ],\n",
       "        [0.37421325, 0.3728741 ],\n",
       "        [0.3738914 , 0.37319595],\n",
       "        [0.37643299, 0.37065437],\n",
       "        [0.3735908 , 0.37349656],\n",
       "        [0.3767755 , 0.37031186],\n",
       "        [0.37876928, 0.36831808],\n",
       "        [0.3761026 , 0.37098476],\n",
       "        [0.37489593, 0.37219143],\n",
       "        [0.3761059 , 0.37098145],\n",
       "        [0.3722278 , 0.37485957],\n",
       "        [0.37246665, 0.3746207 ],\n",
       "        [0.37483367, 0.3722537 ],\n",
       "        [0.37441522, 0.37267214],\n",
       "        [0.37493116, 0.3721562 ],\n",
       "        [0.37389824, 0.37318912],\n",
       "        [0.37793225, 0.3691551 ],\n",
       "        [0.3759992 , 0.37108815],\n",
       "        [0.37504694, 0.37204042],\n",
       "        [0.3778591 , 0.36922827],\n",
       "        [0.37460238, 0.37248498],\n",
       "        [0.37562943, 0.37145793],\n",
       "        [0.37420702, 0.37288034],\n",
       "        [0.37472844, 0.37235892],\n",
       "        [0.3755146 , 0.37157276],\n",
       "        [0.37498134, 0.37210602],\n",
       "        [0.37735075, 0.3697366 ],\n",
       "        [0.3736302 , 0.37345716],\n",
       "        [0.37521353, 0.37187383],\n",
       "        [0.3752795 , 0.37180787],\n",
       "        [0.37379882, 0.37328854],\n",
       "        [0.37408012, 0.37300724],\n",
       "        [0.3748311 , 0.37225625],\n",
       "        [0.37311336, 0.373974  ],\n",
       "        [0.37502107, 0.3720663 ],\n",
       "        [0.3768192 , 0.37026817],\n",
       "        [0.37537476, 0.3717126 ],\n",
       "        [0.3739945 , 0.37309286],\n",
       "        [0.37445143, 0.37263593],\n",
       "        [0.37349054, 0.37359682],\n",
       "        [0.37592593, 0.37116143],\n",
       "        [0.37357354, 0.37351382],\n",
       "        [0.37589747, 0.3711899 ],\n",
       "        [0.37582514, 0.37126222],\n",
       "        [0.37712628, 0.36996108],\n",
       "        [0.37268746, 0.3743999 ],\n",
       "        [0.37434193, 0.37274542],\n",
       "        [0.377444  , 0.36964336],\n",
       "        [0.37519157, 0.3718958 ],\n",
       "        [0.37723067, 0.3698567 ],\n",
       "        [0.37676355, 0.3703238 ],\n",
       "        [0.37535477, 0.3717326 ],\n",
       "        [0.37506709, 0.37202027],\n",
       "        [0.37599212, 0.37109524],\n",
       "        [0.37666044, 0.37042692],\n",
       "        [0.37305543, 0.37403193],\n",
       "        [0.37422207, 0.3728653 ],\n",
       "        [0.3753461 , 0.37174127],\n",
       "        [0.3750339 , 0.37205347],\n",
       "        [0.37634593, 0.37074143],\n",
       "        [0.37652984, 0.37055752],\n",
       "        [0.37383518, 0.37325218],\n",
       "        [0.3744889 , 0.37259847],\n",
       "        [0.3748228 , 0.37226456],\n",
       "        [0.3753501 , 0.37173727],\n",
       "        [0.37724373, 0.36984363],\n",
       "        [0.37514165, 0.3719457 ],\n",
       "        [0.37657243, 0.37051493],\n",
       "        [0.37749377, 0.3695936 ],\n",
       "        [0.37710622, 0.36998114],\n",
       "        [0.37482163, 0.37226573],\n",
       "        [0.37454367, 0.3725437 ],\n",
       "        [0.37558335, 0.371504  ],\n",
       "        [0.37632138, 0.37076598],\n",
       "        [0.37512156, 0.3719658 ],\n",
       "        [0.37479472, 0.37229264],\n",
       "        [0.37537286, 0.3717145 ],\n",
       "        [0.3756202 , 0.37146717],\n",
       "        [0.37494063, 0.37214673],\n",
       "        [0.37595525, 0.3711321 ],\n",
       "        [0.3710815 , 0.37600586],\n",
       "        [0.37295648, 0.37413087],\n",
       "        [0.37687448, 0.37021288],\n",
       "        [0.37493995, 0.3721474 ],\n",
       "        [0.37642682, 0.37066054],\n",
       "        [0.37514126, 0.3719461 ],\n",
       "        [0.3751222 , 0.37196517],\n",
       "        [0.37718028, 0.36990708],\n",
       "        [0.37239376, 0.3746936 ],\n",
       "        [0.37791473, 0.36917263],\n",
       "        [0.37639675, 0.3706906 ],\n",
       "        [0.37675348, 0.37033388],\n",
       "        [0.376297  , 0.37079036],\n",
       "        [0.37610072, 0.37098664],\n",
       "        [0.37316   , 0.37392735],\n",
       "        [0.37645516, 0.3706322 ],\n",
       "        [0.37596762, 0.37111974],\n",
       "        [0.37570906, 0.3713783 ],\n",
       "        [0.37675673, 0.37033063],\n",
       "        [0.37570167, 0.3713857 ],\n",
       "        [0.3757791 , 0.37130827],\n",
       "        [0.37724316, 0.3698442 ],\n",
       "        [0.3746315 , 0.37245587],\n",
       "        [0.37574854, 0.3713388 ],\n",
       "        [0.37660626, 0.3704811 ],\n",
       "        [0.37600103, 0.37108633],\n",
       "        [0.37797877, 0.3691086 ],\n",
       "        [0.37478054, 0.37230682],\n",
       "        [0.37201542, 0.37507194],\n",
       "        [0.37485376, 0.3722336 ],\n",
       "        [0.3759898 , 0.37109756],\n",
       "        [0.37451875, 0.3725686 ],\n",
       "        [0.37557182, 0.37151554],\n",
       "        [0.37716287, 0.3699245 ],\n",
       "        [0.37383458, 0.37325278],\n",
       "        [0.37726182, 0.36982554],\n",
       "        [0.3712991 , 0.37578827],\n",
       "        [0.37696007, 0.3701273 ],\n",
       "        [0.37515765, 0.3719297 ],\n",
       "        [0.37283057, 0.3742568 ],\n",
       "        [0.37508595, 0.3720014 ],\n",
       "        [0.37408015, 0.3730072 ],\n",
       "        [0.37735334, 0.36973402],\n",
       "        [0.375118  , 0.37196937],\n",
       "        [0.37406272, 0.37302464],\n",
       "        [0.37681332, 0.37027404],\n",
       "        [0.37507328, 0.37201408],\n",
       "        [0.3751927 , 0.37189466],\n",
       "        [0.37480995, 0.3722774 ],\n",
       "        [0.37367418, 0.37341318],\n",
       "        [0.37608945, 0.3709979 ],\n",
       "        [0.37642023, 0.37066713],\n",
       "        [0.37410024, 0.37298712],\n",
       "        [0.37620726, 0.3708801 ],\n",
       "        [0.37641633, 0.37067103],\n",
       "        [0.3771012 , 0.36998615],\n",
       "        [0.37683487, 0.3702525 ],\n",
       "        [0.376008  , 0.37107936],\n",
       "        [0.37579742, 0.37128994],\n",
       "        [0.3757579 , 0.37132946],\n",
       "        [0.37614673, 0.37094063],\n",
       "        [0.3725853 , 0.37450206],\n",
       "        [0.37519398, 0.37189338],\n",
       "        [0.37590382, 0.37118354],\n",
       "        [0.3736548 , 0.37343255],\n",
       "        [0.37423062, 0.37285674],\n",
       "        [0.37413347, 0.3729539 ],\n",
       "        [0.3772628 , 0.36982456],\n",
       "        [0.37447342, 0.37261394],\n",
       "        [0.37311834, 0.37396902],\n",
       "        [0.3777324 , 0.36935496],\n",
       "        [0.37496313, 0.37212422],\n",
       "        [0.37430146, 0.3727859 ],\n",
       "        [0.37650996, 0.3705774 ],\n",
       "        [0.37545803, 0.37162933],\n",
       "        [0.3773178 , 0.36976957],\n",
       "        [0.37756792, 0.36951944],\n",
       "        [0.3771809 , 0.36990646],\n",
       "        [0.3779158 , 0.36917156],\n",
       "        [0.37694752, 0.37013984],\n",
       "        [0.37856016, 0.3685272 ],\n",
       "        [0.37400466, 0.3730827 ],\n",
       "        [0.37595433, 0.37113303],\n",
       "        [0.3755166 , 0.37157077],\n",
       "        [0.37671188, 0.37037548],\n",
       "        [0.3763765 , 0.37071085],\n",
       "        [0.37524796, 0.3718394 ],\n",
       "        [0.37522525, 0.3718621 ],\n",
       "        [0.3736492 , 0.37343815],\n",
       "        [0.37722972, 0.36985764],\n",
       "        [0.37441713, 0.37267023],\n",
       "        [0.3770974 , 0.36998996],\n",
       "        [0.37546274, 0.37162462],\n",
       "        [0.37578803, 0.37129933],\n",
       "        [0.373315  , 0.37377235],\n",
       "        [0.3758424 , 0.37124497],\n",
       "        [0.3705612 , 0.37652615],\n",
       "        [0.3753827 , 0.37170467],\n",
       "        [0.37655437, 0.370533  ],\n",
       "        [0.3756835 , 0.37140387],\n",
       "        [0.37653887, 0.3705485 ],\n",
       "        [0.37346756, 0.3736198 ],\n",
       "        [0.37531385, 0.3717735 ],\n",
       "        [0.37431127, 0.3727761 ],\n",
       "        [0.37556157, 0.3715258 ],\n",
       "        [0.37534192, 0.37174544],\n",
       "        [0.37693417, 0.3701532 ],\n",
       "        [0.37399372, 0.37309363],\n",
       "        [0.37386894, 0.37321842],\n",
       "        [0.37751856, 0.3695688 ],\n",
       "        [0.3760809 , 0.37100646],\n",
       "        [0.372451  , 0.37463635],\n",
       "        [0.37549838, 0.37158898],\n",
       "        [0.37771007, 0.3693773 ],\n",
       "        [0.37463665, 0.3724507 ],\n",
       "        [0.37349316, 0.3735942 ],\n",
       "        [0.37555543, 0.37153193],\n",
       "        [0.3758628 , 0.37122455],\n",
       "        [0.37330937, 0.373778  ],\n",
       "        [0.3712984 , 0.37578896],\n",
       "        [0.37474325, 0.3723441 ],\n",
       "        [0.37738565, 0.3697017 ],\n",
       "        [0.37560806, 0.3714793 ],\n",
       "        [0.37761554, 0.36947182],\n",
       "        [0.37632823, 0.37075913],\n",
       "        [0.377148  , 0.36993936],\n",
       "        [0.3769621 , 0.37012526],\n",
       "        [0.37540263, 0.37168473],\n",
       "        [0.37819389, 0.36889347],\n",
       "        [0.37723956, 0.3698478 ],\n",
       "        [0.3766656 , 0.37042177],\n",
       "        [0.3779009 , 0.36918646],\n",
       "        [0.3745994 , 0.37248796],\n",
       "        [0.37673062, 0.37035674],\n",
       "        [0.3765062 , 0.37058115],\n",
       "        [0.3760016 , 0.37108576],\n",
       "        [0.3745378 , 0.37254956],\n",
       "        [0.37296355, 0.3741238 ],\n",
       "        [0.37459826, 0.3724891 ],\n",
       "        [0.3735748 , 0.37351257],\n",
       "        [0.37618425, 0.3709031 ],\n",
       "        [0.3747328 , 0.37235457],\n",
       "        [0.37589   , 0.37119737],\n",
       "        [0.3765188 , 0.37056857],\n",
       "        [0.37602666, 0.3710607 ],\n",
       "        [0.37385336, 0.373234  ],\n",
       "        [0.37710533, 0.36998203],\n",
       "        [0.3734119 , 0.37367547],\n",
       "        [0.37236688, 0.37472048],\n",
       "        [0.37442046, 0.3726669 ],\n",
       "        [0.37485862, 0.37222874],\n",
       "        [0.37743524, 0.36965212],\n",
       "        [0.37636444, 0.37072292],\n",
       "        [0.37386325, 0.3732241 ],\n",
       "        [0.37531194, 0.37177542],\n",
       "        [0.37504315, 0.3720442 ],\n",
       "        [0.37407267, 0.3730147 ],\n",
       "        [0.37378523, 0.37330213],\n",
       "        [0.37473267, 0.3723547 ],\n",
       "        [0.37919682, 0.36789054],\n",
       "        [0.37480375, 0.3722836 ],\n",
       "        [0.376062  , 0.37102535],\n",
       "        [0.3765912 , 0.37049615],\n",
       "        [0.37448332, 0.37260404],\n",
       "        [0.37508407, 0.3720033 ],\n",
       "        [0.375615  , 0.37147236],\n",
       "        [0.377167  , 0.36992037],\n",
       "        [0.37765625, 0.3694311 ],\n",
       "        [0.37712562, 0.36996174],\n",
       "        [0.3750023 , 0.37208506],\n",
       "        [0.37520108, 0.37188628],\n",
       "        [0.3753097 , 0.37177765],\n",
       "        [0.37552565, 0.3715617 ]], dtype=float32),\n",
       " 'advantage_module.A.b': array([ 0.00145083, -0.00145083], dtype=float32),\n",
       " 'value_module.dueling_V_0.sigma_w': array([[-0.22550143, -0.25085464,  0.06938586, ..., -0.16016124,\n",
       "         -0.10928396,  0.01000143],\n",
       "        [-0.05703746, -0.33207354,  0.17043658, ...,  0.16527407,\n",
       "         -0.20696141,  0.28379038],\n",
       "        [ 0.10512429, -0.10028339,  0.12517148, ..., -0.11545372,\n",
       "          0.17122816, -0.21460956],\n",
       "        ...,\n",
       "        [-0.29429924, -0.01714855, -0.14844465, ...,  0.0838881 ,\n",
       "          0.310874  , -0.03090678],\n",
       "        [ 0.01794651, -0.09023565, -0.21408494, ...,  0.14545254,\n",
       "          0.14343274, -0.06622279],\n",
       "        [ 0.3198584 ,  0.00621956, -0.10797425, ..., -0.04838849,\n",
       "         -0.07050309,  0.04752676]], dtype=float32),\n",
       " 'value_module.dueling_V_0.sigma_b': array([0.17793652, 0.17696029, 0.17587025, 0.17914046, 0.17929284,\n",
       "        0.1765207 , 0.17355974, 0.1866468 , 0.173182  , 0.17412558,\n",
       "        0.1752721 , 0.17598625, 0.17256819, 0.17875712, 0.17121992,\n",
       "        0.17488015, 0.16742685, 0.1811556 , 0.17850657, 0.1788138 ,\n",
       "        0.17249562, 0.17957504, 0.17672206, 0.17534907, 0.17538543,\n",
       "        0.16885445, 0.1741739 , 0.17141324, 0.17692526, 0.17637365,\n",
       "        0.18029116, 0.17788544, 0.1749328 , 0.18264362, 0.17338383,\n",
       "        0.1745725 , 0.17778523, 0.17900322, 0.1753758 , 0.18024458,\n",
       "        0.17537215, 0.18196964, 0.17432995, 0.1786938 , 0.17890434,\n",
       "        0.1784113 , 0.17679489, 0.17873096, 0.17808385, 0.17531995,\n",
       "        0.17423518, 0.1708936 , 0.17067401, 0.16665742, 0.17127064,\n",
       "        0.17924307, 0.1779094 , 0.18905944, 0.16871049, 0.17743024,\n",
       "        0.17707947, 0.18150377, 0.17629434, 0.1727046 , 0.17046465,\n",
       "        0.1759027 , 0.17941904, 0.17658481, 0.16972011, 0.1768868 ,\n",
       "        0.1786532 , 0.17104599, 0.18035321, 0.17731899, 0.1801591 ,\n",
       "        0.17354804, 0.18294027, 0.17731875, 0.1842017 , 0.1718542 ,\n",
       "        0.18319832, 0.17858574, 0.17010252, 0.17835793, 0.1784612 ,\n",
       "        0.18316086, 0.17960122, 0.17575575, 0.17484924, 0.18208264,\n",
       "        0.17666574, 0.17258173, 0.16830507, 0.17758532, 0.17743164,\n",
       "        0.17420143, 0.1748026 , 0.17126031, 0.17153442, 0.17477809,\n",
       "        0.18133783, 0.1837753 , 0.17066926, 0.18021263, 0.18111192,\n",
       "        0.171751  , 0.17156696, 0.17021833, 0.17270026, 0.17824823,\n",
       "        0.18132126, 0.17355812, 0.1755944 , 0.17249969, 0.18082954,\n",
       "        0.18482518, 0.18029416, 0.17378895, 0.17748494, 0.17587207,\n",
       "        0.17802534, 0.17861448, 0.17398655, 0.18079898, 0.1788479 ,\n",
       "        0.17669266, 0.1741783 , 0.18090713, 0.18589821, 0.1862233 ,\n",
       "        0.17368531, 0.17489342, 0.17455147, 0.1745104 , 0.17782275,\n",
       "        0.17856345, 0.1784121 , 0.18121415, 0.17047863, 0.17423916,\n",
       "        0.1718483 , 0.16989632, 0.17658333, 0.1777005 , 0.1804573 ,\n",
       "        0.17846783, 0.17343172, 0.1799243 , 0.18260615, 0.18329303,\n",
       "        0.16934943, 0.17183617, 0.18050443, 0.17599823, 0.17347501,\n",
       "        0.17572477, 0.1741698 , 0.18576987, 0.17694125, 0.17736854,\n",
       "        0.17616342, 0.17098159, 0.17817056, 0.17868096, 0.17447676,\n",
       "        0.17384347, 0.17656909, 0.17791446, 0.17304133, 0.17775097,\n",
       "        0.1746471 , 0.18394482, 0.16796176, 0.18354164, 0.17501318,\n",
       "        0.18031786, 0.17673501, 0.1740418 , 0.17841333, 0.17939852,\n",
       "        0.17939113, 0.17209685, 0.17877443, 0.17522351, 0.17024729,\n",
       "        0.17367445, 0.17917818, 0.18067986, 0.17875399, 0.1764958 ,\n",
       "        0.17955168, 0.18192159, 0.16846305, 0.17327584, 0.175389  ,\n",
       "        0.1796862 , 0.17531723, 0.16981408, 0.17836079, 0.18179269,\n",
       "        0.1745648 , 0.1760654 , 0.18134287, 0.17610696, 0.17611724,\n",
       "        0.18107869, 0.17232117, 0.17701751, 0.18494888, 0.1776291 ,\n",
       "        0.1801367 , 0.18030417, 0.17155264, 0.17612468, 0.1719982 ,\n",
       "        0.17531003, 0.17997862, 0.16778384, 0.1740981 , 0.17168576,\n",
       "        0.18033825, 0.17644736, 0.17702313, 0.17721283, 0.17754816,\n",
       "        0.18353458, 0.17295124, 0.17308235, 0.17744751, 0.17984289,\n",
       "        0.18130165, 0.17038003, 0.17038232, 0.17101657, 0.16997293,\n",
       "        0.17875464, 0.1721778 , 0.1746217 , 0.16970855, 0.17147744,\n",
       "        0.1767065 , 0.1711725 , 0.18158992, 0.18087162, 0.17973804,\n",
       "        0.18421608, 0.17316765, 0.17813936, 0.17858852, 0.17362371,\n",
       "        0.175133  , 0.17453043, 0.17720601, 0.17825702, 0.17272966,\n",
       "        0.16911937], dtype=float32),\n",
       " 'value_module.dueling_V_0.w': array([[0.3821552 , 0.35572034, 0.35603556, ..., 0.35615686, 0.35609353,\n",
       "         0.35616228],\n",
       "        [0.38401127, 0.35396916, 0.35445458, ..., 0.3543536 , 0.35450605,\n",
       "         0.35438734],\n",
       "        [0.38130942, 0.35382336, 0.35390636, ..., 0.35418493, 0.35644332,\n",
       "         0.35450172],\n",
       "        ...,\n",
       "        [0.38186973, 0.35592225, 0.35636497, ..., 0.3563827 , 0.35628912,\n",
       "         0.35642758],\n",
       "        [0.38359594, 0.3541908 , 0.35492006, ..., 0.35477737, 0.3548735 ,\n",
       "         0.3548265 ],\n",
       "        [0.35567904, 0.38196275, 0.3827183 , ..., 0.38293433, 0.38288036,\n",
       "         0.3827646 ]], dtype=float32),\n",
       " 'value_module.dueling_V_0.b': array([ 0.01322359, -0.0144971 , -0.01330221, -0.01421168,  0.01400915,\n",
       "         0.01383436,  0.01487376,  0.01437962, -0.01359181, -0.0132431 ,\n",
       "         0.01433772,  0.01342871, -0.01376848, -0.01393131, -0.01335633,\n",
       "         0.0145011 ,  0.01470775,  0.01462192,  0.01486325, -0.01360096,\n",
       "         0.01471459, -0.00934593, -0.01393588,  0.0160842 , -0.01389923,\n",
       "         0.01434095,  0.01440819, -0.01350082, -0.01128817,  0.01459887,\n",
       "         0.01351247, -0.01267961,  0.00515585,  0.01408033, -0.01294527,\n",
       "        -0.01309833, -0.01395807,  0.01386017, -0.01358716, -0.01395223,\n",
       "        -0.01360073,  0.01420072,  0.01559568, -0.01354429,  0.01456461,\n",
       "        -0.01433061,  0.01558491, -0.01393895,  0.01629921,  0.01428033,\n",
       "        -0.014751  ,  0.01484026, -0.01414941,  0.01473279, -0.01292332,\n",
       "        -0.00194449,  0.01659436,  0.01518199, -0.01260835,  0.01388317,\n",
       "        -0.01397166,  0.01531808, -0.01282715, -0.01275235, -0.01344672,\n",
       "        -0.01286666,  0.01419371,  0.01382941, -0.01313841, -0.01377297,\n",
       "         0.01381065, -0.01313129, -0.01271482,  0.01413783,  0.01549086,\n",
       "        -0.0134572 ,  0.01384327,  0.0144185 ,  0.01437331,  0.01442247,\n",
       "        -0.01513439, -0.01379057, -0.01280285,  0.01362509,  0.01473881,\n",
       "        -0.01297599,  0.01437093, -0.01362241, -0.01371134,  0.014103  ,\n",
       "        -0.0138594 ,  0.0147166 ,  0.01487207,  0.01456093,  0.01569554,\n",
       "         0.01463248,  0.0179854 ,  0.01388626,  0.01492417,  0.01464091,\n",
       "         0.01374511,  0.01395314, -0.01294713,  0.01392976,  0.01338086,\n",
       "        -0.01265473, -0.01395905, -0.01294143, -0.01344752, -0.01288866,\n",
       "         0.01401585,  0.01383993, -0.01372171, -0.01306136,  0.01360708,\n",
       "         0.01384583,  0.01370654,  0.01467533,  0.01413949,  0.01426933,\n",
       "         0.01459455,  0.01469578,  0.01472062,  0.01423268,  0.01440457,\n",
       "        -0.01374354, -0.01244561,  0.0141085 ,  0.01364323,  0.01386833,\n",
       "        -0.01331831, -0.01327596, -0.01394796,  0.01391434, -0.01353015,\n",
       "         0.01476421, -0.01324415,  0.01456484,  0.01473426, -0.01257831,\n",
       "        -0.01312247, -0.0128081 ,  0.01393251, -0.01361947, -0.01410253,\n",
       "        -0.01450782, -0.01338756,  0.01461514, -0.01500603, -0.01385856,\n",
       "        -0.01414166, -0.01378187,  0.01418356, -0.01415079, -0.0127189 ,\n",
       "         0.0152229 , -0.01325017,  0.01359182,  0.01474708,  0.01440447,\n",
       "         0.01455841,  0.01508101,  0.01389901,  0.01375836,  0.01374225,\n",
       "        -0.01341979,  0.01402804, -0.01331721, -0.01198658,  0.0143373 ,\n",
       "         0.01461958,  0.01408595,  0.01571915,  0.01373742,  0.01531857,\n",
       "         0.01572741,  0.01414389, -0.01326576,  0.01393396,  0.01467619,\n",
       "         0.01374969, -0.01323468,  0.01325475,  0.01371505, -0.01307162,\n",
       "        -0.01386189, -0.01428163, -0.01351404,  0.0137995 , -0.01324738,\n",
       "         0.01463139, -0.01422979, -0.01311765, -0.01391134, -0.01343387,\n",
       "         0.01459344, -0.01360553, -0.01317349,  0.01368073, -0.01348636,\n",
       "         0.01415357, -0.01408079,  0.01382217, -0.01314489, -0.01336836,\n",
       "        -0.01415741, -0.01148043,  0.01530882,  0.0150172 ,  0.0157536 ,\n",
       "         0.0154782 ,  0.01374849,  0.01496855, -0.01295845, -0.01201718,\n",
       "        -0.01463925,  0.01466498,  0.01436389,  0.01467326, -0.01318261,\n",
       "        -0.00960267,  0.01368603,  0.01426737,  0.01403593,  0.01467769,\n",
       "         0.01471389,  0.0141559 ,  0.01479579, -0.0133274 , -0.01400214,\n",
       "         0.01374259,  0.01452074, -0.0061869 , -0.01286892, -0.01306203,\n",
       "         0.01409547, -0.01354535, -0.01389635, -0.01260216, -0.01387348,\n",
       "         0.01461203, -0.01336546,  0.01404191,  0.01824567,  0.00526463,\n",
       "         0.01363577, -0.01145703, -0.01328958,  0.01375903, -0.01217552,\n",
       "        -0.01149396, -0.01169296, -0.0140572 , -0.01354051, -0.01295722,\n",
       "        -0.01329658], dtype=float32),\n",
       " 'value_module.V._model.0.weight': array([[ 1.35526031e-01, -1.37578860e-01, -2.43879091e-02,\n",
       "         -3.44518572e-02,  9.45333764e-02,  1.05262518e-01,\n",
       "          1.15862779e-01,  3.63021642e-02, -4.24813293e-02,\n",
       "         -1.28937304e-01,  1.54044375e-01,  1.49307325e-01,\n",
       "         -1.24973901e-01, -4.06055637e-02, -1.14375494e-01,\n",
       "          9.08592492e-02,  5.69291934e-02,  7.27491155e-02,\n",
       "          7.11519867e-02, -8.56698975e-02,  1.38112485e-01,\n",
       "          5.69478935e-03, -1.30679220e-01,  2.39193086e-02,\n",
       "         -1.30599171e-01,  1.00033686e-01,  1.50934011e-01,\n",
       "         -1.01996161e-01, -9.93768219e-04,  1.54788345e-01,\n",
       "          1.44014612e-01, -2.16549188e-02,  1.19592985e-02,\n",
       "          5.37343957e-02, -6.32512718e-02, -2.36361008e-02,\n",
       "         -1.23316996e-01,  1.52516812e-01, -1.22812562e-01,\n",
       "         -4.90345098e-02, -1.19442955e-01,  4.59880047e-02,\n",
       "          4.20125797e-02, -4.72532101e-02,  4.29329984e-02,\n",
       "         -7.05581978e-02,  2.71213371e-02, -9.61172804e-02,\n",
       "          2.04940513e-02,  1.17796086e-01, -4.19591591e-02,\n",
       "          1.19100831e-01, -4.69774082e-02,  1.10980093e-01,\n",
       "         -5.51333055e-02,  9.86100454e-03,  1.92153081e-02,\n",
       "          1.39685515e-02, -4.28045653e-02,  7.98309743e-02,\n",
       "         -1.22988120e-01,  2.89167408e-02, -4.50082049e-02,\n",
       "         -5.70781110e-03, -6.68110102e-02, -1.02440156e-02,\n",
       "          9.35231522e-02,  1.42323256e-01, -2.94148680e-02,\n",
       "         -1.97519455e-02,  9.04629230e-02, -8.75505209e-02,\n",
       "         -1.26873646e-02,  7.90371224e-02,  4.16995808e-02,\n",
       "         -1.13801219e-01,  8.99431854e-02,  1.47695899e-01,\n",
       "          3.73112485e-02,  9.21721458e-02, -6.77900165e-02,\n",
       "         -1.21559627e-01, -4.68829162e-02,  1.56101182e-01,\n",
       "          5.97487427e-02, -9.25443694e-02,  6.60427809e-02,\n",
       "         -6.36197925e-02, -4.55123819e-02,  7.48444200e-02,\n",
       "         -6.16524406e-02,  1.01871774e-01,  6.74060807e-02,\n",
       "          1.06233291e-01,  3.59559618e-02,  4.12562862e-02,\n",
       "          1.52159361e-02,  1.11335479e-01,  5.67948855e-02,\n",
       "          1.04386061e-01,  1.00272365e-01,  9.20611471e-02,\n",
       "         -9.40539911e-02,  8.95814076e-02,  1.59681767e-01,\n",
       "         -3.78518850e-02, -1.14636816e-01, -4.69808914e-02,\n",
       "         -2.93640662e-02, -4.43133432e-03,  1.15532622e-01,\n",
       "          1.08995773e-01, -9.86800417e-02, -9.60187837e-02,\n",
       "          1.55536398e-01,  6.64348900e-02,  1.36964515e-01,\n",
       "          5.64996079e-02,  8.99915174e-02,  1.42272264e-01,\n",
       "          2.90043838e-02,  9.79507416e-02,  1.30157351e-01,\n",
       "          6.33439645e-02,  7.99256265e-02, -3.18287611e-02,\n",
       "         -2.27729846e-02,  1.58336878e-01,  7.51268491e-02,\n",
       "          1.54677972e-01, -1.29577667e-01, -7.47342929e-02,\n",
       "         -9.15740207e-02,  1.30940646e-01, -4.85463366e-02,\n",
       "          4.67495136e-02, -6.08106442e-02,  6.57996163e-02,\n",
       "          8.27771276e-02, -2.17718128e-02, -1.28309771e-01,\n",
       "         -5.19273765e-02,  1.49553970e-01, -5.14180996e-02,\n",
       "         -7.31379613e-02, -9.37509760e-02, -1.26639947e-01,\n",
       "          3.11049763e-02, -1.13974422e-01, -3.76655161e-02,\n",
       "         -9.25537497e-02, -8.43640417e-02,  5.64211532e-02,\n",
       "         -6.21195622e-02, -1.44015970e-02,  7.28446096e-02,\n",
       "         -7.88226873e-02,  8.23065490e-02,  4.20514010e-02,\n",
       "          1.57542020e-01,  1.44447744e-01,  7.79595524e-02,\n",
       "          7.56716654e-02,  1.63352653e-01,  1.44042790e-01,\n",
       "         -1.71219315e-02,  1.11765161e-01, -1.34817243e-01,\n",
       "         -1.12338364e-02,  3.78586054e-02,  1.39335454e-01,\n",
       "          6.46292120e-02,  2.76934057e-02,  9.35117081e-02,\n",
       "          1.13057271e-01,  2.44295876e-02,  1.02022827e-01,\n",
       "         -6.93323240e-02,  1.17467374e-01,  8.01642239e-02,\n",
       "          1.32322878e-01, -7.53123462e-02,  1.03293270e-01,\n",
       "          1.31305322e-01, -1.06180809e-01, -7.07298517e-02,\n",
       "         -7.80379549e-02, -7.68795013e-02,  1.56397179e-01,\n",
       "         -7.10197017e-02,  1.31314710e-01, -5.46382740e-02,\n",
       "         -1.33769125e-01, -9.10911933e-02, -7.90915415e-02,\n",
       "          1.20535091e-01, -6.56775609e-02, -1.25506625e-01,\n",
       "          1.10845454e-01, -4.82169911e-02,  1.34490505e-01,\n",
       "         -7.69397542e-02,  1.64585590e-01, -1.10047154e-01,\n",
       "         -7.08245412e-02, -9.98394191e-02, -3.15512950e-03,\n",
       "          2.87654996e-02,  2.88693253e-02,  2.44077835e-02,\n",
       "          2.93870140e-02,  1.52004525e-01,  8.53066891e-02,\n",
       "         -1.11908354e-01, -1.27838207e-02, -1.01591766e-01,\n",
       "          6.59742504e-02,  1.18108980e-01,  6.17924295e-02,\n",
       "         -1.03336565e-01,  6.85666455e-03,  1.11555167e-01,\n",
       "          1.17190532e-01,  1.18941762e-01,  1.59516588e-01,\n",
       "          6.21456988e-02,  1.49563700e-01,  4.03152257e-02,\n",
       "         -8.84570107e-02, -5.40761836e-02,  9.14635807e-02,\n",
       "          1.26403138e-01,  6.66380208e-03, -1.94769818e-02,\n",
       "         -1.22583322e-01,  5.67488037e-02, -2.68323943e-02,\n",
       "         -1.36499494e-01, -3.31634171e-02, -1.21195026e-01,\n",
       "          8.98537412e-02, -9.70030949e-02,  5.32839559e-02,\n",
       "          1.70481354e-02,  1.11564025e-02,  1.12295799e-01,\n",
       "         -4.52286750e-03, -5.71659841e-02,  7.85847753e-02,\n",
       "         -2.06694403e-03, -5.77084022e-04, -2.37674467e-05,\n",
       "         -1.02368340e-01, -9.30509716e-02, -8.74913409e-02,\n",
       "         -6.54570982e-02]], dtype=float32),\n",
       " 'value_module.V._model.0.bias': array([0.01329712], dtype=float32)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent0.get_policy().get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf8267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, stop_criteria):\n",
    "    \"\"\"\n",
    "    Train an RLlib PPO agent using tune until any of the configured stopping criteria is met.\n",
    "    :param stop_criteria: Dict with stopping criteria.\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run\n",
    "    :return: Return the path to the saved agent (checkpoint) and tune's ExperimentAnalysis object\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/analysis.html#experimentanalysis-tune-experimentanalysis\n",
    "    \"\"\"\n",
    "    analysis = ray.tune.run(ppo.PPOTrainer, config=self.config, local_dir=self.save_dir, stop=stop_criteria,\n",
    "                            checkpoint_at_end=True)\n",
    "    # list of lists: one list per checkpoint; each checkpoint list contains 1st the path, 2nd the metric value\n",
    "    checkpoints = analysis.get_trial_checkpoints_paths(trial=analysis.get_best_trial('episode_reward_mean'),\n",
    "                                                       metric='episode_reward_mean')\n",
    "    # retriev the checkpoint path; we only have a single checkpoint, so take the first one\n",
    "    checkpoint_path = checkpoints[0][0]\n",
    "    return checkpoint_path, analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40d76276",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_dqn0 = DEFAULT_CONFIG_DQN.copy()\n",
    "# trainer_config_dqn0['num_workers'] = 3\n",
    "trainer_config_dqn0['n_step'] = 3\n",
    "trainer_config_dqn0['noisy'] = True\n",
    "trainer_config_dqn0['v_min'] = -10.\n",
    "trainer_config_dqn0['v_max'] = 10.\n",
    "trainer_config_dqn0['model']['fcnet_hiddens'] = [256,32,8]\n",
    "trainer_config_dqn0['framework'] = 'torch'\n",
    "# trainer_config_dqn0['framework'] = 'tf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52ac55d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_dqn1 = DEFAULT_CONFIG_DQN.copy()\n",
    "# trainer_config_dqn1['num_workers'] = 3\n",
    "trainer_config_dqn1['n_step'] = 3\n",
    "trainer_config_dqn1['noisy'] = True\n",
    "trainer_config_dqn1['v_min'] = -10.\n",
    "trainer_config_dqn1['v_max'] = 10.\n",
    "trainer_config_dqn1['model']['fcnet_hiddens'] = [256,32,8]\n",
    "trainer_config_dqn1['framework'] = 'torch'\n",
    "# trainer_config_dqn1['framework'] = 'tf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2705616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0faacb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up stuff for pre-training with tit-for-tat\n",
    "\n",
    "def env_creator(_):\n",
    "    return envs.TwoAgentSeparateMatrixGameEnv()\n",
    "single_env = envs.TwoAgentSeparateMatrixGameEnv()\n",
    "env_name = 'two_agent_t4t_MG_env'\n",
    "register_env(env_name, env_creator)\n",
    "\n",
    "\n",
    "\n",
    "obs_space = single_env.observation_space\n",
    "act_space = single_env.action_space\n",
    "num_agents = single_env.num_agents\n",
    "# DQNTorchPolicy0 = DQNTorchPolicy.copy()\n",
    "\n",
    "def gen_policy(i):\n",
    "#     return (PPOTorchPolicy, obs_space, act_space, {})\n",
    "    if i == 0:\n",
    "        conf = trainer_config_dqn0\n",
    "#         conf['restore']='wtf_this better not work'#results.get_last_checkpoint()\n",
    "#         return (DQNTorchPolicy0, obs_space, act_space, conf)\n",
    "#         policy0 = copy.copy(agent0.get_policy())\n",
    "#         policy0 = DQNTorchPolicy(obs_space, act_space, conf)\n",
    "        policy0 = DQNTorchPolicy\n",
    "#         policy0 = agent0.get_policy\n",
    "#         policy0.set_weights(agent0.get_weights()['default_policy'])\n",
    "#         policy0 = lambda c: policy0\n",
    "#         return policy0\n",
    "        return (policy0, obs_space, act_space, conf)\n",
    "\n",
    "    elif i == 1:\n",
    "        conf = trainer_config_dqn1\n",
    "        return (DQNTorchPolicy, obs_space, act_space, conf)\n",
    "    else:\n",
    "        print('NO CONF!')\n",
    "    \n",
    "    return (DQNTorchPolicy, obs_space, act_space, conf)\n",
    "\n",
    "policy_graphs = {}\n",
    "\n",
    "# def restore_fn(agent_id):\n",
    "# #     if agent_id == 0:\n",
    "#     return results.get_last_checkpoint()\n",
    "# #     else:\n",
    "# #         return None\n",
    "    \n",
    "# restore_dict = {}\n",
    "# for i in range(num_agents):\n",
    "#     restore_dict['agent-' + str(i)] = restore_fn(i)\n",
    "\n",
    "for i in range(num_agents):\n",
    "    policy_graphs['agent-' + str(i)] = gen_policy(i)\n",
    "def policy_mapping_fn(agent_id):\n",
    "        return 'agent-' + str(agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "07fb38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make config for pretraining with tit-for-tat\n",
    "\n",
    "config={\n",
    "    \"log_level\": \"WARN\",\n",
    "#     \"num_workers\": 3,\n",
    "#     \"num_cpus_for_driver\": 1,\n",
    "#     \"num_cpus_per_worker\": 1,\n",
    "#     \"lr\": 5e-3,\n",
    "#     \"model\":{\"fcnet_hiddens\": [1024, 512,256,32,8]},\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policy_graphs,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "    },\n",
    "    \"env\": \"two_agent_t4t_MG_env\",\n",
    "    'framework': 'torch',\n",
    "#     'framework': 'tf',\n",
    "#             \"resume\":True,\n",
    "#         \"restore\": results.get_last_checkpoint()\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b559382c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.3/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-41-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 474.0\n",
      "  episode_reward_mean: 455.6\n",
      "  episode_reward_min: 420.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 10\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 1000\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.6481744050979614\n",
      "        max_q: 0.3614499270915985\n",
      "        mean_q: 0.04180954396724701\n",
      "        mean_td_error: -3.8718888759613037\n",
      "        min_q: -0.14508198201656342\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 5.041872978210449\n",
      "        max_q: 0.6048080325126648\n",
      "        mean_q: 0.27389317750930786\n",
      "        mean_td_error: -3.382216215133667\n",
      "        min_q: -0.013645298779010773\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 64\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 32\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.78333333333333\n",
      "    ram_util_percent: 62.4\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 238.0\n",
      "    agent-1: 242.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 227.9\n",
      "    agent-1: 227.7\n",
      "  policy_reward_min:\n",
      "    agent-0: 210.0\n",
      "    agent-1: 210.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07478888337309664\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07380781831083957\n",
      "    mean_inference_ms: 2.9879959670456495\n",
      "    mean_raw_obs_processing_ms: 0.2266348420561372\n",
      "  time_since_restore: 3.6069395542144775\n",
      "  time_this_iter_s: 3.6069395542144775\n",
      "  time_total_s: 3.6069395542144775\n",
      "  timers:\n",
      "    learn_throughput: 1585.505\n",
      "    learn_time_ms: 20.183\n",
      "  timestamp: 1628739700\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.60694</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">   455.6</td><td style=\"text-align: right;\">                 474</td><td style=\"text-align: right;\">                 420</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-41-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 481.0\n",
      "  episode_reward_mean: 457.3\n",
      "  episode_reward_min: 416.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 20\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 1504\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.2518510818481445\n",
      "        max_q: 11.82480525970459\n",
      "        mean_q: 10.3193998336792\n",
      "        mean_td_error: 1.7746717929840088\n",
      "        min_q: 7.780777454376221\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.8816297054290771\n",
      "        max_q: 12.973074913024902\n",
      "        mean_q: 8.467057228088379\n",
      "        mean_td_error: 1.6628128290176392\n",
      "        min_q: 4.246359825134277\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 16064\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 8032\n",
      "    num_target_updates: 2\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.914285714285715\n",
      "    ram_util_percent: 62.8\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 251.0\n",
      "    agent-1: 242.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 229.8\n",
      "    agent-1: 227.5\n",
      "  policy_reward_min:\n",
      "    agent-0: 210.0\n",
      "    agent-1: 200.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07571507505196401\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07535661997957313\n",
      "    mean_inference_ms: 3.042840078111974\n",
      "    mean_raw_obs_processing_ms: 0.22984572284853294\n",
      "  time_since_restore: 13.490777254104614\n",
      "  time_this_iter_s: 9.883837699890137\n",
      "  time_total_s: 13.490777254104614\n",
      "  timers:\n",
      "    learn_throughput: 1870.641\n",
      "    learn_time_ms: 17.106\n",
      "  timestamp: 1628739710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         13.4908</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">   457.3</td><td style=\"text-align: right;\">                 481</td><td style=\"text-align: right;\">                 416</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-41-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 488.0\n",
      "  episode_reward_mean: 462.73333333333335\n",
      "  episode_reward_min: 416.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 30\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 2512\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.32697227597236633\n",
      "        max_q: 20.253963470458984\n",
      "        mean_q: 16.4215030670166\n",
      "        mean_td_error: 0.10050493478775024\n",
      "        min_q: 3.997117757797241\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.0108208656311035\n",
      "        max_q: 22.64305877685547\n",
      "        mean_q: 17.926467895507812\n",
      "        mean_td_error: 1.5186495780944824\n",
      "        min_q: 5.166675567626953\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 32064\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 16032\n",
      "    num_target_updates: 4\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.307692307692307\n",
      "    ram_util_percent: 62.93076923076922\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 252.0\n",
      "    agent-1: 251.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 232.93333333333334\n",
      "    agent-1: 229.8\n",
      "  policy_reward_min:\n",
      "    agent-0: 210.0\n",
      "    agent-1: 200.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07582763207137044\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07600721202229906\n",
      "    mean_inference_ms: 3.053577268507199\n",
      "    mean_raw_obs_processing_ms: 0.23021566366067323\n",
      "  time_since_restore: 22.782445430755615\n",
      "  time_this_iter_s: 9.291668176651001\n",
      "  time_total_s: 22.782445430755615\n",
      "  timers:\n",
      "    learn_throughput: 2095.113\n",
      "    learn_time_ms: 15.274\n",
      "  timestamp: 1628739719\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         22.7824</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\"> 462.733</td><td style=\"text-align: right;\">                 488</td><td style=\"text-align: right;\">                 416</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-42-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 513.0\n",
      "  episode_reward_mean: 468.925\n",
      "  episode_reward_min: 416.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 40\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 3520\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.544997215270996\n",
      "        max_q: 32.24666976928711\n",
      "        mean_q: 24.94795799255371\n",
      "        mean_td_error: 0.551760196685791\n",
      "        min_q: 6.551080703735352\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.52547287940979\n",
      "        max_q: 31.847286224365234\n",
      "        mean_q: 25.046173095703125\n",
      "        mean_td_error: 2.240262031555176\n",
      "        min_q: 9.48935604095459\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 48064\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 24032\n",
      "    num_target_updates: 6\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.385714285714283\n",
      "    ram_util_percent: 62.828571428571415\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 257.0\n",
      "    agent-1: 261.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 236.925\n",
      "    agent-1: 232.0\n",
      "  policy_reward_min:\n",
      "    agent-0: 210.0\n",
      "    agent-1: 200.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0759161585080932\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07648364380620959\n",
      "    mean_inference_ms: 3.059595594066476\n",
      "    mean_raw_obs_processing_ms: 0.23030864585927144\n",
      "  time_since_restore: 32.246784687042236\n",
      "  time_this_iter_s: 9.464339256286621\n",
      "  time_total_s: 32.246784687042236\n",
      "  timers:\n",
      "    learn_throughput: 2155.502\n",
      "    learn_time_ms: 14.846\n",
      "  timestamp: 1628739729\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         32.2468</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 468.925</td><td style=\"text-align: right;\">                 513</td><td style=\"text-align: right;\">                 416</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-42-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 526.0\n",
      "  episode_reward_mean: 474.98\n",
      "  episode_reward_min: 416.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 50\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 4528\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.5623810291290283\n",
      "        max_q: 37.224159240722656\n",
      "        mean_q: 30.01862335205078\n",
      "        mean_td_error: 0.15802502632141113\n",
      "        min_q: 2.2900261878967285\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.3078413009643555\n",
      "        max_q: 36.924312591552734\n",
      "        mean_q: 30.194814682006836\n",
      "        mean_td_error: 1.1953985691070557\n",
      "        min_q: 14.937613487243652\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 64064\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 32032\n",
      "    num_target_updates: 8\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.185714285714283\n",
      "    ram_util_percent: 63.142857142857125\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 267.0\n",
      "    agent-1: 269.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 239.66\n",
      "    agent-1: 235.32\n",
      "  policy_reward_min:\n",
      "    agent-0: 210.0\n",
      "    agent-1: 200.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0760948637269518\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07698363096118385\n",
      "    mean_inference_ms: 3.069547559639208\n",
      "    mean_raw_obs_processing_ms: 0.2306951275030177\n",
      "  time_since_restore: 42.154260873794556\n",
      "  time_this_iter_s: 9.90747618675232\n",
      "  time_total_s: 42.154260873794556\n",
      "  timers:\n",
      "    learn_throughput: 1931.496\n",
      "    learn_time_ms: 16.567\n",
      "  timestamp: 1628739739\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         42.1543</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">  474.98</td><td style=\"text-align: right;\">                 526</td><td style=\"text-align: right;\">                 416</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-42-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 539.0\n",
      "  episode_reward_mean: 479.0\n",
      "  episode_reward_min: 416.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 60\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 5536\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.9839478731155396\n",
      "        max_q: 44.71141052246094\n",
      "        mean_q: 38.80061340332031\n",
      "        mean_td_error: 2.3029065132141113\n",
      "        min_q: 6.2857770919799805\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.8328343033790588\n",
      "        max_q: 41.4616813659668\n",
      "        mean_q: 30.310789108276367\n",
      "        mean_td_error: -0.9842209219932556\n",
      "        min_q: 12.983595848083496\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 80064\n",
      "    num_steps_sampled: 6000\n",
      "    num_steps_trained: 40032\n",
      "    num_target_updates: 10\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.72307692307692\n",
      "    ram_util_percent: 63.15384615384616\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 272.0\n",
      "    agent-1: 277.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 240.68333333333334\n",
      "    agent-1: 238.31666666666666\n",
      "  policy_reward_min:\n",
      "    agent-0: 210.0\n",
      "    agent-1: 200.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07617482584038363\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07735173160462402\n",
      "    mean_inference_ms: 3.075096871845771\n",
      "    mean_raw_obs_processing_ms: 0.2307212195122874\n",
      "  time_since_restore: 51.57110071182251\n",
      "  time_this_iter_s: 9.416839838027954\n",
      "  time_total_s: 51.57110071182251\n",
      "  timers:\n",
      "    learn_throughput: 2160.078\n",
      "    learn_time_ms: 14.814\n",
      "  timestamp: 1628739748\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 6000\n",
      "  training_iteration: 6\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         51.5711</td><td style=\"text-align: right;\">6000</td><td style=\"text-align: right;\">     479</td><td style=\"text-align: right;\">                 539</td><td style=\"text-align: right;\">                 416</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 14000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-42-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 553.0\n",
      "  episode_reward_mean: 485.98571428571427\n",
      "  episode_reward_min: 416.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 70\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 6544\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.734700083732605\n",
      "        max_q: 53.000431060791016\n",
      "        mean_q: 43.0416259765625\n",
      "        mean_td_error: 0.6992284059524536\n",
      "        min_q: 3.607372522354126\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.8971948027610779\n",
      "        max_q: 47.192562103271484\n",
      "        mean_q: 36.55112075805664\n",
      "        mean_td_error: -1.0066094398498535\n",
      "        min_q: 17.14791488647461\n",
      "    num_agent_steps_sampled: 14000\n",
      "    num_agent_steps_trained: 96064\n",
      "    num_steps_sampled: 7000\n",
      "    num_steps_trained: 48032\n",
      "    num_target_updates: 12\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.56923076923077\n",
      "    ram_util_percent: 63.092307692307706\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 276.0\n",
      "    agent-1: 282.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 243.8\n",
      "    agent-1: 242.18571428571428\n",
      "  policy_reward_min:\n",
      "    agent-0: 210.0\n",
      "    agent-1: 200.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07612504172616819\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07751881883203145\n",
      "    mean_inference_ms: 3.0741727831850247\n",
      "    mean_raw_obs_processing_ms: 0.23046274621660018\n",
      "  time_since_restore: 60.415770530700684\n",
      "  time_this_iter_s: 8.844669818878174\n",
      "  time_total_s: 60.415770530700684\n",
      "  timers:\n",
      "    learn_throughput: 2198.409\n",
      "    learn_time_ms: 14.556\n",
      "  timestamp: 1628739757\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7000\n",
      "  training_iteration: 7\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         60.4158</td><td style=\"text-align: right;\">7000</td><td style=\"text-align: right;\"> 485.986</td><td style=\"text-align: right;\">                 553</td><td style=\"text-align: right;\">                 416</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-42-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 564.0\n",
      "  episode_reward_mean: 493.6125\n",
      "  episode_reward_min: 416.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 80\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 7552\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.5542335510253906\n",
      "        max_q: 59.67994689941406\n",
      "        mean_q: 49.02157974243164\n",
      "        mean_td_error: 1.0995283126831055\n",
      "        min_q: 4.861546993255615\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.3477234840393066\n",
      "        max_q: 54.24774932861328\n",
      "        mean_q: 42.82968521118164\n",
      "        mean_td_error: -1.37383234500885\n",
      "        min_q: 2.039933443069458\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 112064\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 56032\n",
      "    num_target_updates: 14\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.075\n",
      "    ram_util_percent: 63.10833333333334\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 291.0\n",
      "    agent-1: 282.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 247.8375\n",
      "    agent-1: 245.775\n",
      "  policy_reward_min:\n",
      "    agent-0: 210.0\n",
      "    agent-1: 200.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07600886508583754\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07757367032965752\n",
      "    mean_inference_ms: 3.0696095241970958\n",
      "    mean_raw_obs_processing_ms: 0.23004447009193402\n",
      "  time_since_restore: 69.24762463569641\n",
      "  time_this_iter_s: 8.831854104995728\n",
      "  time_total_s: 69.24762463569641\n",
      "  timers:\n",
      "    learn_throughput: 2130.055\n",
      "    learn_time_ms: 15.023\n",
      "  timestamp: 1628739766\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 8\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         69.2476</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> 493.613</td><td style=\"text-align: right;\">                 564</td><td style=\"text-align: right;\">                 416</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 18000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-42-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 580.0\n",
      "  episode_reward_mean: 499.2\n",
      "  episode_reward_min: 416.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 90\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 8560\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.1582638025283813\n",
      "        max_q: 63.62294006347656\n",
      "        mean_q: 51.55921173095703\n",
      "        mean_td_error: 2.304328441619873\n",
      "        min_q: 18.765295028686523\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.511793851852417\n",
      "        max_q: 60.742794036865234\n",
      "        mean_q: 47.63117599487305\n",
      "        mean_td_error: -3.037719249725342\n",
      "        min_q: 9.263669967651367\n",
      "    num_agent_steps_sampled: 18000\n",
      "    num_agent_steps_trained: 128064\n",
      "    num_steps_sampled: 9000\n",
      "    num_steps_trained: 64032\n",
      "    num_target_updates: 16\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.646153846153847\n",
      "    ram_util_percent: 63.26923076923076\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 293.0\n",
      "    agent-1: 287.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 250.45555555555555\n",
      "    agent-1: 248.74444444444444\n",
      "  policy_reward_min:\n",
      "    agent-0: 205.0\n",
      "    agent-1: 200.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07586977540843821\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07756773887137275\n",
      "    mean_inference_ms: 3.064122175525691\n",
      "    mean_raw_obs_processing_ms: 0.22956676222515568\n",
      "  time_since_restore: 78.329176902771\n",
      "  time_this_iter_s: 9.081552267074585\n",
      "  time_total_s: 78.329176902771\n",
      "  timers:\n",
      "    learn_throughput: 2226.523\n",
      "    learn_time_ms: 14.372\n",
      "  timestamp: 1628739775\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 9000\n",
      "  training_iteration: 9\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         78.3292</td><td style=\"text-align: right;\">9000</td><td style=\"text-align: right;\">   499.2</td><td style=\"text-align: right;\">                 580</td><td style=\"text-align: right;\">                 416</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-43-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 584.0\n",
      "  episode_reward_mean: 505.8\n",
      "  episode_reward_min: 416.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 100\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 9568\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.1007777452468872\n",
      "        max_q: 72.11653900146484\n",
      "        mean_q: 54.545631408691406\n",
      "        mean_td_error: 1.0295968055725098\n",
      "        min_q: 3.4416556358337402\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.9678183794021606\n",
      "        max_q: 71.53487396240234\n",
      "        mean_q: 58.572078704833984\n",
      "        mean_td_error: 0.8106950521469116\n",
      "        min_q: 4.461781978607178\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 144064\n",
      "    num_steps_sampled: 10000\n",
      "    num_steps_trained: 72032\n",
      "    num_target_updates: 18\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.192307692307693\n",
      "    ram_util_percent: 63.369230769230754\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 298.0\n",
      "    agent-1: 296.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 253.65\n",
      "    agent-1: 252.15\n",
      "  policy_reward_min:\n",
      "    agent-0: 205.0\n",
      "    agent-1: 200.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0757223086047836\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07752096077695356\n",
      "    mean_inference_ms: 3.0582356781824727\n",
      "    mean_raw_obs_processing_ms: 0.22907462545644017\n",
      "  time_since_restore: 87.25712466239929\n",
      "  time_this_iter_s: 8.927947759628296\n",
      "  time_total_s: 87.25712466239929\n",
      "  timers:\n",
      "    learn_throughput: 1972.496\n",
      "    learn_time_ms: 16.223\n",
      "  timestamp: 1628739784\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 10000\n",
      "  training_iteration: 10\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         87.2571</td><td style=\"text-align: right;\">10000</td><td style=\"text-align: right;\">   505.8</td><td style=\"text-align: right;\">                 584</td><td style=\"text-align: right;\">                 416</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 22000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-43-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 594.0\n",
      "  episode_reward_mean: 517.77\n",
      "  episode_reward_min: 416.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 110\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 10576\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.268284320831299\n",
      "        max_q: 73.69377899169922\n",
      "        mean_q: 55.32630157470703\n",
      "        mean_td_error: -3.517836570739746\n",
      "        min_q: 1.134737253189087\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.845705270767212\n",
      "        max_q: 79.38607788085938\n",
      "        mean_q: 63.53093338012695\n",
      "        mean_td_error: 3.2871274948120117\n",
      "        min_q: 5.225124359130859\n",
      "    num_agent_steps_sampled: 22000\n",
      "    num_agent_steps_trained: 160064\n",
      "    num_steps_sampled: 11000\n",
      "    num_steps_trained: 80032\n",
      "    num_target_updates: 20\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.94615384615384\n",
      "    ram_util_percent: 63.8076923076923\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 298.0\n",
      "    agent-1: 301.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 259.41\n",
      "    agent-1: 258.36\n",
      "  policy_reward_min:\n",
      "    agent-0: 205.0\n",
      "    agent-1: 200.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07568123254280175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07786589016189766\n",
      "    mean_inference_ms: 3.0598187260417644\n",
      "    mean_raw_obs_processing_ms: 0.2288524281706045\n",
      "  time_since_restore: 96.49752426147461\n",
      "  time_this_iter_s: 9.240399599075317\n",
      "  time_total_s: 96.49752426147461\n",
      "  timers:\n",
      "    learn_throughput: 2287.134\n",
      "    learn_time_ms: 13.991\n",
      "  timestamp: 1628739793\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 11000\n",
      "  training_iteration: 11\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         96.4975</td><td style=\"text-align: right;\">11000</td><td style=\"text-align: right;\">  517.77</td><td style=\"text-align: right;\">                 594</td><td style=\"text-align: right;\">                 416</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-43-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 594.0\n",
      "  episode_reward_mean: 529.55\n",
      "  episode_reward_min: 439.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 120\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 11584\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.785524606704712\n",
      "        max_q: 81.37461853027344\n",
      "        mean_q: 69.53360748291016\n",
      "        mean_td_error: -2.9970529079437256\n",
      "        min_q: 5.811276435852051\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.9152727127075195\n",
      "        max_q: 85.21179962158203\n",
      "        mean_q: 66.71085357666016\n",
      "        mean_td_error: -0.31105470657348633\n",
      "        min_q: 21.378738403320312\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 176064\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 88032\n",
      "    num_target_updates: 22\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.184615384615384\n",
      "    ram_util_percent: 64.0\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 298.0\n",
      "    agent-1: 301.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 264.97\n",
      "    agent-1: 264.58\n",
      "  policy_reward_min:\n",
      "    agent-0: 205.0\n",
      "    agent-1: 211.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07543735519711132\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07788617158189762\n",
      "    mean_inference_ms: 3.0492524185943766\n",
      "    mean_raw_obs_processing_ms: 0.22794138461240582\n",
      "  time_since_restore: 105.47984957695007\n",
      "  time_this_iter_s: 8.982325315475464\n",
      "  time_total_s: 105.47984957695007\n",
      "  timers:\n",
      "    learn_throughput: 2068.768\n",
      "    learn_time_ms: 15.468\n",
      "  timestamp: 1628739802\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 12\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">          105.48</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  529.55</td><td style=\"text-align: right;\">                 594</td><td style=\"text-align: right;\">                 439</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 26000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-43-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 594.0\n",
      "  episode_reward_mean: 539.48\n",
      "  episode_reward_min: 439.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 130\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 12592\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.6593286991119385\n",
      "        max_q: 87.46568298339844\n",
      "        mean_q: 65.27024841308594\n",
      "        mean_td_error: 0.07227236032485962\n",
      "        min_q: 4.805069446563721\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.444260358810425\n",
      "        max_q: 90.92266082763672\n",
      "        mean_q: 73.63475799560547\n",
      "        mean_td_error: 1.5020864009857178\n",
      "        min_q: 17.19365692138672\n",
      "    num_agent_steps_sampled: 26000\n",
      "    num_agent_steps_trained: 192064\n",
      "    num_steps_sampled: 13000\n",
      "    num_steps_trained: 96032\n",
      "    num_target_updates: 24\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.076923076923073\n",
      "    ram_util_percent: 63.630769230769246\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 298.0\n",
      "    agent-1: 301.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 269.7\n",
      "    agent-1: 269.78\n",
      "  policy_reward_min:\n",
      "    agent-0: 205.0\n",
      "    agent-1: 216.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07523648397333242\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07785770264987478\n",
      "    mean_inference_ms: 3.0400288007069105\n",
      "    mean_raw_obs_processing_ms: 0.22718626528480854\n",
      "  time_since_restore: 114.43051481246948\n",
      "  time_this_iter_s: 8.95066523551941\n",
      "  time_total_s: 114.43051481246948\n",
      "  timers:\n",
      "    learn_throughput: 2254.782\n",
      "    learn_time_ms: 14.192\n",
      "  timestamp: 1628739811\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 13000\n",
      "  training_iteration: 13\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         114.431</td><td style=\"text-align: right;\">13000</td><td style=\"text-align: right;\">  539.48</td><td style=\"text-align: right;\">                 594</td><td style=\"text-align: right;\">                 439</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-43-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 595.0\n",
      "  episode_reward_mean: 549.04\n",
      "  episode_reward_min: 439.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 140\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 13600\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.3017842769622803\n",
      "        max_q: 93.8823471069336\n",
      "        mean_q: 71.10501098632812\n",
      "        mean_td_error: -0.3695831298828125\n",
      "        min_q: 22.24057388305664\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.070382833480835\n",
      "        max_q: 95.1711654663086\n",
      "        mean_q: 80.5385513305664\n",
      "        mean_td_error: 0.43057870864868164\n",
      "        min_q: 28.246768951416016\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 208064\n",
      "    num_steps_sampled: 14000\n",
      "    num_steps_trained: 104032\n",
      "    num_target_updates: 26\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.42307692307692\n",
      "    ram_util_percent: 63.79230769230768\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 300.0\n",
      "    agent-1: 301.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 273.93\n",
      "    agent-1: 275.11\n",
      "  policy_reward_min:\n",
      "    agent-0: 205.0\n",
      "    agent-1: 216.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07501614699970215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07776125641099833\n",
      "    mean_inference_ms: 3.0298945858033353\n",
      "    mean_raw_obs_processing_ms: 0.22644511639096024\n",
      "  time_since_restore: 123.37574625015259\n",
      "  time_this_iter_s: 8.945231437683105\n",
      "  time_total_s: 123.37574625015259\n",
      "  timers:\n",
      "    learn_throughput: 2230.116\n",
      "    learn_time_ms: 14.349\n",
      "  timestamp: 1628739820\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 14000\n",
      "  training_iteration: 14\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         123.376</td><td style=\"text-align: right;\">14000</td><td style=\"text-align: right;\">  549.04</td><td style=\"text-align: right;\">                 595</td><td style=\"text-align: right;\">                 439</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 30000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-43-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 596.0\n",
      "  episode_reward_mean: 556.8\n",
      "  episode_reward_min: 439.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 150\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 14608\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.797641396522522\n",
      "        max_q: 99.42556762695312\n",
      "        mean_q: 66.24397277832031\n",
      "        mean_td_error: 0.484588623046875\n",
      "        min_q: 6.423999786376953\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.236603021621704\n",
      "        max_q: 99.19448852539062\n",
      "        mean_q: 81.52855682373047\n",
      "        mean_td_error: -1.7514609098434448\n",
      "        min_q: 20.538043975830078\n",
      "    num_agent_steps_sampled: 30000\n",
      "    num_agent_steps_trained: 224064\n",
      "    num_steps_sampled: 15000\n",
      "    num_steps_trained: 112032\n",
      "    num_target_updates: 28\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.146153846153847\n",
      "    ram_util_percent: 64.0923076923077\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 301.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 278.62\n",
      "    agent-1: 278.18\n",
      "  policy_reward_min:\n",
      "    agent-0: 205.0\n",
      "    agent-1: 197.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07472529147389012\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07755575895774652\n",
      "    mean_inference_ms: 3.0160858670320545\n",
      "    mean_raw_obs_processing_ms: 0.22550716740045032\n",
      "  time_since_restore: 132.4023871421814\n",
      "  time_this_iter_s: 9.026640892028809\n",
      "  time_total_s: 132.4023871421814\n",
      "  timers:\n",
      "    learn_throughput: 2173.211\n",
      "    learn_time_ms: 14.725\n",
      "  timestamp: 1628739829\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15000\n",
      "  training_iteration: 15\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         132.402</td><td style=\"text-align: right;\">15000</td><td style=\"text-align: right;\">   556.8</td><td style=\"text-align: right;\">                 596</td><td style=\"text-align: right;\">                 439</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-43-58\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 596.0\n",
      "  episode_reward_mean: 564.9\n",
      "  episode_reward_min: 460.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 160\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 15616\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.580039620399475\n",
      "        max_q: 103.74931335449219\n",
      "        mean_q: 84.22549438476562\n",
      "        mean_td_error: -1.4046964645385742\n",
      "        min_q: 2.6974728107452393\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.9241063594818115\n",
      "        max_q: 104.74608612060547\n",
      "        mean_q: 85.57647705078125\n",
      "        mean_td_error: 1.0935627222061157\n",
      "        min_q: 18.519960403442383\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 240064\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 120032\n",
      "    num_target_updates: 30\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.553846153846152\n",
      "    ram_util_percent: 64.19230769230771\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 301.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 283.21\n",
      "    agent-1: 281.69\n",
      "  policy_reward_min:\n",
      "    agent-0: 205.0\n",
      "    agent-1: 197.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0744412647246343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07731359882240443\n",
      "    mean_inference_ms: 3.0022444411550118\n",
      "    mean_raw_obs_processing_ms: 0.2246631599239481\n",
      "  time_since_restore: 141.3273630142212\n",
      "  time_this_iter_s: 8.924975872039795\n",
      "  time_total_s: 141.3273630142212\n",
      "  timers:\n",
      "    learn_throughput: 2234.787\n",
      "    learn_time_ms: 14.319\n",
      "  timestamp: 1628739838\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 16\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         141.327</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">   564.9</td><td style=\"text-align: right;\">                 596</td><td style=\"text-align: right;\">                 460</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 34000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-44-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 596.0\n",
      "  episode_reward_mean: 571.13\n",
      "  episode_reward_min: 460.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 170\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 16624\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.8578619956970215\n",
      "        max_q: 105.50225067138672\n",
      "        mean_q: 88.30193328857422\n",
      "        mean_td_error: -4.942717552185059\n",
      "        min_q: 19.582843780517578\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.4686153531074524\n",
      "        max_q: 109.93115997314453\n",
      "        mean_q: 79.34584045410156\n",
      "        mean_td_error: 1.0892850160598755\n",
      "        min_q: 5.97398567199707\n",
      "    num_agent_steps_sampled: 34000\n",
      "    num_agent_steps_trained: 256064\n",
      "    num_steps_sampled: 17000\n",
      "    num_steps_trained: 128032\n",
      "    num_target_updates: 32\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.15833333333333\n",
      "    ram_util_percent: 64.03333333333333\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 301.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 286.24\n",
      "    agent-1: 284.89\n",
      "  policy_reward_min:\n",
      "    agent-0: 205.0\n",
      "    agent-1: 197.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07422064923350413\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07712333684633794\n",
      "    mean_inference_ms: 2.9913075052845968\n",
      "    mean_raw_obs_processing_ms: 0.22399049972043614\n",
      "  time_since_restore: 150.3324465751648\n",
      "  time_this_iter_s: 9.005083560943604\n",
      "  time_total_s: 150.3324465751648\n",
      "  timers:\n",
      "    learn_throughput: 2248.877\n",
      "    learn_time_ms: 14.229\n",
      "  timestamp: 1628739847\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 17000\n",
      "  training_iteration: 17\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         150.332</td><td style=\"text-align: right;\">17000</td><td style=\"text-align: right;\">  571.13</td><td style=\"text-align: right;\">                 596</td><td style=\"text-align: right;\">                 460</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-44-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 597.0\n",
      "  episode_reward_mean: 575.35\n",
      "  episode_reward_min: 460.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 180\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 17632\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.0989890098571777\n",
      "        max_q: 108.71524047851562\n",
      "        mean_q: 84.53984832763672\n",
      "        mean_td_error: -1.036083459854126\n",
      "        min_q: 26.84507942199707\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.396727442741394\n",
      "        max_q: 109.60945129394531\n",
      "        mean_q: 85.19401550292969\n",
      "        mean_td_error: -2.3754775524139404\n",
      "        min_q: 4.959340572357178\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 272064\n",
      "    num_steps_sampled: 18000\n",
      "    num_steps_trained: 136032\n",
      "    num_target_updates: 34\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.11538461538462\n",
      "    ram_util_percent: 64.0\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 301.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 287.98\n",
      "    agent-1: 287.37\n",
      "  policy_reward_min:\n",
      "    agent-0: 205.0\n",
      "    agent-1: 197.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07405341019823\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07697869099566644\n",
      "    mean_inference_ms: 2.983014833500714\n",
      "    mean_raw_obs_processing_ms: 0.22346973637242443\n",
      "  time_since_restore: 159.4616415500641\n",
      "  time_this_iter_s: 9.129194974899292\n",
      "  time_total_s: 159.4616415500641\n",
      "  timers:\n",
      "    learn_throughput: 2208.066\n",
      "    learn_time_ms: 14.492\n",
      "  timestamp: 1628739857\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 18000\n",
      "  training_iteration: 18\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         159.462</td><td style=\"text-align: right;\">18000</td><td style=\"text-align: right;\">  575.35</td><td style=\"text-align: right;\">                 597</td><td style=\"text-align: right;\">                 460</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 38000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-44-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 599.0\n",
      "  episode_reward_mean: 580.4\n",
      "  episode_reward_min: 498.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 190\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 18640\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.607286214828491\n",
      "        max_q: 113.39686584472656\n",
      "        mean_q: 85.75332641601562\n",
      "        mean_td_error: -1.314283847808838\n",
      "        min_q: 3.6033332347869873\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.4703963100910187\n",
      "        max_q: 115.02317810058594\n",
      "        mean_q: 87.5347900390625\n",
      "        mean_td_error: 1.5741628408432007\n",
      "        min_q: 6.510323524475098\n",
      "    num_agent_steps_sampled: 38000\n",
      "    num_agent_steps_trained: 288064\n",
      "    num_steps_sampled: 19000\n",
      "    num_steps_trained: 144032\n",
      "    num_target_updates: 36\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.342857142857145\n",
      "    ram_util_percent: 64.05714285714286\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 290.51\n",
      "    agent-1: 289.89\n",
      "  policy_reward_min:\n",
      "    agent-0: 253.0\n",
      "    agent-1: 197.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07393489570726529\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07688808728411199\n",
      "    mean_inference_ms: 2.976574044493653\n",
      "    mean_raw_obs_processing_ms: 0.22308469214974566\n",
      "  time_since_restore: 168.73335933685303\n",
      "  time_this_iter_s: 9.27171778678894\n",
      "  time_total_s: 168.73335933685303\n",
      "  timers:\n",
      "    learn_throughput: 1866.954\n",
      "    learn_time_ms: 17.14\n",
      "  timestamp: 1628739866\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 19000\n",
      "  training_iteration: 19\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         168.733</td><td style=\"text-align: right;\">19000</td><td style=\"text-align: right;\">   580.4</td><td style=\"text-align: right;\">                 599</td><td style=\"text-align: right;\">                 498</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-44-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 599.0\n",
      "  episode_reward_mean: 582.81\n",
      "  episode_reward_min: 498.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 200\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 19648\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.3590688705444336\n",
      "        max_q: 116.22228240966797\n",
      "        mean_q: 95.60297393798828\n",
      "        mean_td_error: -0.7150129079818726\n",
      "        min_q: 3.611713171005249\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.6466624736785889\n",
      "        max_q: 119.38143157958984\n",
      "        mean_q: 92.87417602539062\n",
      "        mean_td_error: -0.7132628560066223\n",
      "        min_q: 2.0798895359039307\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 304064\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 152032\n",
      "    num_target_updates: 38\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.01875\n",
      "    ram_util_percent: 64.43125\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 291.42\n",
      "    agent-1: 291.39\n",
      "  policy_reward_min:\n",
      "    agent-0: 253.0\n",
      "    agent-1: 197.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07392380864815058\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07692856561243273\n",
      "    mean_inference_ms: 2.9747180641066353\n",
      "    mean_raw_obs_processing_ms: 0.22302410137287093\n",
      "  time_since_restore: 179.82445621490479\n",
      "  time_this_iter_s: 11.091096878051758\n",
      "  time_total_s: 179.82445621490479\n",
      "  timers:\n",
      "    learn_throughput: 1608.7\n",
      "    learn_time_ms: 19.892\n",
      "  timestamp: 1628739877\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 20\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         179.824</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  582.81</td><td style=\"text-align: right;\">                 599</td><td style=\"text-align: right;\">                 498</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 42000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-44-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 599.0\n",
      "  episode_reward_mean: 584.14\n",
      "  episode_reward_min: 498.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 210\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 20656\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.4212651252746582\n",
      "        max_q: 123.25554656982422\n",
      "        mean_q: 96.43870544433594\n",
      "        mean_td_error: 1.7206447124481201\n",
      "        min_q: 6.464217662811279\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.0590656995773315\n",
      "        max_q: 123.25466918945312\n",
      "        mean_q: 94.09251403808594\n",
      "        mean_td_error: -1.8008642196655273\n",
      "        min_q: 3.924022912979126\n",
      "    num_agent_steps_sampled: 42000\n",
      "    num_agent_steps_trained: 320064\n",
      "    num_steps_sampled: 21000\n",
      "    num_steps_trained: 160032\n",
      "    num_target_updates: 40\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.692857142857143\n",
      "    ram_util_percent: 64.55000000000001\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 292.12\n",
      "    agent-1: 292.02\n",
      "  policy_reward_min:\n",
      "    agent-0: 254.0\n",
      "    agent-1: 197.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07395747790422853\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07700154530492215\n",
      "    mean_inference_ms: 2.974894787706296\n",
      "    mean_raw_obs_processing_ms: 0.22309614582372092\n",
      "  time_since_restore: 190.19582056999207\n",
      "  time_this_iter_s: 10.37136435508728\n",
      "  time_total_s: 190.19582056999207\n",
      "  timers:\n",
      "    learn_throughput: 2221.471\n",
      "    learn_time_ms: 14.405\n",
      "  timestamp: 1628739887\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 21000\n",
      "  training_iteration: 21\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         190.196</td><td style=\"text-align: right;\">21000</td><td style=\"text-align: right;\">  584.14</td><td style=\"text-align: right;\">                 599</td><td style=\"text-align: right;\">                 498</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 44000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-44-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 599.0\n",
      "  episode_reward_mean: 584.79\n",
      "  episode_reward_min: 498.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 220\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 21664\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.946033239364624\n",
      "        max_q: 123.74053192138672\n",
      "        mean_q: 93.25946044921875\n",
      "        mean_td_error: -1.4015085697174072\n",
      "        min_q: 2.0915937423706055\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.9799991250038147\n",
      "        max_q: 125.76876831054688\n",
      "        mean_q: 94.2366943359375\n",
      "        mean_td_error: 1.2182074785232544\n",
      "        min_q: 3.067985773086548\n",
      "    num_agent_steps_sampled: 44000\n",
      "    num_agent_steps_trained: 336064\n",
      "    num_steps_sampled: 22000\n",
      "    num_steps_trained: 168032\n",
      "    num_target_updates: 42\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.87692307692308\n",
      "    ram_util_percent: 64.53076923076922\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 292.06\n",
      "    agent-1: 292.73\n",
      "  policy_reward_min:\n",
      "    agent-0: 254.0\n",
      "    agent-1: 197.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0739924662522729\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07707456220521695\n",
      "    mean_inference_ms: 2.975558810383868\n",
      "    mean_raw_obs_processing_ms: 0.22319448459093544\n",
      "  time_since_restore: 199.130784034729\n",
      "  time_this_iter_s: 8.934963464736938\n",
      "  time_total_s: 199.130784034729\n",
      "  timers:\n",
      "    learn_throughput: 2190.141\n",
      "    learn_time_ms: 14.611\n",
      "  timestamp: 1628739896\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 22000\n",
      "  training_iteration: 22\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         199.131</td><td style=\"text-align: right;\">22000</td><td style=\"text-align: right;\">  584.79</td><td style=\"text-align: right;\">                 599</td><td style=\"text-align: right;\">                 498</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 46000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-45-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 599.0\n",
      "  episode_reward_mean: 586.53\n",
      "  episode_reward_min: 498.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 230\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 22672\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.1895101070404053\n",
      "        max_q: 125.29666900634766\n",
      "        mean_q: 97.67959594726562\n",
      "        mean_td_error: 0.5368950963020325\n",
      "        min_q: 18.596221923828125\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.6386190056800842\n",
      "        max_q: 129.42703247070312\n",
      "        mean_q: 88.74403381347656\n",
      "        mean_td_error: -0.5769714117050171\n",
      "        min_q: 2.1466612815856934\n",
      "    num_agent_steps_sampled: 46000\n",
      "    num_agent_steps_trained: 352064\n",
      "    num_steps_sampled: 23000\n",
      "    num_steps_trained: 176032\n",
      "    num_target_updates: 44\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.861538461538462\n",
      "    ram_util_percent: 64.56923076923077\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 292.79\n",
      "    agent-1: 293.74\n",
      "  policy_reward_min:\n",
      "    agent-0: 256.0\n",
      "    agent-1: 197.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07402400986215524\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07713669821166519\n",
      "    mean_inference_ms: 2.9764552256932\n",
      "    mean_raw_obs_processing_ms: 0.22330621856764224\n",
      "  time_since_restore: 208.03756713867188\n",
      "  time_this_iter_s: 8.906783103942871\n",
      "  time_total_s: 208.03756713867188\n",
      "  timers:\n",
      "    learn_throughput: 2145.078\n",
      "    learn_time_ms: 14.918\n",
      "  timestamp: 1628739905\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23000\n",
      "  training_iteration: 23\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         208.038</td><td style=\"text-align: right;\">23000</td><td style=\"text-align: right;\">  586.53</td><td style=\"text-align: right;\">                 599</td><td style=\"text-align: right;\">                 498</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 48000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-45-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 599.0\n",
      "  episode_reward_mean: 587.43\n",
      "  episode_reward_min: 498.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 240\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 23680\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.230688095092773\n",
      "        max_q: 133.45083618164062\n",
      "        mean_q: 101.66297149658203\n",
      "        mean_td_error: 3.827542304992676\n",
      "        min_q: 6.385387420654297\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.260343074798584\n",
      "        max_q: 133.00828552246094\n",
      "        mean_q: 108.79894256591797\n",
      "        mean_td_error: -1.8426897525787354\n",
      "        min_q: 25.072399139404297\n",
      "    num_agent_steps_sampled: 48000\n",
      "    num_agent_steps_trained: 368064\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 184032\n",
      "    num_target_updates: 46\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.55625\n",
      "    ram_util_percent: 65.16250000000001\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 293.11\n",
      "    agent-1: 294.32\n",
      "  policy_reward_min:\n",
      "    agent-0: 256.0\n",
      "    agent-1: 197.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07411847466714494\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07727726887703494\n",
      "    mean_inference_ms: 2.9806820319869622\n",
      "    mean_raw_obs_processing_ms: 0.22362150879907772\n",
      "  time_since_restore: 219.26618218421936\n",
      "  time_this_iter_s: 11.228615045547485\n",
      "  time_total_s: 219.26618218421936\n",
      "  timers:\n",
      "    learn_throughput: 2262.449\n",
      "    learn_time_ms: 14.144\n",
      "  timestamp: 1628739917\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 24\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         219.266</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  587.43</td><td style=\"text-align: right;\">                 599</td><td style=\"text-align: right;\">                 498</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 50000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-45-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 599.0\n",
      "  episode_reward_mean: 589.28\n",
      "  episode_reward_min: 555.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 250\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 24688\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.3538388013839722\n",
      "        max_q: 133.25872802734375\n",
      "        mean_q: 105.13809204101562\n",
      "        mean_td_error: 0.42599108815193176\n",
      "        min_q: 24.0632266998291\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.4596022367477417\n",
      "        max_q: 138.9717254638672\n",
      "        mean_q: 106.50447082519531\n",
      "        mean_td_error: 1.6557536125183105\n",
      "        min_q: 24.21310806274414\n",
      "    num_agent_steps_sampled: 50000\n",
      "    num_agent_steps_trained: 384064\n",
      "    num_steps_sampled: 25000\n",
      "    num_steps_trained: 192032\n",
      "    num_target_updates: 48\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.8625\n",
      "    ram_util_percent: 65.375\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 293.11\n",
      "    agent-1: 296.17\n",
      "  policy_reward_min:\n",
      "    agent-0: 256.0\n",
      "    agent-1: 268.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07428000153876915\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0774933994923922\n",
      "    mean_inference_ms: 2.9879319666766664\n",
      "    mean_raw_obs_processing_ms: 0.22413442248990606\n",
      "  time_since_restore: 230.5772762298584\n",
      "  time_this_iter_s: 11.311094045639038\n",
      "  time_total_s: 230.5772762298584\n",
      "  timers:\n",
      "    learn_throughput: 1355.637\n",
      "    learn_time_ms: 23.605\n",
      "  timestamp: 1628739928\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 25000\n",
      "  training_iteration: 25\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         230.577</td><td style=\"text-align: right;\">25000</td><td style=\"text-align: right;\">  589.28</td><td style=\"text-align: right;\">                 599</td><td style=\"text-align: right;\">                 555</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 52000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-45-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 599.0\n",
      "  episode_reward_mean: 589.92\n",
      "  episode_reward_min: 555.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 260\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 25696\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.5814064741134644\n",
      "        max_q: 134.75848388671875\n",
      "        mean_q: 102.90290832519531\n",
      "        mean_td_error: 1.6246025562286377\n",
      "        min_q: 6.4414520263671875\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.47414302825927734\n",
      "        max_q: 144.33319091796875\n",
      "        mean_q: 112.53096008300781\n",
      "        mean_td_error: 0.837277889251709\n",
      "        min_q: 56.16407012939453\n",
      "    num_agent_steps_sampled: 52000\n",
      "    num_agent_steps_trained: 400064\n",
      "    num_steps_sampled: 26000\n",
      "    num_steps_trained: 200032\n",
      "    num_target_updates: 50\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.126666666666665\n",
      "    ram_util_percent: 65.26666666666668\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 293.2\n",
      "    agent-1: 296.72\n",
      "  policy_reward_min:\n",
      "    agent-0: 256.0\n",
      "    agent-1: 283.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07447443579647837\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07774645205332292\n",
      "    mean_inference_ms: 2.9967677326017905\n",
      "    mean_raw_obs_processing_ms: 0.2247515123859715\n",
      "  time_since_restore: 240.74403882026672\n",
      "  time_this_iter_s: 10.166762590408325\n",
      "  time_total_s: 240.74403882026672\n",
      "  timers:\n",
      "    learn_throughput: 2110.581\n",
      "    learn_time_ms: 15.162\n",
      "  timestamp: 1628739938\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 26000\n",
      "  training_iteration: 26\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">         240.744</td><td style=\"text-align: right;\">26000</td><td style=\"text-align: right;\">  589.92</td><td style=\"text-align: right;\">                 599</td><td style=\"text-align: right;\">                 555</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 54000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-45-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 599.0\n",
      "  episode_reward_mean: 588.64\n",
      "  episode_reward_min: 545.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 270\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 26704\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.091402530670166\n",
      "        max_q: 136.94482421875\n",
      "        mean_q: 113.34873962402344\n",
      "        mean_td_error: -1.3921951055526733\n",
      "        min_q: 18.70035743713379\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.6798931956291199\n",
      "        max_q: 146.48211669921875\n",
      "        mean_q: 113.98743438720703\n",
      "        mean_td_error: 1.1602932214736938\n",
      "        min_q: 34.76413345336914\n",
      "    num_agent_steps_sampled: 54000\n",
      "    num_agent_steps_trained: 416064\n",
      "    num_steps_sampled: 27000\n",
      "    num_steps_trained: 208032\n",
      "    num_target_updates: 52\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.342857142857145\n",
      "    ram_util_percent: 65.12857142857145\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 293.01\n",
      "    agent-1: 295.63\n",
      "  policy_reward_min:\n",
      "    agent-0: 256.0\n",
      "    agent-1: 256.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07468999331430219\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07802949463295579\n",
      "    mean_inference_ms: 3.0067029281259767\n",
      "    mean_raw_obs_processing_ms: 0.22541054747515957\n",
      "  time_since_restore: 250.70628905296326\n",
      "  time_this_iter_s: 9.962250232696533\n",
      "  time_total_s: 250.70628905296326\n",
      "  timers:\n",
      "    learn_throughput: 1918.435\n",
      "    learn_time_ms: 16.68\n",
      "  timestamp: 1628739948\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 27000\n",
      "  training_iteration: 27\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         250.706</td><td style=\"text-align: right;\">27000</td><td style=\"text-align: right;\">  588.64</td><td style=\"text-align: right;\">                 599</td><td style=\"text-align: right;\">                 545</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 56000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-45-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 599.0\n",
      "  episode_reward_mean: 588.1\n",
      "  episode_reward_min: 545.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 280\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 27712\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.101341962814331\n",
      "        max_q: 139.79254150390625\n",
      "        mean_q: 98.73606872558594\n",
      "        mean_td_error: 1.4639530181884766\n",
      "        min_q: 0.665795624256134\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.4876958131790161\n",
      "        max_q: 148.30809020996094\n",
      "        mean_q: 105.74872589111328\n",
      "        mean_td_error: 2.6087896823883057\n",
      "        min_q: 21.540903091430664\n",
      "    num_agent_steps_sampled: 56000\n",
      "    num_agent_steps_trained: 432064\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 216032\n",
      "    num_target_updates: 54\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.02307692307693\n",
      "    ram_util_percent: 65.01538461538462\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 292.76\n",
      "    agent-1: 295.34\n",
      "  policy_reward_min:\n",
      "    agent-0: 256.0\n",
      "    agent-1: 256.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0749049020020251\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07831441751287067\n",
      "    mean_inference_ms: 3.016598053782255\n",
      "    mean_raw_obs_processing_ms: 0.2260761636606086\n",
      "  time_since_restore: 259.8780310153961\n",
      "  time_this_iter_s: 9.171741962432861\n",
      "  time_total_s: 259.8780310153961\n",
      "  timers:\n",
      "    learn_throughput: 2192.051\n",
      "    learn_time_ms: 14.598\n",
      "  timestamp: 1628739957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 28\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         259.878</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">   588.1</td><td style=\"text-align: right;\">                 599</td><td style=\"text-align: right;\">                 545</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 58000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-46-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 599.0\n",
      "  episode_reward_mean: 587.16\n",
      "  episode_reward_min: 545.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 290\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 28720\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.9215235114097595\n",
      "        max_q: 141.86351013183594\n",
      "        mean_q: 97.91081237792969\n",
      "        mean_td_error: 1.6212453842163086\n",
      "        min_q: 0.41882193088531494\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.4810386598110199\n",
      "        max_q: 148.052001953125\n",
      "        mean_q: 108.66938018798828\n",
      "        mean_td_error: 0.8027165532112122\n",
      "        min_q: 22.914506912231445\n",
      "    num_agent_steps_sampled: 58000\n",
      "    num_agent_steps_trained: 448064\n",
      "    num_steps_sampled: 29000\n",
      "    num_steps_trained: 224032\n",
      "    num_target_updates: 56\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.330769230769235\n",
      "    ram_util_percent: 65.14615384615385\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 292.01\n",
      "    agent-1: 295.15\n",
      "  policy_reward_min:\n",
      "    agent-0: 256.0\n",
      "    agent-1: 256.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07510833370227284\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07858060057627672\n",
      "    mean_inference_ms: 3.0267846044489954\n",
      "    mean_raw_obs_processing_ms: 0.2267314883964647\n",
      "  time_since_restore: 269.33168363571167\n",
      "  time_this_iter_s: 9.453652620315552\n",
      "  time_total_s: 269.33168363571167\n",
      "  timers:\n",
      "    learn_throughput: 2101.401\n",
      "    learn_time_ms: 15.228\n",
      "  timestamp: 1628739967\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 29000\n",
      "  training_iteration: 29\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         269.332</td><td style=\"text-align: right;\">29000</td><td style=\"text-align: right;\">  587.16</td><td style=\"text-align: right;\">                 599</td><td style=\"text-align: right;\">                 545</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 60000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-46-18\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 599.0\n",
      "  episode_reward_mean: 586.0\n",
      "  episode_reward_min: 536.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 300\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 29728\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.142400026321411\n",
      "        max_q: 143.57427978515625\n",
      "        mean_q: 106.33390808105469\n",
      "        mean_td_error: -0.1392269730567932\n",
      "        min_q: 12.910603523254395\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.7029507160186768\n",
      "        max_q: 149.54299926757812\n",
      "        mean_q: 113.94766998291016\n",
      "        mean_td_error: 1.0733574628829956\n",
      "        min_q: 4.603976249694824\n",
      "    num_agent_steps_sampled: 60000\n",
      "    num_agent_steps_trained: 464064\n",
      "    num_steps_sampled: 30000\n",
      "    num_steps_trained: 232032\n",
      "    num_target_updates: 58\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.63125\n",
      "    ram_util_percent: 65.59375\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 291.96\n",
      "    agent-1: 294.04\n",
      "  policy_reward_min:\n",
      "    agent-0: 256.0\n",
      "    agent-1: 253.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07527773024603766\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07880173560777624\n",
      "    mean_inference_ms: 3.0356726875613895\n",
      "    mean_raw_obs_processing_ms: 0.22729460464972162\n",
      "  time_since_restore: 280.34941053390503\n",
      "  time_this_iter_s: 11.01772689819336\n",
      "  time_total_s: 280.34941053390503\n",
      "  timers:\n",
      "    learn_throughput: 1996.477\n",
      "    learn_time_ms: 16.028\n",
      "  timestamp: 1628739978\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 30000\n",
      "  training_iteration: 30\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         280.349</td><td style=\"text-align: right;\">30000</td><td style=\"text-align: right;\">     586</td><td style=\"text-align: right;\">                 599</td><td style=\"text-align: right;\">                 536</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 62000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-46-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 599.0\n",
      "  episode_reward_mean: 586.17\n",
      "  episode_reward_min: 536.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 310\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 30736\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.2357004880905151\n",
      "        max_q: 146.3794708251953\n",
      "        mean_q: 102.05789947509766\n",
      "        mean_td_error: 1.3752917051315308\n",
      "        min_q: 12.197693824768066\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.7463781833648682\n",
      "        max_q: 153.3220977783203\n",
      "        mean_q: 112.49140167236328\n",
      "        mean_td_error: -1.1757354736328125\n",
      "        min_q: 23.2219295501709\n",
      "    num_agent_steps_sampled: 62000\n",
      "    num_agent_steps_trained: 480064\n",
      "    num_steps_sampled: 31000\n",
      "    num_steps_trained: 240032\n",
      "    num_target_updates: 60\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.95882352941176\n",
      "    ram_util_percent: 66.31764705882354\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 291.95\n",
      "    agent-1: 294.22\n",
      "  policy_reward_min:\n",
      "    agent-0: 256.0\n",
      "    agent-1: 253.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07546652053220287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07904491006324071\n",
      "    mean_inference_ms: 3.0454935988062415\n",
      "    mean_raw_obs_processing_ms: 0.2279331862285548\n",
      "  time_since_restore: 292.3865463733673\n",
      "  time_this_iter_s: 12.03713583946228\n",
      "  time_total_s: 292.3865463733673\n",
      "  timers:\n",
      "    learn_throughput: 2068.851\n",
      "    learn_time_ms: 15.468\n",
      "  timestamp: 1628739990\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31000\n",
      "  training_iteration: 31\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         292.387</td><td style=\"text-align: right;\">31000</td><td style=\"text-align: right;\">  586.17</td><td style=\"text-align: right;\">                 599</td><td style=\"text-align: right;\">                 536</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 64000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-46-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 599.0\n",
      "  episode_reward_mean: 586.27\n",
      "  episode_reward_min: 536.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 320\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 31744\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.7638015747070312\n",
      "        max_q: 150.47023010253906\n",
      "        mean_q: 108.12074279785156\n",
      "        mean_td_error: 2.2565183639526367\n",
      "        min_q: 4.548588752746582\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.0778478384017944\n",
      "        max_q: 153.82139587402344\n",
      "        mean_q: 108.45819091796875\n",
      "        mean_td_error: -0.9754807949066162\n",
      "        min_q: 33.804080963134766\n",
      "    num_agent_steps_sampled: 64000\n",
      "    num_agent_steps_trained: 496064\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 248032\n",
      "    num_target_updates: 62\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.96428571428571\n",
      "    ram_util_percent: 66.38571428571427\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 292.12\n",
      "    agent-1: 294.15\n",
      "  policy_reward_min:\n",
      "    agent-0: 261.0\n",
      "    agent-1: 253.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07566951991935159\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.07930065269149052\n",
      "    mean_inference_ms: 3.0557508728576908\n",
      "    mean_raw_obs_processing_ms: 0.22858134834086485\n",
      "  time_since_restore: 301.82111120224\n",
      "  time_this_iter_s: 9.43456482887268\n",
      "  time_total_s: 301.82111120224\n",
      "  timers:\n",
      "    learn_throughput: 1370.947\n",
      "    learn_time_ms: 23.342\n",
      "  timestamp: 1628739999\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 32\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         301.821</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  586.27</td><td style=\"text-align: right;\">                 599</td><td style=\"text-align: right;\">                 536</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 66000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-46-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 599.0\n",
      "  episode_reward_mean: 585.6\n",
      "  episode_reward_min: 536.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 330\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 32752\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.6413657665252686\n",
      "        max_q: 149.73570251464844\n",
      "        mean_q: 96.9141616821289\n",
      "        mean_td_error: -1.7926791906356812\n",
      "        min_q: 2.5362391471862793\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.46538689732551575\n",
      "        max_q: 154.62916564941406\n",
      "        mean_q: 112.63867950439453\n",
      "        mean_td_error: -0.9165062308311462\n",
      "        min_q: 4.597894191741943\n",
      "    num_agent_steps_sampled: 66000\n",
      "    num_agent_steps_trained: 512064\n",
      "    num_steps_sampled: 33000\n",
      "    num_steps_trained: 256032\n",
      "    num_target_updates: 64\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.118750000000006\n",
      "    ram_util_percent: 66.34375\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 291.87\n",
      "    agent-1: 293.73\n",
      "  policy_reward_min:\n",
      "    agent-0: 261.0\n",
      "    agent-1: 253.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0759253250269412\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0796129924126112\n",
      "    mean_inference_ms: 3.0681844870655164\n",
      "    mean_raw_obs_processing_ms: 0.2293774180867265\n",
      "  time_since_restore: 312.95957040786743\n",
      "  time_this_iter_s: 11.138459205627441\n",
      "  time_total_s: 312.95957040786743\n",
      "  timers:\n",
      "    learn_throughput: 2192.423\n",
      "    learn_time_ms: 14.596\n",
      "  timestamp: 1628740011\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 33000\n",
      "  training_iteration: 33\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">          312.96</td><td style=\"text-align: right;\">33000</td><td style=\"text-align: right;\">   585.6</td><td style=\"text-align: right;\">                 599</td><td style=\"text-align: right;\">                 536</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 68000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-47-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 599.0\n",
      "  episode_reward_mean: 584.84\n",
      "  episode_reward_min: 536.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 340\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 33760\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.686849594116211\n",
      "        max_q: 153.47032165527344\n",
      "        mean_q: 100.70893859863281\n",
      "        mean_td_error: -2.9996609687805176\n",
      "        min_q: 4.224076747894287\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.5562334060668945\n",
      "        max_q: 157.0497283935547\n",
      "        mean_q: 111.82275390625\n",
      "        mean_td_error: 1.7235230207443237\n",
      "        min_q: 15.875720977783203\n",
      "    num_agent_steps_sampled: 68000\n",
      "    num_agent_steps_trained: 528064\n",
      "    num_steps_sampled: 34000\n",
      "    num_steps_trained: 264032\n",
      "    num_target_updates: 66\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.599999999999994\n",
      "    ram_util_percent: 66.66250000000001\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 292.12\n",
      "    agent-1: 292.72\n",
      "  policy_reward_min:\n",
      "    agent-0: 261.0\n",
      "    agent-1: 250.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07616085457496612\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0799007883766625\n",
      "    mean_inference_ms: 3.079543061868441\n",
      "    mean_raw_obs_processing_ms: 0.23009988328833286\n",
      "  time_since_restore: 324.1885437965393\n",
      "  time_this_iter_s: 11.228973388671875\n",
      "  time_total_s: 324.1885437965393\n",
      "  timers:\n",
      "    learn_throughput: 2197.783\n",
      "    learn_time_ms: 14.56\n",
      "  timestamp: 1628740022\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 34000\n",
      "  training_iteration: 34\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         324.189</td><td style=\"text-align: right;\">34000</td><td style=\"text-align: right;\">  584.84</td><td style=\"text-align: right;\">                 599</td><td style=\"text-align: right;\">                 536</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 70000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-47-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 598.0\n",
      "  episode_reward_mean: 583.21\n",
      "  episode_reward_min: 536.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 350\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 34768\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.5416929721832275\n",
      "        max_q: 157.32716369628906\n",
      "        mean_q: 114.92213439941406\n",
      "        mean_td_error: 0.44695743918418884\n",
      "        min_q: 3.190402030944824\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.7236559391021729\n",
      "        max_q: 157.1600341796875\n",
      "        mean_q: 108.07706451416016\n",
      "        mean_td_error: 0.9398499727249146\n",
      "        min_q: 19.181352615356445\n",
      "    num_agent_steps_sampled: 70000\n",
      "    num_agent_steps_trained: 544064\n",
      "    num_steps_sampled: 35000\n",
      "    num_steps_trained: 272032\n",
      "    num_target_updates: 68\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 39.199999999999996\n",
      "    ram_util_percent: 67.35625\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 291.79\n",
      "    agent-1: 291.42\n",
      "  policy_reward_min:\n",
      "    agent-0: 261.0\n",
      "    agent-1: 250.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07637278744282243\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08015905418422252\n",
      "    mean_inference_ms: 3.089935330397571\n",
      "    mean_raw_obs_processing_ms: 0.230759118326138\n",
      "  time_since_restore: 335.4929358959198\n",
      "  time_this_iter_s: 11.304392099380493\n",
      "  time_total_s: 335.4929358959198\n",
      "  timers:\n",
      "    learn_throughput: 1933.672\n",
      "    learn_time_ms: 16.549\n",
      "  timestamp: 1628740033\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 35000\n",
      "  training_iteration: 35\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.4/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         335.493</td><td style=\"text-align: right;\">35000</td><td style=\"text-align: right;\">  583.21</td><td style=\"text-align: right;\">                 598</td><td style=\"text-align: right;\">                 536</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 72000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-47-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 598.0\n",
      "  episode_reward_mean: 583.37\n",
      "  episode_reward_min: 536.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 360\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 35776\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.6833651065826416\n",
      "        max_q: 157.15406799316406\n",
      "        mean_q: 118.06761932373047\n",
      "        mean_td_error: 2.71657657623291\n",
      "        min_q: 47.462547302246094\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.5452460646629333\n",
      "        max_q: 159.14222717285156\n",
      "        mean_q: 98.87963104248047\n",
      "        mean_td_error: 1.8987241983413696\n",
      "        min_q: 17.542165756225586\n",
      "    num_agent_steps_sampled: 72000\n",
      "    num_agent_steps_trained: 560064\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 280032\n",
      "    num_target_updates: 70\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.449999999999996\n",
      "    ram_util_percent: 67.06428571428572\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 291.77\n",
      "    agent-1: 291.6\n",
      "  policy_reward_min:\n",
      "    agent-0: 261.0\n",
      "    agent-1: 250.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07657861827088559\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08041147371259745\n",
      "    mean_inference_ms: 3.09981225678566\n",
      "    mean_raw_obs_processing_ms: 0.23137176546580057\n",
      "  time_since_restore: 345.67391705513\n",
      "  time_this_iter_s: 10.180981159210205\n",
      "  time_total_s: 345.67391705513\n",
      "  timers:\n",
      "    learn_throughput: 2221.096\n",
      "    learn_time_ms: 14.407\n",
      "  timestamp: 1628740043\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 36\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">         345.674</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  583.37</td><td style=\"text-align: right;\">                 598</td><td style=\"text-align: right;\">                 536</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 74000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-47-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 598.0\n",
      "  episode_reward_mean: 584.35\n",
      "  episode_reward_min: 536.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 370\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 36784\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.6996979713439941\n",
      "        max_q: 159.42063903808594\n",
      "        mean_q: 113.54029846191406\n",
      "        mean_td_error: 0.7765350341796875\n",
      "        min_q: 0.5180991888046265\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.3964499235153198\n",
      "        max_q: 159.34255981445312\n",
      "        mean_q: 117.47835540771484\n",
      "        mean_td_error: -2.18135404586792\n",
      "        min_q: 17.57147216796875\n",
      "    num_agent_steps_sampled: 74000\n",
      "    num_agent_steps_trained: 576064\n",
      "    num_steps_sampled: 37000\n",
      "    num_steps_trained: 288032\n",
      "    num_target_updates: 72\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.592857142857138\n",
      "    ram_util_percent: 66.35\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 291.92\n",
      "    agent-1: 292.43\n",
      "  policy_reward_min:\n",
      "    agent-0: 261.0\n",
      "    agent-1: 250.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07676346975426604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08063592275668213\n",
      "    mean_inference_ms: 3.1093624598406366\n",
      "    mean_raw_obs_processing_ms: 0.23192712239710023\n",
      "  time_since_restore: 355.1748297214508\n",
      "  time_this_iter_s: 9.5009126663208\n",
      "  time_total_s: 355.1748297214508\n",
      "  timers:\n",
      "    learn_throughput: 2101.233\n",
      "    learn_time_ms: 15.229\n",
      "  timestamp: 1628740053\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 37000\n",
      "  training_iteration: 37\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         355.175</td><td style=\"text-align: right;\">37000</td><td style=\"text-align: right;\">  584.35</td><td style=\"text-align: right;\">                 598</td><td style=\"text-align: right;\">                 536</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 76000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-47-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 598.0\n",
      "  episode_reward_mean: 585.0\n",
      "  episode_reward_min: 536.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 380\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 37792\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.9624204635620117\n",
      "        max_q: 160.1611785888672\n",
      "        mean_q: 110.46002960205078\n",
      "        mean_td_error: 0.6588978171348572\n",
      "        min_q: 14.505244255065918\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.491780161857605\n",
      "        max_q: 163.3395233154297\n",
      "        mean_q: 93.25183868408203\n",
      "        mean_td_error: -1.248767614364624\n",
      "        min_q: 16.97381591796875\n",
      "    num_agent_steps_sampled: 76000\n",
      "    num_agent_steps_trained: 592064\n",
      "    num_steps_sampled: 38000\n",
      "    num_steps_trained: 296032\n",
      "    num_target_updates: 74\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.33571428571429\n",
      "    ram_util_percent: 65.81428571428573\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 292.11\n",
      "    agent-1: 292.89\n",
      "  policy_reward_min:\n",
      "    agent-0: 261.0\n",
      "    agent-1: 250.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07695867524996923\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08087054651584101\n",
      "    mean_inference_ms: 3.1193422852175683\n",
      "    mean_raw_obs_processing_ms: 0.2325013348647623\n",
      "  time_since_restore: 364.94991993904114\n",
      "  time_this_iter_s: 9.775090217590332\n",
      "  time_total_s: 364.94991993904114\n",
      "  timers:\n",
      "    learn_throughput: 2006.286\n",
      "    learn_time_ms: 15.95\n",
      "  timestamp: 1628740063\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 38000\n",
      "  training_iteration: 38\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">          364.95</td><td style=\"text-align: right;\">38000</td><td style=\"text-align: right;\">     585</td><td style=\"text-align: right;\">                 598</td><td style=\"text-align: right;\">                 536</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 78000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-47-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 598.0\n",
      "  episode_reward_mean: 584.74\n",
      "  episode_reward_min: 536.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 390\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 38800\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.1359996795654297\n",
      "        max_q: 157.91065979003906\n",
      "        mean_q: 116.8792953491211\n",
      "        mean_td_error: 0.1534343957901001\n",
      "        min_q: 28.735538482666016\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.33261239528656\n",
      "        max_q: 163.7919464111328\n",
      "        mean_q: 127.02340698242188\n",
      "        mean_td_error: 1.5062496662139893\n",
      "        min_q: 36.81267166137695\n",
      "    num_agent_steps_sampled: 78000\n",
      "    num_agent_steps_trained: 608064\n",
      "    num_steps_sampled: 39000\n",
      "    num_steps_trained: 304032\n",
      "    num_target_updates: 76\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.485714285714288\n",
      "    ram_util_percent: 65.40714285714287\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 291.77\n",
      "    agent-1: 292.97\n",
      "  policy_reward_min:\n",
      "    agent-0: 270.0\n",
      "    agent-1: 250.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07716197070127355\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08111534609771578\n",
      "    mean_inference_ms: 3.129028578038155\n",
      "    mean_raw_obs_processing_ms: 0.2330855526350534\n",
      "  time_since_restore: 374.83686661720276\n",
      "  time_this_iter_s: 9.886946678161621\n",
      "  time_total_s: 374.83686661720276\n",
      "  timers:\n",
      "    learn_throughput: 1477.759\n",
      "    learn_time_ms: 21.654\n",
      "  timestamp: 1628740073\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39000\n",
      "  training_iteration: 39\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         374.837</td><td style=\"text-align: right;\">39000</td><td style=\"text-align: right;\">  584.74</td><td style=\"text-align: right;\">                 598</td><td style=\"text-align: right;\">                 536</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 80000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-48-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 598.0\n",
      "  episode_reward_mean: 584.73\n",
      "  episode_reward_min: 544.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 400\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 39808\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.9521923065185547\n",
      "        max_q: 156.9783477783203\n",
      "        mean_q: 114.34748077392578\n",
      "        mean_td_error: -1.8729937076568604\n",
      "        min_q: 2.2216522693634033\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.6135170459747314\n",
      "        max_q: 164.42108154296875\n",
      "        mean_q: 121.88822937011719\n",
      "        mean_td_error: -1.6420392990112305\n",
      "        min_q: 2.1802022457122803\n",
      "    num_agent_steps_sampled: 80000\n",
      "    num_agent_steps_trained: 624064\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 312032\n",
      "    num_target_updates: 78\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.880000000000003\n",
      "    ram_util_percent: 65.69333333333334\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 291.56\n",
      "    agent-1: 293.17\n",
      "  policy_reward_min:\n",
      "    agent-0: 270.0\n",
      "    agent-1: 250.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07734577981222074\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08133761805376956\n",
      "    mean_inference_ms: 3.137658256733594\n",
      "    mean_raw_obs_processing_ms: 0.23358956392616503\n",
      "  time_since_restore: 385.52325415611267\n",
      "  time_this_iter_s: 10.686387538909912\n",
      "  time_total_s: 385.52325415611267\n",
      "  timers:\n",
      "    learn_throughput: 2242.606\n",
      "    learn_time_ms: 14.269\n",
      "  timestamp: 1628740083\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 40\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         385.523</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  584.73</td><td style=\"text-align: right;\">                 598</td><td style=\"text-align: right;\">                 544</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 82000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-48-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 598.0\n",
      "  episode_reward_mean: 584.35\n",
      "  episode_reward_min: 544.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 410\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 40816\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.6495016813278198\n",
      "        max_q: 163.01939392089844\n",
      "        mean_q: 117.23020935058594\n",
      "        mean_td_error: -0.14141875505447388\n",
      "        min_q: 18.172565460205078\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.1275804042816162\n",
      "        max_q: 164.23426818847656\n",
      "        mean_q: 103.13462829589844\n",
      "        mean_td_error: -0.9934132099151611\n",
      "        min_q: 1.8084478378295898\n",
      "    num_agent_steps_sampled: 82000\n",
      "    num_agent_steps_trained: 640064\n",
      "    num_steps_sampled: 41000\n",
      "    num_steps_trained: 320032\n",
      "    num_target_updates: 80\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.957142857142856\n",
      "    ram_util_percent: 65.61428571428573\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 291.21\n",
      "    agent-1: 293.14\n",
      "  policy_reward_min:\n",
      "    agent-0: 259.0\n",
      "    agent-1: 250.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07746637429589184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08148865265246238\n",
      "    mean_inference_ms: 3.143367954162491\n",
      "    mean_raw_obs_processing_ms: 0.23390572651820507\n",
      "  time_since_restore: 395.3571445941925\n",
      "  time_this_iter_s: 9.833890438079834\n",
      "  time_total_s: 395.3571445941925\n",
      "  timers:\n",
      "    learn_throughput: 1871.256\n",
      "    learn_time_ms: 17.101\n",
      "  timestamp: 1628740093\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 41000\n",
      "  training_iteration: 41\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         395.357</td><td style=\"text-align: right;\">41000</td><td style=\"text-align: right;\">  584.35</td><td style=\"text-align: right;\">                 598</td><td style=\"text-align: right;\">                 544</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 84000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-48-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 598.0\n",
      "  episode_reward_mean: 585.2\n",
      "  episode_reward_min: 544.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 420\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 41824\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.88560152053833\n",
      "        max_q: 161.81617736816406\n",
      "        mean_q: 112.50260162353516\n",
      "        mean_td_error: -0.14336220920085907\n",
      "        min_q: 4.234795093536377\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.6580560207366943\n",
      "        max_q: 166.0885772705078\n",
      "        mean_q: 106.34927368164062\n",
      "        mean_td_error: -0.6150449514389038\n",
      "        min_q: 27.512189865112305\n",
      "    num_agent_steps_sampled: 84000\n",
      "    num_agent_steps_trained: 656064\n",
      "    num_steps_sampled: 42000\n",
      "    num_steps_trained: 328032\n",
      "    num_target_updates: 82\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.03076923076923\n",
      "    ram_util_percent: 65.66153846153847\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 291.8\n",
      "    agent-1: 293.4\n",
      "  policy_reward_min:\n",
      "    agent-0: 259.0\n",
      "    agent-1: 250.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07756882177647166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08161862426312164\n",
      "    mean_inference_ms: 3.1484814391209746\n",
      "    mean_raw_obs_processing_ms: 0.23418095307087283\n",
      "  time_since_restore: 404.1812059879303\n",
      "  time_this_iter_s: 8.824061393737793\n",
      "  time_total_s: 404.1812059879303\n",
      "  timers:\n",
      "    learn_throughput: 2198.395\n",
      "    learn_time_ms: 14.556\n",
      "  timestamp: 1628740102\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 42000\n",
      "  training_iteration: 42\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         404.181</td><td style=\"text-align: right;\">42000</td><td style=\"text-align: right;\">   585.2</td><td style=\"text-align: right;\">                 598</td><td style=\"text-align: right;\">                 544</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 86000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-48-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 598.0\n",
      "  episode_reward_mean: 585.66\n",
      "  episode_reward_min: 544.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 430\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 42832\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.447944402694702\n",
      "        max_q: 165.4915313720703\n",
      "        mean_q: 103.06645202636719\n",
      "        mean_td_error: -1.3129428625106812\n",
      "        min_q: 0.24162718653678894\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.5620067119598389\n",
      "        max_q: 166.42471313476562\n",
      "        mean_q: 114.99899291992188\n",
      "        mean_td_error: 1.8236000537872314\n",
      "        min_q: 5.348145008087158\n",
      "    num_agent_steps_sampled: 86000\n",
      "    num_agent_steps_trained: 672064\n",
      "    num_steps_sampled: 43000\n",
      "    num_steps_trained: 336032\n",
      "    num_target_updates: 84\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.969230769230766\n",
      "    ram_util_percent: 65.78461538461538\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 291.79\n",
      "    agent-1: 293.87\n",
      "  policy_reward_min:\n",
      "    agent-0: 259.0\n",
      "    agent-1: 250.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07762005193498446\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08169159643902102\n",
      "    mean_inference_ms: 3.151354259522841\n",
      "    mean_raw_obs_processing_ms: 0.23430281753258803\n",
      "  time_since_restore: 413.073490858078\n",
      "  time_this_iter_s: 8.892284870147705\n",
      "  time_total_s: 413.073490858078\n",
      "  timers:\n",
      "    learn_throughput: 2068.248\n",
      "    learn_time_ms: 15.472\n",
      "  timestamp: 1628740111\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 43000\n",
      "  training_iteration: 43\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">         413.073</td><td style=\"text-align: right;\">43000</td><td style=\"text-align: right;\">  585.66</td><td style=\"text-align: right;\">                 598</td><td style=\"text-align: right;\">                 544</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 88000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-48-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 599.0\n",
      "  episode_reward_mean: 586.22\n",
      "  episode_reward_min: 546.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 440\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 43840\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.2548909187316895\n",
      "        max_q: 165.9440155029297\n",
      "        mean_q: 116.04070281982422\n",
      "        mean_td_error: 1.6462026834487915\n",
      "        min_q: 17.084260940551758\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.1456798315048218\n",
      "        max_q: 160.9351806640625\n",
      "        mean_q: 111.55499267578125\n",
      "        mean_td_error: -0.062467992305755615\n",
      "        min_q: 3.0516014099121094\n",
      "    num_agent_steps_sampled: 88000\n",
      "    num_agent_steps_trained: 688064\n",
      "    num_steps_sampled: 44000\n",
      "    num_steps_trained: 344032\n",
      "    num_target_updates: 86\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.63333333333333\n",
      "    ram_util_percent: 65.86666666666666\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 291.52\n",
      "    agent-1: 294.7\n",
      "  policy_reward_min:\n",
      "    agent-0: 259.0\n",
      "    agent-1: 262.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07761762916143876\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08169670171777078\n",
      "    mean_inference_ms: 3.1518173430530023\n",
      "    mean_raw_obs_processing_ms: 0.23427586470087572\n",
      "  time_since_restore: 421.9363524913788\n",
      "  time_this_iter_s: 8.862861633300781\n",
      "  time_total_s: 421.9363524913788\n",
      "  timers:\n",
      "    learn_throughput: 2172.835\n",
      "    learn_time_ms: 14.727\n",
      "  timestamp: 1628740120\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 44000\n",
      "  training_iteration: 44\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         421.936</td><td style=\"text-align: right;\">44000</td><td style=\"text-align: right;\">  586.22</td><td style=\"text-align: right;\">                 599</td><td style=\"text-align: right;\">                 546</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 90000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-48-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 600.0\n",
      "  episode_reward_mean: 586.76\n",
      "  episode_reward_min: 546.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 450\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 44848\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.416385650634766\n",
      "        max_q: 169.11534118652344\n",
      "        mean_q: 124.3006591796875\n",
      "        mean_td_error: 2.3463587760925293\n",
      "        min_q: 1.64486563205719\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.9051894545555115\n",
      "        max_q: 169.30099487304688\n",
      "        mean_q: 119.26942443847656\n",
      "        mean_td_error: 1.183222770690918\n",
      "        min_q: 4.166130065917969\n",
      "    num_agent_steps_sampled: 90000\n",
      "    num_agent_steps_trained: 704064\n",
      "    num_steps_sampled: 45000\n",
      "    num_steps_trained: 352032\n",
      "    num_target_updates: 88\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.88666666666666\n",
      "    ram_util_percent: 66.20666666666668\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 290.68\n",
      "    agent-1: 296.08\n",
      "  policy_reward_min:\n",
      "    agent-0: 251.0\n",
      "    agent-1: 263.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0775873393410862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08166700875284309\n",
      "    mean_inference_ms: 3.151060237969445\n",
      "    mean_raw_obs_processing_ms: 0.23417534790757333\n",
      "  time_since_restore: 432.1603229045868\n",
      "  time_this_iter_s: 10.223970413208008\n",
      "  time_total_s: 432.1603229045868\n",
      "  timers:\n",
      "    learn_throughput: 2087.578\n",
      "    learn_time_ms: 15.329\n",
      "  timestamp: 1628740130\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 45000\n",
      "  training_iteration: 45\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">          432.16</td><td style=\"text-align: right;\">45000</td><td style=\"text-align: right;\">  586.76</td><td style=\"text-align: right;\">                 600</td><td style=\"text-align: right;\">                 546</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 92000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-49-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 600.0\n",
      "  episode_reward_mean: 587.15\n",
      "  episode_reward_min: 546.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 460\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 45856\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.976709008216858\n",
      "        max_q: 170.90975952148438\n",
      "        mean_q: 124.51403045654297\n",
      "        mean_td_error: 1.2653934955596924\n",
      "        min_q: 1.5411485433578491\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.1009011268615723\n",
      "        max_q: 171.08717346191406\n",
      "        mean_q: 114.17514038085938\n",
      "        mean_td_error: 2.416740894317627\n",
      "        min_q: 4.0955915451049805\n",
      "    num_agent_steps_sampled: 92000\n",
      "    num_agent_steps_trained: 720064\n",
      "    num_steps_sampled: 46000\n",
      "    num_steps_trained: 360032\n",
      "    num_target_updates: 90\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 33.233333333333334\n",
      "    ram_util_percent: 66.77333333333333\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 290.88\n",
      "    agent-1: 296.27\n",
      "  policy_reward_min:\n",
      "    agent-0: 251.0\n",
      "    agent-1: 263.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07755697587659002\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08163294304902723\n",
      "    mean_inference_ms: 3.1503383416681605\n",
      "    mean_raw_obs_processing_ms: 0.23408549045150182\n",
      "  time_since_restore: 442.4983947277069\n",
      "  time_this_iter_s: 10.338071823120117\n",
      "  time_total_s: 442.4983947277069\n",
      "  timers:\n",
      "    learn_throughput: 1878.44\n",
      "    learn_time_ms: 17.035\n",
      "  timestamp: 1628740140\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 46000\n",
      "  training_iteration: 46\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.3/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         442.498</td><td style=\"text-align: right;\">46000</td><td style=\"text-align: right;\">  587.15</td><td style=\"text-align: right;\">                 600</td><td style=\"text-align: right;\">                 546</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 94000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-49-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 600.0\n",
      "  episode_reward_mean: 586.82\n",
      "  episode_reward_min: 546.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 470\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 46864\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.2939252853393555\n",
      "        max_q: 169.14920043945312\n",
      "        mean_q: 123.15155792236328\n",
      "        mean_td_error: -1.5941877365112305\n",
      "        min_q: 20.332735061645508\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.853285551071167\n",
      "        max_q: 168.72409057617188\n",
      "        mean_q: 118.66243743896484\n",
      "        mean_td_error: 1.857347846031189\n",
      "        min_q: 26.586376190185547\n",
      "    num_agent_steps_sampled: 94000\n",
      "    num_agent_steps_trained: 736064\n",
      "    num_steps_sampled: 47000\n",
      "    num_steps_trained: 368032\n",
      "    num_target_updates: 92\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.85294117647059\n",
      "    ram_util_percent: 67.4470588235294\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 290.88\n",
      "    agent-1: 295.94\n",
      "  policy_reward_min:\n",
      "    agent-0: 251.0\n",
      "    agent-1: 263.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07757040872690965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0816554647278532\n",
      "    mean_inference_ms: 3.150957582899589\n",
      "    mean_raw_obs_processing_ms: 0.2341276815051048\n",
      "  time_since_restore: 454.46227526664734\n",
      "  time_this_iter_s: 11.96388053894043\n",
      "  time_total_s: 454.46227526664734\n",
      "  timers:\n",
      "    learn_throughput: 1722.94\n",
      "    learn_time_ms: 18.573\n",
      "  timestamp: 1628740152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47000\n",
      "  training_iteration: 47\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         454.462</td><td style=\"text-align: right;\">47000</td><td style=\"text-align: right;\">  586.82</td><td style=\"text-align: right;\">                 600</td><td style=\"text-align: right;\">                 546</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 96000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-49-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 600.0\n",
      "  episode_reward_mean: 586.06\n",
      "  episode_reward_min: 546.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 480\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 47872\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.362778902053833\n",
      "        max_q: 171.4575958251953\n",
      "        mean_q: 113.6778793334961\n",
      "        mean_td_error: 0.0469663143157959\n",
      "        min_q: 19.232181549072266\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.7623841166496277\n",
      "        max_q: 169.10313415527344\n",
      "        mean_q: 122.5348129272461\n",
      "        mean_td_error: 1.7315293550491333\n",
      "        min_q: 6.75140380859375\n",
      "    num_agent_steps_sampled: 96000\n",
      "    num_agent_steps_trained: 752064\n",
      "    num_steps_sampled: 48000\n",
      "    num_steps_trained: 376032\n",
      "    num_target_updates: 94\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.61363636363637\n",
      "    ram_util_percent: 69.0590909090909\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 290.84\n",
      "    agent-1: 295.22\n",
      "  policy_reward_min:\n",
      "    agent-0: 251.0\n",
      "    agent-1: 263.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07767506262758024\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08177956407174902\n",
      "    mean_inference_ms: 3.1555494134751827\n",
      "    mean_raw_obs_processing_ms: 0.23444885103812915\n",
      "  time_since_restore: 469.96957540512085\n",
      "  time_this_iter_s: 15.50730013847351\n",
      "  time_total_s: 469.96957540512085\n",
      "  timers:\n",
      "    learn_throughput: 1883.559\n",
      "    learn_time_ms: 16.989\n",
      "  timestamp: 1628740168\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 48000\n",
      "  training_iteration: 48\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">          469.97</td><td style=\"text-align: right;\">48000</td><td style=\"text-align: right;\">  586.06</td><td style=\"text-align: right;\">                 600</td><td style=\"text-align: right;\">                 546</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 98000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-49-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 600.0\n",
      "  episode_reward_mean: 585.62\n",
      "  episode_reward_min: 530.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 490\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 48880\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.2546868324279785\n",
      "        max_q: 175.7544403076172\n",
      "        mean_q: 128.48800659179688\n",
      "        mean_td_error: 0.4570903778076172\n",
      "        min_q: 5.8091325759887695\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.1071712970733643\n",
      "        max_q: 170.13436889648438\n",
      "        mean_q: 118.329345703125\n",
      "        mean_td_error: -0.24098989367485046\n",
      "        min_q: 1.179273247718811\n",
      "    num_agent_steps_sampled: 98000\n",
      "    num_agent_steps_trained: 768064\n",
      "    num_steps_sampled: 49000\n",
      "    num_steps_trained: 384032\n",
      "    num_target_updates: 96\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.866666666666667\n",
      "    ram_util_percent: 70.0\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 291.46\n",
      "    agent-1: 294.16\n",
      "  policy_reward_min:\n",
      "    agent-0: 251.0\n",
      "    agent-1: 237.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07778601432434014\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08191154882758568\n",
      "    mean_inference_ms: 3.1603131236906243\n",
      "    mean_raw_obs_processing_ms: 0.2347955754758842\n",
      "  time_since_restore: 480.3226020336151\n",
      "  time_this_iter_s: 10.353026628494263\n",
      "  time_total_s: 480.3226020336151\n",
      "  timers:\n",
      "    learn_throughput: 1434.732\n",
      "    learn_time_ms: 22.304\n",
      "  timestamp: 1628740178\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 49000\n",
      "  training_iteration: 49\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         480.323</td><td style=\"text-align: right;\">49000</td><td style=\"text-align: right;\">  585.62</td><td style=\"text-align: right;\">                 600</td><td style=\"text-align: right;\">                 530</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_30405_00000:\n",
      "  agent_timesteps_total: 100000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-49-50\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 600.0\n",
      "  episode_reward_mean: 585.86\n",
      "  episode_reward_min: 530.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 500\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 49888\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.5128663778305054\n",
      "        max_q: 165.98611450195312\n",
      "        mean_q: 107.05435180664062\n",
      "        mean_td_error: 0.4205758273601532\n",
      "        min_q: 18.293712615966797\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.49860724806785583\n",
      "        max_q: 171.54190063476562\n",
      "        mean_q: 117.03024291992188\n",
      "        mean_td_error: 0.6482353806495667\n",
      "        min_q: 2.658019781112671\n",
      "    num_agent_steps_sampled: 100000\n",
      "    num_agent_steps_trained: 784064\n",
      "    num_steps_sampled: 50000\n",
      "    num_steps_trained: 392032\n",
      "    num_target_updates: 98\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 35.34375\n",
      "    ram_util_percent: 69.7875\n",
      "  pid: 121225\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 302.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 291.51\n",
      "    agent-1: 294.35\n",
      "  policy_reward_min:\n",
      "    agent-0: 251.0\n",
      "    agent-1: 237.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07789801021154795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08204664990872847\n",
      "    mean_inference_ms: 3.1653091908169064\n",
      "    mean_raw_obs_processing_ms: 0.23515383376409155\n",
      "  time_since_restore: 491.6394214630127\n",
      "  time_this_iter_s: 11.316819429397583\n",
      "  time_total_s: 491.6394214630127\n",
      "  timers:\n",
      "    learn_throughput: 2079.216\n",
      "    learn_time_ms: 15.39\n",
      "  timestamp: 1628740190\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 50000\n",
      "  training_iteration: 50\n",
      "  trial_id: '30405_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>RUNNING </td><td>192.168.1.21:121225</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         491.639</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\">  585.86</td><td style=\"text-align: right;\">                 600</td><td style=\"text-align: right;\">                 530</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_62c038ae215535bfbb5745a3d2251446, 0.0/1.0 CPU_group_0_62c038ae215535bfbb5745a3d2251446)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_30405_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         491.639</td><td style=\"text-align: right;\">50000</td><td style=\"text-align: right;\">  585.86</td><td style=\"text-align: right;\">                 600</td><td style=\"text-align: right;\">                 530</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-12 15:49:50,608\tINFO tune.py:549 -- Total run time: 496.91 seconds (496.58 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# pretraining against tit-for-tat\n",
    "\n",
    "exp_name = 'TA_TEST2'\n",
    "exp_dict = {\n",
    "        'name': exp_name,\n",
    "        'run_or_experiment': 'DQN',\n",
    "        \"stop\": {\n",
    "            \"training_iteration\": 50\n",
    "        },\n",
    "        'checkpoint_freq': 0,\n",
    "        'checkpoint_at_end': True,\n",
    "        \"config\": config,\n",
    "}\n",
    "# ray.init()\n",
    "MA_result=tune.run(**exp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "08156a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up stuff for 1 v 1 training\n",
    "\n",
    "def env_creator(_):\n",
    "    return envs.TwoAgentMatrixGameEnv()\n",
    "single_env = envs.TwoAgentMatrixGameEnv()\n",
    "env_name = \"two_agent_MG_env\"\n",
    "register_env(env_name, env_creator)\n",
    "\n",
    "\n",
    "\n",
    "obs_space = single_env.observation_space\n",
    "act_space = single_env.action_space\n",
    "num_agents = single_env.num_agents\n",
    "# DQNTorchPolicy0 = DQNTorchPolicy.copy()\n",
    "\n",
    "def gen_policy(i):\n",
    "#     return (PPOTorchPolicy, obs_space, act_space, {})\n",
    "    if i == 0:\n",
    "        conf = trainer_config_dqn0\n",
    "#         conf['restore']='wtf_this better not work'#results.get_last_checkpoint()\n",
    "#         return (DQNTorchPolicy0, obs_space, act_space, conf)\n",
    "#         policy0 = copy.copy(agent0.get_policy())\n",
    "#         policy0 = DQNTorchPolicy(obs_space, act_space, conf)\n",
    "        policy0 = DQNTorchPolicy\n",
    "#         policy0 = agent0.get_policy\n",
    "#         policy0.set_weights(agent0.get_weights()['default_policy'])\n",
    "#         policy0 = lambda c: policy0\n",
    "#         return policy0\n",
    "        return (policy0, obs_space, act_space, conf)\n",
    "\n",
    "    elif i == 1:\n",
    "        conf = trainer_config_dqn1\n",
    "        return (DQNTorchPolicy, obs_space, act_space, conf)\n",
    "    else:\n",
    "        print('NO CONF!')\n",
    "    \n",
    "    return (DQNTorchPolicy, obs_space, act_space, conf)\n",
    "\n",
    "policy_graphs = {}\n",
    "\n",
    "# def restore_fn(agent_id):\n",
    "# #     if agent_id == 0:\n",
    "#     return results.get_last_checkpoint()\n",
    "# #     else:\n",
    "# #         return None\n",
    "    \n",
    "# restore_dict = {}\n",
    "# for i in range(num_agents):\n",
    "#     restore_dict['agent-' + str(i)] = restore_fn(i)\n",
    "\n",
    "for i in range(num_agents):\n",
    "    policy_graphs['agent-' + str(i)] = gen_policy(i)\n",
    "def policy_mapping_fn(agent_id):\n",
    "        return 'agent-' + str(agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "90311976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making config for 1-v-1 training\n",
    "\n",
    "config={\n",
    "    \"log_level\": \"WARN\",\n",
    "#     \"num_workers\": 3,\n",
    "#     \"num_cpus_for_driver\": 1,\n",
    "#     \"num_cpus_per_worker\": 1,\n",
    "#     \"lr\": 5e-3,\n",
    "#     \"model\":{\"fcnet_hiddens\": [1024, 512,256,32,8]},\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policy_graphs,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "    },\n",
    "    \"env\": \"two_agent_MG_env\",\n",
    "    'framework': 'torch',\n",
    "#     'framework': 'tf',\n",
    "#             \"resume\":True,\n",
    "#         \"restore\": results.get_last_checkpoint()\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09fa11f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 102000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-54-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 476.0\n",
      "  episode_reward_mean: 455.8\n",
      "  episode_reward_min: 438.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 510\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 51000\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 27.034523010253906\n",
      "        max_q: 155.60800170898438\n",
      "        mean_q: 99.80780029296875\n",
      "        mean_td_error: 1.153451919555664\n",
      "        min_q: 5.13901424407959\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 16.904460906982422\n",
      "        max_q: 173.2588653564453\n",
      "        mean_q: 112.38602447509766\n",
      "        mean_td_error: 1.7448171377182007\n",
      "        min_q: 3.8681387901306152\n",
      "    num_agent_steps_sampled: 102000\n",
      "    num_agent_steps_trained: 784128\n",
      "    num_steps_sampled: 51000\n",
      "    num_steps_trained: 392064\n",
      "    num_target_updates: 99\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.05\n",
      "    ram_util_percent: 66.46666666666665\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 280.0\n",
      "    agent-1: 262.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 230.4\n",
      "    agent-1: 225.4\n",
      "  policy_reward_min:\n",
      "    agent-0: 186.0\n",
      "    agent-1: 183.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07958035845380206\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05527809783295319\n",
      "    mean_inference_ms: 3.2396709525978173\n",
      "    mean_raw_obs_processing_ms: 0.2455716128354068\n",
      "  time_since_restore: 3.8722755908966064\n",
      "  time_this_iter_s: 3.8722755908966064\n",
      "  time_total_s: 495.5116970539093\n",
      "  timers:\n",
      "    learn_throughput: 1594.451\n",
      "    learn_time_ms: 20.07\n",
      "  timestamp: 1628740489\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 51000\n",
      "  training_iteration: 51\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         495.512</td><td style=\"text-align: right;\">51000</td><td style=\"text-align: right;\">   455.8</td><td style=\"text-align: right;\">                 476</td><td style=\"text-align: right;\">                 438</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 104000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-54-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 485.0\n",
      "  episode_reward_mean: 395.2\n",
      "  episode_reward_min: 224.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 520\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 51504\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.2104597091674805\n",
      "        max_q: 166.86334228515625\n",
      "        mean_q: 108.39470672607422\n",
      "        mean_td_error: 3.252502679824829\n",
      "        min_q: 16.601627349853516\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.75736927986145\n",
      "        max_q: 161.55484008789062\n",
      "        mean_q: 102.17491149902344\n",
      "        mean_td_error: 1.1177082061767578\n",
      "        min_q: 24.189285278320312\n",
      "    num_agent_steps_sampled: 104000\n",
      "    num_agent_steps_trained: 800128\n",
      "    num_steps_sampled: 52000\n",
      "    num_steps_trained: 400064\n",
      "    num_target_updates: 100\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.314285714285713\n",
      "    ram_util_percent: 66.32857142857142\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 371.0\n",
      "    agent-1: 262.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 228.1\n",
      "    agent-1: 167.1\n",
      "  policy_reward_min:\n",
      "    agent-0: 127.0\n",
      "    agent-1: 68.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07747491056499785\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05465994036436837\n",
      "    mean_inference_ms: 3.1673675793498606\n",
      "    mean_raw_obs_processing_ms: 0.24027576558545838\n",
      "  time_since_restore: 13.456892251968384\n",
      "  time_this_iter_s: 9.584616661071777\n",
      "  time_total_s: 505.0963137149811\n",
      "  timers:\n",
      "    learn_throughput: 1918.344\n",
      "    learn_time_ms: 16.681\n",
      "  timestamp: 1628740499\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 52000\n",
      "  training_iteration: 52\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         505.096</td><td style=\"text-align: right;\">52000</td><td style=\"text-align: right;\">   395.2</td><td style=\"text-align: right;\">                 485</td><td style=\"text-align: right;\">                 224</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 106000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-55-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 485.0\n",
      "  episode_reward_mean: 338.4\n",
      "  episode_reward_min: 212.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 530\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 52512\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.0178651809692383\n",
      "        max_q: 166.6663818359375\n",
      "        mean_q: 99.54045104980469\n",
      "        mean_td_error: -0.05738043785095215\n",
      "        min_q: 1.1824305057525635\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.7658445835113525\n",
      "        max_q: 147.7841033935547\n",
      "        mean_q: 98.72047424316406\n",
      "        mean_td_error: 0.06434124708175659\n",
      "        min_q: 19.07533836364746\n",
      "    num_agent_steps_sampled: 106000\n",
      "    num_agent_steps_trained: 816128\n",
      "    num_steps_sampled: 53000\n",
      "    num_steps_trained: 408064\n",
      "    num_target_updates: 102\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 38.84117647058823\n",
      "    ram_util_percent: 67.0529411764706\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 371.0\n",
      "    agent-1: 262.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 193.03333333333333\n",
      "    agent-1: 145.36666666666667\n",
      "  policy_reward_min:\n",
      "    agent-0: 106.0\n",
      "    agent-1: 68.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07809893709543746\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05598517267894974\n",
      "    mean_inference_ms: 3.213836787138113\n",
      "    mean_raw_obs_processing_ms: 0.24192447378990262\n",
      "  time_since_restore: 25.264708995819092\n",
      "  time_this_iter_s: 11.807816743850708\n",
      "  time_total_s: 516.9041304588318\n",
      "  timers:\n",
      "    learn_throughput: 1752.178\n",
      "    learn_time_ms: 18.263\n",
      "  timestamp: 1628740510\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 53000\n",
      "  training_iteration: 53\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         516.904</td><td style=\"text-align: right;\">53000</td><td style=\"text-align: right;\">   338.4</td><td style=\"text-align: right;\">                 485</td><td style=\"text-align: right;\">                 212</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 108000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-55-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 485.0\n",
      "  episode_reward_mean: 312.775\n",
      "  episode_reward_min: 207.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 540\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 53520\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.981268286705017\n",
      "        max_q: 158.4272003173828\n",
      "        mean_q: 87.83901977539062\n",
      "        mean_td_error: 2.948458194732666\n",
      "        min_q: 10.469043731689453\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.848538875579834\n",
      "        max_q: 143.5056915283203\n",
      "        mean_q: 84.76904296875\n",
      "        mean_td_error: -1.8629183769226074\n",
      "        min_q: 0.8064664602279663\n",
      "    num_agent_steps_sampled: 108000\n",
      "    num_agent_steps_trained: 832128\n",
      "    num_steps_sampled: 54000\n",
      "    num_steps_trained: 416064\n",
      "    num_target_updates: 104\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.166666666666664\n",
      "    ram_util_percent: 68.25999999999998\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 371.0\n",
      "    agent-1: 262.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 173.575\n",
      "    agent-1: 139.2\n",
      "  policy_reward_min:\n",
      "    agent-0: 97.0\n",
      "    agent-1: 68.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07843899630204862\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05682096128845774\n",
      "    mean_inference_ms: 3.236547366841042\n",
      "    mean_raw_obs_processing_ms: 0.2428382251022362\n",
      "  time_since_restore: 35.928033113479614\n",
      "  time_this_iter_s: 10.663324117660522\n",
      "  time_total_s: 527.5674545764923\n",
      "  timers:\n",
      "    learn_throughput: 1869.807\n",
      "    learn_time_ms: 17.114\n",
      "  timestamp: 1628740521\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 54000\n",
      "  training_iteration: 54\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         527.567</td><td style=\"text-align: right;\">54000</td><td style=\"text-align: right;\"> 312.775</td><td style=\"text-align: right;\">                 485</td><td style=\"text-align: right;\">                 207</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 110000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-55-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 485.0\n",
      "  episode_reward_mean: 292.74\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 550\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 54528\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.0569233894348145\n",
      "        max_q: 171.7708740234375\n",
      "        mean_q: 83.79629516601562\n",
      "        mean_td_error: 0.14588358998298645\n",
      "        min_q: 0.5886510014533997\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.4172863960266113\n",
      "        max_q: 145.26271057128906\n",
      "        mean_q: 80.71556854248047\n",
      "        mean_td_error: -0.3846362233161926\n",
      "        min_q: 0.5085428953170776\n",
      "    num_agent_steps_sampled: 110000\n",
      "    num_agent_steps_trained: 848128\n",
      "    num_steps_sampled: 55000\n",
      "    num_steps_trained: 424064\n",
      "    num_target_updates: 106\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.607142857142854\n",
      "    ram_util_percent: 68.4\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 371.0\n",
      "    agent-1: 262.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 161.12\n",
      "    agent-1: 131.62\n",
      "  policy_reward_min:\n",
      "    agent-0: 97.0\n",
      "    agent-1: 68.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07853448790848992\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05733383773933893\n",
      "    mean_inference_ms: 3.244251658481062\n",
      "    mean_raw_obs_processing_ms: 0.24304027560557478\n",
      "  time_since_restore: 46.17439913749695\n",
      "  time_this_iter_s: 10.246366024017334\n",
      "  time_total_s: 537.8138206005096\n",
      "  timers:\n",
      "    learn_throughput: 1914.587\n",
      "    learn_time_ms: 16.714\n",
      "  timestamp: 1628740531\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55000\n",
      "  training_iteration: 55\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         537.814</td><td style=\"text-align: right;\">55000</td><td style=\"text-align: right;\">  292.74</td><td style=\"text-align: right;\">                 485</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 112000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-55-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 485.0\n",
      "  episode_reward_mean: 280.46666666666664\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 560\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 55536\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.076622009277344\n",
      "        max_q: 168.30828857421875\n",
      "        mean_q: 85.93861389160156\n",
      "        mean_td_error: 0.6688815355300903\n",
      "        min_q: 7.587299346923828\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 7.067570209503174\n",
      "        max_q: 140.94715881347656\n",
      "        mean_q: 75.21141052246094\n",
      "        mean_td_error: 1.1967573165893555\n",
      "        min_q: 8.730504989624023\n",
      "    num_agent_steps_sampled: 112000\n",
      "    num_agent_steps_trained: 864128\n",
      "    num_steps_sampled: 56000\n",
      "    num_steps_trained: 432064\n",
      "    num_target_updates: 108\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.5\n",
      "    ram_util_percent: 67.85625\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 371.0\n",
      "    agent-1: 262.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 153.06666666666666\n",
      "    agent-1: 127.4\n",
      "  policy_reward_min:\n",
      "    agent-0: 97.0\n",
      "    agent-1: 68.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0786250222211982\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05776349081493192\n",
      "    mean_inference_ms: 3.2507070727592366\n",
      "    mean_raw_obs_processing_ms: 0.24327046226375562\n",
      "  time_since_restore: 56.86246991157532\n",
      "  time_this_iter_s: 10.68807077407837\n",
      "  time_total_s: 548.501891374588\n",
      "  timers:\n",
      "    learn_throughput: 1992.174\n",
      "    learn_time_ms: 16.063\n",
      "  timestamp: 1628740542\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 56000\n",
      "  training_iteration: 56\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         548.502</td><td style=\"text-align: right;\">56000</td><td style=\"text-align: right;\"> 280.467</td><td style=\"text-align: right;\">                 485</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 114000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-55-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 485.0\n",
      "  episode_reward_mean: 271.42857142857144\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 570\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 56544\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.798417329788208\n",
      "        max_q: 174.6477813720703\n",
      "        mean_q: 88.07584381103516\n",
      "        mean_td_error: -0.5914784669876099\n",
      "        min_q: 0.5813115835189819\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.082193851470947\n",
      "        max_q: 134.71229553222656\n",
      "        mean_q: 63.47781753540039\n",
      "        mean_td_error: -0.41161584854125977\n",
      "        min_q: 6.441315174102783\n",
      "    num_agent_steps_sampled: 114000\n",
      "    num_agent_steps_trained: 880128\n",
      "    num_steps_sampled: 57000\n",
      "    num_steps_trained: 440064\n",
      "    num_target_updates: 110\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.226666666666667\n",
      "    ram_util_percent: 68.02000000000001\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 371.0\n",
      "    agent-1: 262.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 147.5\n",
      "    agent-1: 123.92857142857143\n",
      "  policy_reward_min:\n",
      "    agent-0: 97.0\n",
      "    agent-1: 68.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07867141236693287\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05809057500141101\n",
      "    mean_inference_ms: 3.2546631797226806\n",
      "    mean_raw_obs_processing_ms: 0.24355407046810487\n",
      "  time_since_restore: 67.4557716846466\n",
      "  time_this_iter_s: 10.593301773071289\n",
      "  time_total_s: 559.0951931476593\n",
      "  timers:\n",
      "    learn_throughput: 1863.817\n",
      "    learn_time_ms: 17.169\n",
      "  timestamp: 1628740553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 57000\n",
      "  training_iteration: 57\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         559.095</td><td style=\"text-align: right;\">57000</td><td style=\"text-align: right;\"> 271.429</td><td style=\"text-align: right;\">                 485</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 116000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-56-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 485.0\n",
      "  episode_reward_mean: 265.4\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 580\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 57552\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.336350917816162\n",
      "        max_q: 165.20941162109375\n",
      "        mean_q: 79.26375579833984\n",
      "        mean_td_error: -0.832337498664856\n",
      "        min_q: 7.870538711547852\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.8760918378829956\n",
      "        max_q: 142.0489501953125\n",
      "        mean_q: 78.19995880126953\n",
      "        mean_td_error: -0.3201208710670471\n",
      "        min_q: 0.7437577843666077\n",
      "    num_agent_steps_sampled: 116000\n",
      "    num_agent_steps_trained: 896128\n",
      "    num_steps_sampled: 58000\n",
      "    num_steps_trained: 448064\n",
      "    num_target_updates: 112\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.92142857142857\n",
      "    ram_util_percent: 68.05714285714286\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 371.0\n",
      "    agent-1: 262.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 143.6375\n",
      "    agent-1: 121.7625\n",
      "  policy_reward_min:\n",
      "    agent-0: 97.0\n",
      "    agent-1: 68.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07860501539842948\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05828778622388309\n",
      "    mean_inference_ms: 3.253092274414007\n",
      "    mean_raw_obs_processing_ms: 0.24347967813647203\n",
      "  time_since_restore: 77.25078129768372\n",
      "  time_this_iter_s: 9.79500961303711\n",
      "  time_total_s: 568.8902027606964\n",
      "  timers:\n",
      "    learn_throughput: 1999.594\n",
      "    learn_time_ms: 16.003\n",
      "  timestamp: 1628740563\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 58000\n",
      "  training_iteration: 58\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">          568.89</td><td style=\"text-align: right;\">58000</td><td style=\"text-align: right;\">   265.4</td><td style=\"text-align: right;\">                 485</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 118000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-56-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 485.0\n",
      "  episode_reward_mean: 264.97777777777776\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 590\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 58560\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.8317086696624756\n",
      "        max_q: 136.72381591796875\n",
      "        mean_q: 79.58561706542969\n",
      "        mean_td_error: -3.0675742626190186\n",
      "        min_q: 9.944418907165527\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.8328142166137695\n",
      "        max_q: 138.6991424560547\n",
      "        mean_q: 60.861820220947266\n",
      "        mean_td_error: 3.5220818519592285\n",
      "        min_q: 7.043130397796631\n",
      "    num_agent_steps_sampled: 118000\n",
      "    num_agent_steps_trained: 912128\n",
      "    num_steps_sampled: 59000\n",
      "    num_steps_trained: 456064\n",
      "    num_target_updates: 114\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.779999999999998\n",
      "    ram_util_percent: 68.33999999999999\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 371.0\n",
      "    agent-1: 262.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 140.87777777777777\n",
      "    agent-1: 124.1\n",
      "  policy_reward_min:\n",
      "    agent-0: 82.0\n",
      "    agent-1: 68.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07858114479254702\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05847003924111512\n",
      "    mean_inference_ms: 3.2522373329532632\n",
      "    mean_raw_obs_processing_ms: 0.24344063387032788\n",
      "  time_since_restore: 88.1386501789093\n",
      "  time_this_iter_s: 10.887868881225586\n",
      "  time_total_s: 579.778071641922\n",
      "  timers:\n",
      "    learn_throughput: 1853.347\n",
      "    learn_time_ms: 17.266\n",
      "  timestamp: 1628740574\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 59000\n",
      "  training_iteration: 59\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         579.778</td><td style=\"text-align: right;\">59000</td><td style=\"text-align: right;\"> 264.978</td><td style=\"text-align: right;\">                 485</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 120000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-56-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 485.0\n",
      "  episode_reward_mean: 261.37\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 600\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 59568\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 5.160741806030273\n",
      "        max_q: 163.47137451171875\n",
      "        mean_q: 79.5583267211914\n",
      "        mean_td_error: 3.3249104022979736\n",
      "        min_q: 9.203466415405273\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.3490219116210938\n",
      "        max_q: 140.30027770996094\n",
      "        mean_q: 74.22000885009766\n",
      "        mean_td_error: -2.4070630073547363\n",
      "        min_q: 0.3736395239830017\n",
      "    num_agent_steps_sampled: 120000\n",
      "    num_agent_steps_trained: 928128\n",
      "    num_steps_sampled: 60000\n",
      "    num_steps_trained: 464064\n",
      "    num_target_updates: 116\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.714285714285715\n",
      "    ram_util_percent: 68.54285714285714\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 371.0\n",
      "    agent-1: 262.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 139.26\n",
      "    agent-1: 122.11\n",
      "  policy_reward_min:\n",
      "    agent-0: 82.0\n",
      "    agent-1: 68.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07850212958918626\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05857346337600436\n",
      "    mean_inference_ms: 3.248755667375452\n",
      "    mean_raw_obs_processing_ms: 0.24324900667629087\n",
      "  time_since_restore: 97.81787848472595\n",
      "  time_this_iter_s: 9.67922830581665\n",
      "  time_total_s: 589.4572999477386\n",
      "  timers:\n",
      "    learn_throughput: 1940.244\n",
      "    learn_time_ms: 16.493\n",
      "  timestamp: 1628740583\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 60000\n",
      "  training_iteration: 60\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         589.457</td><td style=\"text-align: right;\">60000</td><td style=\"text-align: right;\">  261.37</td><td style=\"text-align: right;\">                 485</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 122000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-56-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 485.0\n",
      "  episode_reward_mean: 239.14\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 610\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 60576\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.3012568950653076\n",
      "        max_q: 175.1764373779297\n",
      "        mean_q: 72.87931823730469\n",
      "        mean_td_error: 0.493025004863739\n",
      "        min_q: 1.7313674688339233\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.3489528894424438\n",
      "        max_q: 141.15220642089844\n",
      "        mean_q: 57.02471160888672\n",
      "        mean_td_error: -2.767184019088745\n",
      "        min_q: 2.051384449005127\n",
      "    num_agent_steps_sampled: 122000\n",
      "    num_agent_steps_trained: 944128\n",
      "    num_steps_sampled: 61000\n",
      "    num_steps_trained: 472064\n",
      "    num_target_updates: 118\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.326666666666668\n",
      "    ram_util_percent: 68.54\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 371.0\n",
      "    agent-1: 225.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 128.52\n",
      "    agent-1: 110.62\n",
      "  policy_reward_min:\n",
      "    agent-0: 82.0\n",
      "    agent-1: 68.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07830893022670547\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05900531402249928\n",
      "    mean_inference_ms: 3.2460807471256823\n",
      "    mean_raw_obs_processing_ms: 0.24282624448155246\n",
      "  time_since_restore: 108.18616819381714\n",
      "  time_this_iter_s: 10.368289709091187\n",
      "  time_total_s: 599.8255896568298\n",
      "  timers:\n",
      "    learn_throughput: 1953.807\n",
      "    learn_time_ms: 16.378\n",
      "  timestamp: 1628740594\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 61000\n",
      "  training_iteration: 61\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         599.826</td><td style=\"text-align: right;\">61000</td><td style=\"text-align: right;\">  239.14</td><td style=\"text-align: right;\">                 485</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 124000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-56-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 307.0\n",
      "  episode_reward_mean: 227.89\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 620\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 61584\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.252793788909912\n",
      "        max_q: 174.2864532470703\n",
      "        mean_q: 71.60136413574219\n",
      "        mean_td_error: -2.785088539123535\n",
      "        min_q: 3.7202186584472656\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 6.910184860229492\n",
      "        max_q: 145.00677490234375\n",
      "        mean_q: 56.95759201049805\n",
      "        mean_td_error: -4.259740829467773\n",
      "        min_q: 2.3570005893707275\n",
      "    num_agent_steps_sampled: 124000\n",
      "    num_agent_steps_trained: 960128\n",
      "    num_steps_sampled: 62000\n",
      "    num_steps_trained: 480064\n",
      "    num_target_updates: 120\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.113333333333333\n",
      "    ram_util_percent: 68.73333333333333\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 202.0\n",
      "    agent-1: 211.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 118.02\n",
      "    agent-1: 109.87\n",
      "  policy_reward_min:\n",
      "    agent-0: 82.0\n",
      "    agent-1: 82.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07853489368929535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05957500219141673\n",
      "    mean_inference_ms: 3.2577304383446837\n",
      "    mean_raw_obs_processing_ms: 0.24348100631918407\n",
      "  time_since_restore: 118.6856644153595\n",
      "  time_this_iter_s: 10.499496221542358\n",
      "  time_total_s: 610.3250858783722\n",
      "  timers:\n",
      "    learn_throughput: 1744.943\n",
      "    learn_time_ms: 18.339\n",
      "  timestamp: 1628740604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 62000\n",
      "  training_iteration: 62\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         610.325</td><td style=\"text-align: right;\">62000</td><td style=\"text-align: right;\">  227.89</td><td style=\"text-align: right;\">                 307</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 126000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-56-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 307.0\n",
      "  episode_reward_mean: 228.24\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 630\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 62592\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.218191146850586\n",
      "        max_q: 117.72379302978516\n",
      "        mean_q: 51.02492141723633\n",
      "        mean_td_error: -2.3031182289123535\n",
      "        min_q: 8.937317848205566\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.659163475036621\n",
      "        max_q: 140.87852478027344\n",
      "        mean_q: 56.38914108276367\n",
      "        mean_td_error: -5.166277885437012\n",
      "        min_q: 1.4110609292984009\n",
      "    num_agent_steps_sampled: 126000\n",
      "    num_agent_steps_trained: 976128\n",
      "    num_steps_sampled: 63000\n",
      "    num_steps_trained: 488064\n",
      "    num_target_updates: 122\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.787499999999994\n",
      "    ram_util_percent: 68.93125\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 202.0\n",
      "    agent-1: 211.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 117.02\n",
      "    agent-1: 111.22\n",
      "  policy_reward_min:\n",
      "    agent-0: 82.0\n",
      "    agent-1: 82.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07840344533115891\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.059731029865514686\n",
      "    mean_inference_ms: 3.250185359379501\n",
      "    mean_raw_obs_processing_ms: 0.2432182717417179\n",
      "  time_since_restore: 129.8956332206726\n",
      "  time_this_iter_s: 11.20996880531311\n",
      "  time_total_s: 621.5350546836853\n",
      "  timers:\n",
      "    learn_throughput: 1982.197\n",
      "    learn_time_ms: 16.144\n",
      "  timestamp: 1628740615\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63000\n",
      "  training_iteration: 63\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         621.535</td><td style=\"text-align: right;\">63000</td><td style=\"text-align: right;\">  228.24</td><td style=\"text-align: right;\">                 307</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 128000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-57-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 307.0\n",
      "  episode_reward_mean: 226.42\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 640\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 63600\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.5814411640167236\n",
      "        max_q: 146.86434936523438\n",
      "        mean_q: 57.83948516845703\n",
      "        mean_td_error: 0.7932612895965576\n",
      "        min_q: 10.97075080871582\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 6.519102096557617\n",
      "        max_q: 111.9003677368164\n",
      "        mean_q: 50.18735885620117\n",
      "        mean_td_error: -6.785621643066406\n",
      "        min_q: 5.299564838409424\n",
      "    num_agent_steps_sampled: 128000\n",
      "    num_agent_steps_trained: 992128\n",
      "    num_steps_sampled: 64000\n",
      "    num_steps_trained: 496064\n",
      "    num_target_updates: 124\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.42142857142857\n",
      "    ram_util_percent: 68.86428571428569\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 202.0\n",
      "    agent-1: 211.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 117.01\n",
      "    agent-1: 109.41\n",
      "  policy_reward_min:\n",
      "    agent-0: 82.0\n",
      "    agent-1: 82.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07823480295194588\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.059802192051593955\n",
      "    mean_inference_ms: 3.2417136360985443\n",
      "    mean_raw_obs_processing_ms: 0.2428245568387686\n",
      "  time_since_restore: 139.94169425964355\n",
      "  time_this_iter_s: 10.046061038970947\n",
      "  time_total_s: 631.5811157226562\n",
      "  timers:\n",
      "    learn_throughput: 1916.036\n",
      "    learn_time_ms: 16.701\n",
      "  timestamp: 1628740626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 64000\n",
      "  training_iteration: 64\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">         631.581</td><td style=\"text-align: right;\">64000</td><td style=\"text-align: right;\">  226.42</td><td style=\"text-align: right;\">                 307</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 130000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-57-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 307.0\n",
      "  episode_reward_mean: 227.11\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 650\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 64608\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.4175539016723633\n",
      "        max_q: 186.90281677246094\n",
      "        mean_q: 64.67430877685547\n",
      "        mean_td_error: 2.538435220718384\n",
      "        min_q: 0.9098168611526489\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 5.184064865112305\n",
      "        max_q: 121.99006652832031\n",
      "        mean_q: 54.71598434448242\n",
      "        mean_td_error: 2.281181573867798\n",
      "        min_q: 1.3986910581588745\n",
      "    num_agent_steps_sampled: 130000\n",
      "    num_agent_steps_trained: 1008128\n",
      "    num_steps_sampled: 65000\n",
      "    num_steps_trained: 504064\n",
      "    num_target_updates: 126\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.807142857142857\n",
      "    ram_util_percent: 69.10714285714286\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 202.0\n",
      "    agent-1: 211.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 117.63\n",
      "    agent-1: 109.48\n",
      "  policy_reward_min:\n",
      "    agent-0: 82.0\n",
      "    agent-1: 82.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07808329168657778\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.059840737252680985\n",
      "    mean_inference_ms: 3.2344162225018454\n",
      "    mean_raw_obs_processing_ms: 0.24248470196866645\n",
      "  time_since_restore: 149.63118648529053\n",
      "  time_this_iter_s: 9.689492225646973\n",
      "  time_total_s: 641.2706079483032\n",
      "  timers:\n",
      "    learn_throughput: 1996.928\n",
      "    learn_time_ms: 16.025\n",
      "  timestamp: 1628740635\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 65000\n",
      "  training_iteration: 65\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         641.271</td><td style=\"text-align: right;\">65000</td><td style=\"text-align: right;\">  227.11</td><td style=\"text-align: right;\">                 307</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 132000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-57-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 307.0\n",
      "  episode_reward_mean: 228.03\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 660\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 65616\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 5.396130084991455\n",
      "        max_q: 130.62789916992188\n",
      "        mean_q: 50.44013214111328\n",
      "        mean_td_error: -4.604939937591553\n",
      "        min_q: 9.890642166137695\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.7819762229919434\n",
      "        max_q: 135.50457763671875\n",
      "        mean_q: 55.62464141845703\n",
      "        mean_td_error: -1.836895227432251\n",
      "        min_q: 11.827009201049805\n",
      "    num_agent_steps_sampled: 132000\n",
      "    num_agent_steps_trained: 1024128\n",
      "    num_steps_sampled: 66000\n",
      "    num_steps_trained: 512064\n",
      "    num_target_updates: 128\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.466666666666672\n",
      "    ram_util_percent: 69.41333333333334\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 202.0\n",
      "    agent-1: 211.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 119.34\n",
      "    agent-1: 108.69\n",
      "  policy_reward_min:\n",
      "    agent-0: 82.0\n",
      "    agent-1: 80.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07790780590706282\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.059828593287445456\n",
      "    mean_inference_ms: 3.227393456148231\n",
      "    mean_raw_obs_processing_ms: 0.2420669883706135\n",
      "  time_since_restore: 160.09066224098206\n",
      "  time_this_iter_s: 10.459475755691528\n",
      "  time_total_s: 651.7300837039948\n",
      "  timers:\n",
      "    learn_throughput: 1901.281\n",
      "    learn_time_ms: 16.831\n",
      "  timestamp: 1628740646\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 66000\n",
      "  training_iteration: 66\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">          651.73</td><td style=\"text-align: right;\">66000</td><td style=\"text-align: right;\">  228.03</td><td style=\"text-align: right;\">                 307</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 134000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-57-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 307.0\n",
      "  episode_reward_mean: 227.48\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 670\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 66624\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.6320525407791138\n",
      "        max_q: 171.72801208496094\n",
      "        mean_q: 61.570884704589844\n",
      "        mean_td_error: 3.8097949028015137\n",
      "        min_q: 1.3846269845962524\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.190923690795898\n",
      "        max_q: 142.23977661132812\n",
      "        mean_q: 52.3394660949707\n",
      "        mean_td_error: -1.611850619316101\n",
      "        min_q: 0.9434552192687988\n",
      "    num_agent_steps_sampled: 134000\n",
      "    num_agent_steps_trained: 1040128\n",
      "    num_steps_sampled: 67000\n",
      "    num_steps_trained: 520064\n",
      "    num_target_updates: 130\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.25333333333333\n",
      "    ram_util_percent: 69.37333333333336\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 202.0\n",
      "    agent-1: 211.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 118.59\n",
      "    agent-1: 108.89\n",
      "  policy_reward_min:\n",
      "    agent-0: 82.0\n",
      "    agent-1: 80.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0777408264334931\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05979934982515126\n",
      "    mean_inference_ms: 3.2202713025792127\n",
      "    mean_raw_obs_processing_ms: 0.24153644082385425\n",
      "  time_since_restore: 170.33248043060303\n",
      "  time_this_iter_s: 10.241818189620972\n",
      "  time_total_s: 661.9719018936157\n",
      "  timers:\n",
      "    learn_throughput: 1791.395\n",
      "    learn_time_ms: 17.863\n",
      "  timestamp: 1628740656\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 67000\n",
      "  training_iteration: 67\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         661.972</td><td style=\"text-align: right;\">67000</td><td style=\"text-align: right;\">  227.48</td><td style=\"text-align: right;\">                 307</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 136000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-57-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 307.0\n",
      "  episode_reward_mean: 226.57\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 680\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 67632\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.9384105205535889\n",
      "        max_q: 100.11404418945312\n",
      "        mean_q: 47.128780364990234\n",
      "        mean_td_error: -2.4331581592559814\n",
      "        min_q: 3.6264660358428955\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.614291191101074\n",
      "        max_q: 127.70350646972656\n",
      "        mean_q: 50.437496185302734\n",
      "        mean_td_error: -1.6174745559692383\n",
      "        min_q: 2.306943893432617\n",
      "    num_agent_steps_sampled: 136000\n",
      "    num_agent_steps_trained: 1056128\n",
      "    num_steps_sampled: 68000\n",
      "    num_steps_trained: 528064\n",
      "    num_target_updates: 132\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.06666666666667\n",
      "    ram_util_percent: 69.29999999999998\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 202.0\n",
      "    agent-1: 211.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 118.11\n",
      "    agent-1: 108.46\n",
      "  policy_reward_min:\n",
      "    agent-0: 82.0\n",
      "    agent-1: 80.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07765797651351336\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05981423901118168\n",
      "    mean_inference_ms: 3.216972924275845\n",
      "    mean_raw_obs_processing_ms: 0.2412355650861441\n",
      "  time_since_restore: 180.8868601322174\n",
      "  time_this_iter_s: 10.55437970161438\n",
      "  time_total_s: 672.5262815952301\n",
      "  timers:\n",
      "    learn_throughput: 1693.736\n",
      "    learn_time_ms: 18.893\n",
      "  timestamp: 1628740667\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 68000\n",
      "  training_iteration: 68\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">         672.526</td><td style=\"text-align: right;\">68000</td><td style=\"text-align: right;\">  226.57</td><td style=\"text-align: right;\">                 307</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 138000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-57-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 284.0\n",
      "  episode_reward_mean: 221.25\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 690\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 68640\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.2042771577835083\n",
      "        max_q: 138.10850524902344\n",
      "        mean_q: 51.239376068115234\n",
      "        mean_td_error: -1.1651809215545654\n",
      "        min_q: 4.032567501068115\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.8207602500915527\n",
      "        max_q: 134.83126831054688\n",
      "        mean_q: 40.298221588134766\n",
      "        mean_td_error: -1.479238510131836\n",
      "        min_q: 4.046413898468018\n",
      "    num_agent_steps_sampled: 138000\n",
      "    num_agent_steps_trained: 1072128\n",
      "    num_steps_sampled: 69000\n",
      "    num_steps_trained: 536064\n",
      "    num_target_updates: 134\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.499999999999996\n",
      "    ram_util_percent: 69.14615384615385\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 202.0\n",
      "    agent-1: 137.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 116.5\n",
      "    agent-1: 104.75\n",
      "  policy_reward_min:\n",
      "    agent-0: 92.0\n",
      "    agent-1: 80.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07751253446340892\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05977285654315864\n",
      "    mean_inference_ms: 3.211480343185772\n",
      "    mean_raw_obs_processing_ms: 0.2408182865699948\n",
      "  time_since_restore: 190.31722259521484\n",
      "  time_this_iter_s: 9.430362462997437\n",
      "  time_total_s: 681.9566440582275\n",
      "  timers:\n",
      "    learn_throughput: 1905.741\n",
      "    learn_time_ms: 16.791\n",
      "  timestamp: 1628740676\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 69000\n",
      "  training_iteration: 69\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         681.957</td><td style=\"text-align: right;\">69000</td><td style=\"text-align: right;\">  221.25</td><td style=\"text-align: right;\">                 284</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 140000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-58-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 284.0\n",
      "  episode_reward_mean: 219.36\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 700\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 69648\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.889967679977417\n",
      "        max_q: 95.12759399414062\n",
      "        mean_q: 39.46978759765625\n",
      "        mean_td_error: -1.2895334959030151\n",
      "        min_q: 14.434125900268555\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.8268306255340576\n",
      "        max_q: 140.70652770996094\n",
      "        mean_q: 51.932098388671875\n",
      "        mean_td_error: -5.138312339782715\n",
      "        min_q: 0.7712867259979248\n",
      "    num_agent_steps_sampled: 140000\n",
      "    num_agent_steps_trained: 1088128\n",
      "    num_steps_sampled: 70000\n",
      "    num_steps_trained: 544064\n",
      "    num_target_updates: 136\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.12857142857143\n",
      "    ram_util_percent: 69.12857142857143\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 202.0\n",
      "    agent-1: 137.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 114.83\n",
      "    agent-1: 104.53\n",
      "  policy_reward_min:\n",
      "    agent-0: 92.0\n",
      "    agent-1: 80.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07738390248333353\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05973865303006254\n",
      "    mean_inference_ms: 3.2068379061549064\n",
      "    mean_raw_obs_processing_ms: 0.2404307565778713\n",
      "  time_since_restore: 199.6023542881012\n",
      "  time_this_iter_s: 9.285131692886353\n",
      "  time_total_s: 691.2417757511139\n",
      "  timers:\n",
      "    learn_throughput: 2016.005\n",
      "    learn_time_ms: 15.873\n",
      "  timestamp: 1628740685\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 70000\n",
      "  training_iteration: 70\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         691.242</td><td style=\"text-align: right;\">70000</td><td style=\"text-align: right;\">  219.36</td><td style=\"text-align: right;\">                 284</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 142000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-58-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 260.0\n",
      "  episode_reward_mean: 216.87\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 710\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 70656\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.386252403259277\n",
      "        max_q: 142.27720642089844\n",
      "        mean_q: 42.323387145996094\n",
      "        mean_td_error: 1.495397925376892\n",
      "        min_q: 2.1437907218933105\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.719216346740723\n",
      "        max_q: 124.18888092041016\n",
      "        mean_q: 46.16672897338867\n",
      "        mean_td_error: -4.603818893432617\n",
      "        min_q: 0.8599491119384766\n",
      "    num_agent_steps_sampled: 142000\n",
      "    num_agent_steps_trained: 1104128\n",
      "    num_steps_sampled: 71000\n",
      "    num_steps_trained: 552064\n",
      "    num_target_updates: 138\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.630769230769236\n",
      "    ram_util_percent: 69.18461538461541\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 180.0\n",
      "    agent-1: 132.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 112.86\n",
      "    agent-1: 104.01\n",
      "  policy_reward_min:\n",
      "    agent-0: 92.0\n",
      "    agent-1: 80.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0772323884788088\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05966437469531782\n",
      "    mean_inference_ms: 3.2009666122452383\n",
      "    mean_raw_obs_processing_ms: 0.23995220213429863\n",
      "  time_since_restore: 208.91264748573303\n",
      "  time_this_iter_s: 9.310293197631836\n",
      "  time_total_s: 700.5520689487457\n",
      "  timers:\n",
      "    learn_throughput: 1941.673\n",
      "    learn_time_ms: 16.481\n",
      "  timestamp: 1628740695\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71000\n",
      "  training_iteration: 71\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         700.552</td><td style=\"text-align: right;\">71000</td><td style=\"text-align: right;\">  216.87</td><td style=\"text-align: right;\">                 260</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 144000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-58-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 260.0\n",
      "  episode_reward_mean: 216.45\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 720\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 71664\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.86487340927124\n",
      "        max_q: 198.5260009765625\n",
      "        mean_q: 44.400352478027344\n",
      "        mean_td_error: 0.7350262999534607\n",
      "        min_q: 5.913638591766357\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.3413686752319336\n",
      "        max_q: 142.85049438476562\n",
      "        mean_q: 47.463375091552734\n",
      "        mean_td_error: -0.8971890211105347\n",
      "        min_q: 0.9188774228096008\n",
      "    num_agent_steps_sampled: 144000\n",
      "    num_agent_steps_trained: 1120128\n",
      "    num_steps_sampled: 72000\n",
      "    num_steps_trained: 560064\n",
      "    num_target_updates: 140\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.315384615384612\n",
      "    ram_util_percent: 69.33076923076922\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 180.0\n",
      "    agent-1: 132.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 112.05\n",
      "    agent-1: 104.4\n",
      "  policy_reward_min:\n",
      "    agent-0: 92.0\n",
      "    agent-1: 80.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07705231874979092\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.059548883268056726\n",
      "    mean_inference_ms: 3.1937105431644452\n",
      "    mean_raw_obs_processing_ms: 0.2393583824870252\n",
      "  time_since_restore: 218.23911476135254\n",
      "  time_this_iter_s: 9.326467275619507\n",
      "  time_total_s: 709.8785362243652\n",
      "  timers:\n",
      "    learn_throughput: 1846.862\n",
      "    learn_time_ms: 17.327\n",
      "  timestamp: 1628740704\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 72000\n",
      "  training_iteration: 72\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         709.879</td><td style=\"text-align: right;\">72000</td><td style=\"text-align: right;\">  216.45</td><td style=\"text-align: right;\">                 260</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 146000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-58-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 260.0\n",
      "  episode_reward_mean: 215.18\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 730\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 72672\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.888088226318359\n",
      "        max_q: 150.69102478027344\n",
      "        mean_q: 47.677467346191406\n",
      "        mean_td_error: 1.3710353374481201\n",
      "        min_q: 0.8013239502906799\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.5677447319030762\n",
      "        max_q: 136.89083862304688\n",
      "        mean_q: 34.99897766113281\n",
      "        mean_td_error: -6.477100372314453\n",
      "        min_q: 1.4599459171295166\n",
      "    num_agent_steps_sampled: 146000\n",
      "    num_agent_steps_trained: 1136128\n",
      "    num_steps_sampled: 73000\n",
      "    num_steps_trained: 568064\n",
      "    num_target_updates: 142\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.807142857142857\n",
      "    ram_util_percent: 69.44285714285714\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 180.0\n",
      "    agent-1: 119.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 111.49\n",
      "    agent-1: 103.69\n",
      "  policy_reward_min:\n",
      "    agent-0: 98.0\n",
      "    agent-1: 80.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0768037330507369\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05936515591583774\n",
      "    mean_inference_ms: 3.183080139522803\n",
      "    mean_raw_obs_processing_ms: 0.23856777246388397\n",
      "  time_since_restore: 227.50949597358704\n",
      "  time_this_iter_s: 9.270381212234497\n",
      "  time_total_s: 719.1489174365997\n",
      "  timers:\n",
      "    learn_throughput: 2008.042\n",
      "    learn_time_ms: 15.936\n",
      "  timestamp: 1628740713\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 73000\n",
      "  training_iteration: 73\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         719.149</td><td style=\"text-align: right;\">73000</td><td style=\"text-align: right;\">  215.18</td><td style=\"text-align: right;\">                 260</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 148000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-58-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 260.0\n",
      "  episode_reward_mean: 214.44\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 740\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 73680\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 5.789597034454346\n",
      "        max_q: 73.98471069335938\n",
      "        mean_q: 39.451290130615234\n",
      "        mean_td_error: 1.2514309883117676\n",
      "        min_q: 11.235620498657227\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 6.6267523765563965\n",
      "        max_q: 137.009033203125\n",
      "        mean_q: 37.65888595581055\n",
      "        mean_td_error: 1.6334218978881836\n",
      "        min_q: 7.879613399505615\n",
      "    num_agent_steps_sampled: 148000\n",
      "    num_agent_steps_trained: 1152128\n",
      "    num_steps_sampled: 74000\n",
      "    num_steps_trained: 576064\n",
      "    num_target_updates: 144\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.299999999999997\n",
      "    ram_util_percent: 69.47692307692309\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 180.0\n",
      "    agent-1: 119.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 110.52\n",
      "    agent-1: 103.92\n",
      "  policy_reward_min:\n",
      "    agent-0: 98.0\n",
      "    agent-1: 80.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0765539378399466\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.059176459464779806\n",
      "    mean_inference_ms: 3.1722406543914605\n",
      "    mean_raw_obs_processing_ms: 0.23779202558578702\n",
      "  time_since_restore: 236.78552722930908\n",
      "  time_this_iter_s: 9.276031255722046\n",
      "  time_total_s: 728.4249486923218\n",
      "  timers:\n",
      "    learn_throughput: 1980.682\n",
      "    learn_time_ms: 16.156\n",
      "  timestamp: 1628740723\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 74000\n",
      "  training_iteration: 74\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         728.425</td><td style=\"text-align: right;\">74000</td><td style=\"text-align: right;\">  214.44</td><td style=\"text-align: right;\">                 260</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 150000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-58-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 260.0\n",
      "  episode_reward_mean: 214.16\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 750\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 74688\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.4063706398010254\n",
      "        max_q: 164.09193420410156\n",
      "        mean_q: 45.25761032104492\n",
      "        mean_td_error: -1.0179522037506104\n",
      "        min_q: 6.167726993560791\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.566744804382324\n",
      "        max_q: 136.6692352294922\n",
      "        mean_q: 34.537445068359375\n",
      "        mean_td_error: 0.2463243007659912\n",
      "        min_q: 2.058642625808716\n",
      "    num_agent_steps_sampled: 150000\n",
      "    num_agent_steps_trained: 1168128\n",
      "    num_steps_sampled: 75000\n",
      "    num_steps_trained: 584064\n",
      "    num_target_updates: 146\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.015384615384615\n",
      "    ram_util_percent: 69.76923076923075\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 180.0\n",
      "    agent-1: 135.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 108.73\n",
      "    agent-1: 105.43\n",
      "  policy_reward_min:\n",
      "    agent-0: 95.0\n",
      "    agent-1: 80.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07632225043484875\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.058999302598473004\n",
      "    mean_inference_ms: 3.1621989078263613\n",
      "    mean_raw_obs_processing_ms: 0.23707126547328156\n",
      "  time_since_restore: 246.18292236328125\n",
      "  time_this_iter_s: 9.397395133972168\n",
      "  time_total_s: 737.822343826294\n",
      "  timers:\n",
      "    learn_throughput: 1873.851\n",
      "    learn_time_ms: 17.077\n",
      "  timestamp: 1628740732\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 75000\n",
      "  training_iteration: 75\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         737.822</td><td style=\"text-align: right;\">75000</td><td style=\"text-align: right;\">  214.16</td><td style=\"text-align: right;\">                 260</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 152000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-59-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 248.0\n",
      "  episode_reward_mean: 212.84\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 760\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 75696\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 5.231039047241211\n",
      "        max_q: 144.49801635742188\n",
      "        mean_q: 43.009117126464844\n",
      "        mean_td_error: -6.099702835083008\n",
      "        min_q: 12.490446090698242\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.8951289653778076\n",
      "        max_q: 114.34772491455078\n",
      "        mean_q: 37.062522888183594\n",
      "        mean_td_error: 0.08989331126213074\n",
      "        min_q: 0.5626968145370483\n",
      "    num_agent_steps_sampled: 152000\n",
      "    num_agent_steps_trained: 1184128\n",
      "    num_steps_sampled: 76000\n",
      "    num_steps_trained: 592064\n",
      "    num_target_updates: 148\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.92142857142857\n",
      "    ram_util_percent: 69.82142857142857\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 139.0\n",
      "    agent-1: 135.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 106.22\n",
      "    agent-1: 106.62\n",
      "  policy_reward_min:\n",
      "    agent-0: 92.0\n",
      "    agent-1: 95.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07607550421248277\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05880223921401781\n",
      "    mean_inference_ms: 3.149985004354845\n",
      "    mean_raw_obs_processing_ms: 0.23630129973808028\n",
      "  time_since_restore: 255.49294137954712\n",
      "  time_this_iter_s: 9.31001901626587\n",
      "  time_total_s: 747.1323628425598\n",
      "  timers:\n",
      "    learn_throughput: 1956.86\n",
      "    learn_time_ms: 16.353\n",
      "  timestamp: 1628740741\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 76000\n",
      "  training_iteration: 76\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         747.132</td><td style=\"text-align: right;\">76000</td><td style=\"text-align: right;\">  212.84</td><td style=\"text-align: right;\">                 248</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 154000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-59-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 248.0\n",
      "  episode_reward_mean: 212.99\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 770\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 76704\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.253683090209961\n",
      "        max_q: 193.74217224121094\n",
      "        mean_q: 45.74673843383789\n",
      "        mean_td_error: 3.8963985443115234\n",
      "        min_q: 7.78985595703125\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.5285799503326416\n",
      "        max_q: 132.46688842773438\n",
      "        mean_q: 36.619293212890625\n",
      "        mean_td_error: -2.3937876224517822\n",
      "        min_q: 3.338693380355835\n",
      "    num_agent_steps_sampled: 154000\n",
      "    num_agent_steps_trained: 1200128\n",
      "    num_steps_sampled: 77000\n",
      "    num_steps_trained: 600064\n",
      "    num_target_updates: 150\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 29.513333333333335\n",
      "    ram_util_percent: 69.99333333333334\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 139.0\n",
      "    agent-1: 135.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 106.02\n",
      "    agent-1: 106.97\n",
      "  policy_reward_min:\n",
      "    agent-0: 92.0\n",
      "    agent-1: 95.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07583827645405121\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05861861589764081\n",
      "    mean_inference_ms: 3.1385050697457895\n",
      "    mean_raw_obs_processing_ms: 0.2355694536035459\n",
      "  time_since_restore: 265.97230076789856\n",
      "  time_this_iter_s: 10.47935938835144\n",
      "  time_total_s: 757.6117222309113\n",
      "  timers:\n",
      "    learn_throughput: 1859.19\n",
      "    learn_time_ms: 17.212\n",
      "  timestamp: 1628740752\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 77000\n",
      "  training_iteration: 77\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         757.612</td><td style=\"text-align: right;\">77000</td><td style=\"text-align: right;\">  212.99</td><td style=\"text-align: right;\">                 248</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 156000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-59-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 248.0\n",
      "  episode_reward_mean: 213.12\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 780\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 77712\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.9636889696121216\n",
      "        max_q: 145.17996215820312\n",
      "        mean_q: 47.931358337402344\n",
      "        mean_td_error: -5.2056474685668945\n",
      "        min_q: 7.325664043426514\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.3613643646240234\n",
      "        max_q: 135.59024047851562\n",
      "        mean_q: 37.4146728515625\n",
      "        mean_td_error: -1.0013725757598877\n",
      "        min_q: 0.6918333768844604\n",
      "    num_agent_steps_sampled: 156000\n",
      "    num_agent_steps_trained: 1216128\n",
      "    num_steps_sampled: 78000\n",
      "    num_steps_trained: 608064\n",
      "    num_target_updates: 152\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.033333333333335\n",
      "    ram_util_percent: 69.99333333333334\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 139.0\n",
      "    agent-1: 135.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 105.66\n",
      "    agent-1: 107.46\n",
      "  policy_reward_min:\n",
      "    agent-0: 92.0\n",
      "    agent-1: 95.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.075621804121632\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.058450443657344954\n",
      "    mean_inference_ms: 3.127965206487177\n",
      "    mean_raw_obs_processing_ms: 0.2349007842448566\n",
      "  time_since_restore: 277.0226263999939\n",
      "  time_this_iter_s: 11.050325632095337\n",
      "  time_total_s: 768.6620478630066\n",
      "  timers:\n",
      "    learn_throughput: 1867.811\n",
      "    learn_time_ms: 17.132\n",
      "  timestamp: 1628740763\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 78000\n",
      "  training_iteration: 78\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         768.662</td><td style=\"text-align: right;\">78000</td><td style=\"text-align: right;\">  213.12</td><td style=\"text-align: right;\">                 248</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 158000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-59-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 248.0\n",
      "  episode_reward_mean: 213.29\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 790\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 78720\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.0439919233322144\n",
      "        max_q: 204.1337890625\n",
      "        mean_q: 42.07063674926758\n",
      "        mean_td_error: -5.166178226470947\n",
      "        min_q: 3.485761880874634\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.6208633184432983\n",
      "        max_q: 97.11772155761719\n",
      "        mean_q: 36.69786071777344\n",
      "        mean_td_error: -4.658860206604004\n",
      "        min_q: 4.017719268798828\n",
      "    num_agent_steps_sampled: 158000\n",
      "    num_agent_steps_trained: 1232128\n",
      "    num_steps_sampled: 79000\n",
      "    num_steps_trained: 616064\n",
      "    num_target_updates: 154\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 31.4375\n",
      "    ram_util_percent: 69.94375\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 139.0\n",
      "    agent-1: 135.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 105.77\n",
      "    agent-1: 107.52\n",
      "  policy_reward_min:\n",
      "    agent-0: 92.0\n",
      "    agent-1: 95.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07545876873083669\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.058326933860664294\n",
      "    mean_inference_ms: 3.120132538918595\n",
      "    mean_raw_obs_processing_ms: 0.23438589443622523\n",
      "  time_since_restore: 287.93793749809265\n",
      "  time_this_iter_s: 10.915311098098755\n",
      "  time_total_s: 779.5773589611053\n",
      "  timers:\n",
      "    learn_throughput: 1252.302\n",
      "    learn_time_ms: 25.553\n",
      "  timestamp: 1628740774\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79000\n",
      "  training_iteration: 79\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         779.577</td><td style=\"text-align: right;\">79000</td><td style=\"text-align: right;\">  213.29</td><td style=\"text-align: right;\">                 248</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 160000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-59-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 248.0\n",
      "  episode_reward_mean: 213.35\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 800\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 79728\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.552159309387207\n",
      "        max_q: 142.56021118164062\n",
      "        mean_q: 45.5892448425293\n",
      "        mean_td_error: -4.193195343017578\n",
      "        min_q: 8.949309349060059\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.3330386877059937\n",
      "        max_q: 117.74811553955078\n",
      "        mean_q: 30.902795791625977\n",
      "        mean_td_error: 0.17692340910434723\n",
      "        min_q: 0.7392439246177673\n",
      "    num_agent_steps_sampled: 160000\n",
      "    num_agent_steps_trained: 1248128\n",
      "    num_steps_sampled: 80000\n",
      "    num_steps_trained: 624064\n",
      "    num_target_updates: 156\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.84\n",
      "    ram_util_percent: 70.48666666666666\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 139.0\n",
      "    agent-1: 135.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 105.35\n",
      "    agent-1: 108.0\n",
      "  policy_reward_min:\n",
      "    agent-0: 92.0\n",
      "    agent-1: 100.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07534095449468461\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.058239840527404496\n",
      "    mean_inference_ms: 3.1143061987484226\n",
      "    mean_raw_obs_processing_ms: 0.23401046594464706\n",
      "  time_since_restore: 298.2062735557556\n",
      "  time_this_iter_s: 10.268336057662964\n",
      "  time_total_s: 789.8456950187683\n",
      "  timers:\n",
      "    learn_throughput: 1794.909\n",
      "    learn_time_ms: 17.828\n",
      "  timestamp: 1628740784\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 80000\n",
      "  training_iteration: 80\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         789.846</td><td style=\"text-align: right;\">80000</td><td style=\"text-align: right;\">  213.35</td><td style=\"text-align: right;\">                 248</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 162000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_15-59-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 248.0\n",
      "  episode_reward_mean: 214.08\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 810\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 80736\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 5.175194263458252\n",
      "        max_q: 141.9071807861328\n",
      "        mean_q: 44.25448226928711\n",
      "        mean_td_error: -4.019525051116943\n",
      "        min_q: 9.277872085571289\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.347475051879883\n",
      "        max_q: 146.29161071777344\n",
      "        mean_q: 36.538700103759766\n",
      "        mean_td_error: -1.0231654644012451\n",
      "        min_q: 1.341647744178772\n",
      "    num_agent_steps_sampled: 162000\n",
      "    num_agent_steps_trained: 1264128\n",
      "    num_steps_sampled: 81000\n",
      "    num_steps_trained: 632064\n",
      "    num_target_updates: 158\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.428571428571434\n",
      "    ram_util_percent: 70.38571428571427\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 139.0\n",
      "    agent-1: 135.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 105.39\n",
      "    agent-1: 108.69\n",
      "  policy_reward_min:\n",
      "    agent-0: 92.0\n",
      "    agent-1: 100.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07525936883325561\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05818484285290231\n",
      "    mean_inference_ms: 3.1100669291223464\n",
      "    mean_raw_obs_processing_ms: 0.23372948457572548\n",
      "  time_since_restore: 308.2836790084839\n",
      "  time_this_iter_s: 10.077405452728271\n",
      "  time_total_s: 799.9231004714966\n",
      "  timers:\n",
      "    learn_throughput: 1635.816\n",
      "    learn_time_ms: 19.562\n",
      "  timestamp: 1628740794\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 81000\n",
      "  training_iteration: 81\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.8/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         799.923</td><td style=\"text-align: right;\">81000</td><td style=\"text-align: right;\">  214.08</td><td style=\"text-align: right;\">                 248</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 164000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-00-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 248.0\n",
      "  episode_reward_mean: 213.81\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 820\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 81744\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.412390947341919\n",
      "        max_q: 204.75808715820312\n",
      "        mean_q: 45.22180938720703\n",
      "        mean_td_error: -10.698636054992676\n",
      "        min_q: 8.428478240966797\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.9710614681243896\n",
      "        max_q: 84.77110290527344\n",
      "        mean_q: 29.96697425842285\n",
      "        mean_td_error: 0.035456493496894836\n",
      "        min_q: 1.0662217140197754\n",
      "    num_agent_steps_sampled: 164000\n",
      "    num_agent_steps_trained: 1280128\n",
      "    num_steps_sampled: 82000\n",
      "    num_steps_trained: 640064\n",
      "    num_target_updates: 160\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 25.866666666666664\n",
      "    ram_util_percent: 70.47333333333334\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 139.0\n",
      "    agent-1: 135.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 104.73\n",
      "    agent-1: 109.08\n",
      "  policy_reward_min:\n",
      "    agent-0: 92.0\n",
      "    agent-1: 100.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07520774044525\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.058157203118599635\n",
      "    mean_inference_ms: 3.107237072102886\n",
      "    mean_raw_obs_processing_ms: 0.23353823405806395\n",
      "  time_since_restore: 318.4457297325134\n",
      "  time_this_iter_s: 10.162050724029541\n",
      "  time_total_s: 810.0851511955261\n",
      "  timers:\n",
      "    learn_throughput: 1983.565\n",
      "    learn_time_ms: 16.133\n",
      "  timestamp: 1628740805\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 82000\n",
      "  training_iteration: 82\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         810.085</td><td style=\"text-align: right;\">82000</td><td style=\"text-align: right;\">  213.81</td><td style=\"text-align: right;\">                 248</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 166000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-00-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 232.0\n",
      "  episode_reward_mean: 213.48\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 830\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 82752\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.786606550216675\n",
      "        max_q: 171.6653289794922\n",
      "        mean_q: 49.413490295410156\n",
      "        mean_td_error: 1.2985925674438477\n",
      "        min_q: 1.192701816558838\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.452242851257324\n",
      "        max_q: 133.05038452148438\n",
      "        mean_q: 36.39540481567383\n",
      "        mean_td_error: -1.4340004920959473\n",
      "        min_q: 11.738567352294922\n",
      "    num_agent_steps_sampled: 166000\n",
      "    num_agent_steps_trained: 1296128\n",
      "    num_steps_sampled: 83000\n",
      "    num_steps_trained: 648064\n",
      "    num_target_updates: 162\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.0\n",
      "    ram_util_percent: 70.61538461538463\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 131.0\n",
      "    agent-1: 135.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 104.34\n",
      "    agent-1: 109.14\n",
      "  policy_reward_min:\n",
      "    agent-0: 92.0\n",
      "    agent-1: 100.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07517154154955484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.058142398644794306\n",
      "    mean_inference_ms: 3.105086535465689\n",
      "    mean_raw_obs_processing_ms: 0.23341231543785082\n",
      "  time_since_restore: 328.0378360748291\n",
      "  time_this_iter_s: 9.592106342315674\n",
      "  time_total_s: 819.6772575378418\n",
      "  timers:\n",
      "    learn_throughput: 1978.886\n",
      "    learn_time_ms: 16.171\n",
      "  timestamp: 1628740814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 83000\n",
      "  training_iteration: 83\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         819.677</td><td style=\"text-align: right;\">83000</td><td style=\"text-align: right;\">  213.48</td><td style=\"text-align: right;\">                 232</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 168000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-00-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 232.0\n",
      "  episode_reward_mean: 213.92\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 840\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 83760\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.8537867069244385\n",
      "        max_q: 209.8455047607422\n",
      "        mean_q: 38.99123001098633\n",
      "        mean_td_error: -3.1146881580352783\n",
      "        min_q: 9.360114097595215\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.057645320892334\n",
      "        max_q: 137.5633544921875\n",
      "        mean_q: 44.743221282958984\n",
      "        mean_td_error: 2.0162980556488037\n",
      "        min_q: 13.888172149658203\n",
      "    num_agent_steps_sampled: 168000\n",
      "    num_agent_steps_trained: 1312128\n",
      "    num_steps_sampled: 84000\n",
      "    num_steps_trained: 656064\n",
      "    num_target_updates: 164\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.83571428571429\n",
      "    ram_util_percent: 70.65714285714287\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 131.0\n",
      "    agent-1: 135.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 104.36\n",
      "    agent-1: 109.56\n",
      "  policy_reward_min:\n",
      "    agent-0: 92.0\n",
      "    agent-1: 100.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07514996539781015\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05813671536628069\n",
      "    mean_inference_ms: 3.1036088570259026\n",
      "    mean_raw_obs_processing_ms: 0.23332350987093864\n",
      "  time_since_restore: 337.5292136669159\n",
      "  time_this_iter_s: 9.491377592086792\n",
      "  time_total_s: 829.1686351299286\n",
      "  timers:\n",
      "    learn_throughput: 1857.896\n",
      "    learn_time_ms: 17.224\n",
      "  timestamp: 1628740824\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 84000\n",
      "  training_iteration: 84\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         829.169</td><td style=\"text-align: right;\">84000</td><td style=\"text-align: right;\">  213.92</td><td style=\"text-align: right;\">                 232</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 170000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-00-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 232.0\n",
      "  episode_reward_mean: 213.7\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 850\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 84768\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.3324337005615234\n",
      "        max_q: 145.7215576171875\n",
      "        mean_q: 37.716453552246094\n",
      "        mean_td_error: -0.1345948427915573\n",
      "        min_q: 7.3395161628723145\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.0928488969802856\n",
      "        max_q: 90.02259063720703\n",
      "        mean_q: 29.248985290527344\n",
      "        mean_td_error: 0.06013783812522888\n",
      "        min_q: 1.113919973373413\n",
      "    num_agent_steps_sampled: 170000\n",
      "    num_agent_steps_trained: 1328128\n",
      "    num_steps_sampled: 85000\n",
      "    num_steps_trained: 664064\n",
      "    num_target_updates: 166\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.007142857142863\n",
      "    ram_util_percent: 70.78571428571426\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 131.0\n",
      "    agent-1: 135.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 105.2\n",
      "    agent-1: 108.5\n",
      "  policy_reward_min:\n",
      "    agent-0: 92.0\n",
      "    agent-1: 98.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07513449803988934\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05813577064708879\n",
      "    mean_inference_ms: 3.1025350995732963\n",
      "    mean_raw_obs_processing_ms: 0.23325713039183604\n",
      "  time_since_restore: 347.1160945892334\n",
      "  time_this_iter_s: 9.586880922317505\n",
      "  time_total_s: 838.7555160522461\n",
      "  timers:\n",
      "    learn_throughput: 1975.12\n",
      "    learn_time_ms: 16.202\n",
      "  timestamp: 1628740833\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 85000\n",
      "  training_iteration: 85\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         838.756</td><td style=\"text-align: right;\">85000</td><td style=\"text-align: right;\">   213.7</td><td style=\"text-align: right;\">                 232</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 172000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-00-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 232.0\n",
      "  episode_reward_mean: 213.61\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 860\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 85776\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.6863791942596436\n",
      "        max_q: 116.83362579345703\n",
      "        mean_q: 30.74388313293457\n",
      "        mean_td_error: -1.6691075563430786\n",
      "        min_q: 1.0606253147125244\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.0436632633209229\n",
      "        max_q: 89.01708221435547\n",
      "        mean_q: 28.856481552124023\n",
      "        mean_td_error: -1.2354679107666016\n",
      "        min_q: 2.1983203887939453\n",
      "    num_agent_steps_sampled: 172000\n",
      "    num_agent_steps_trained: 1344128\n",
      "    num_steps_sampled: 86000\n",
      "    num_steps_trained: 672064\n",
      "    num_target_updates: 168\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.015384615384615\n",
      "    ram_util_percent: 70.80769230769229\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 131.0\n",
      "    agent-1: 135.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 105.28\n",
      "    agent-1: 108.33\n",
      "  policy_reward_min:\n",
      "    agent-0: 95.0\n",
      "    agent-1: 98.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0751271781728997\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05814005658935804\n",
      "    mean_inference_ms: 3.1018326895000055\n",
      "    mean_raw_obs_processing_ms: 0.23322347221778458\n",
      "  time_since_restore: 356.5130412578583\n",
      "  time_this_iter_s: 9.396946668624878\n",
      "  time_total_s: 848.152462720871\n",
      "  timers:\n",
      "    learn_throughput: 1829.369\n",
      "    learn_time_ms: 17.492\n",
      "  timestamp: 1628740843\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 86000\n",
      "  training_iteration: 86\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         848.152</td><td style=\"text-align: right;\">86000</td><td style=\"text-align: right;\">  213.61</td><td style=\"text-align: right;\">                 232</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 174000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-00-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 232.0\n",
      "  episode_reward_mean: 213.31\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 870\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 86784\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.4065440893173218\n",
      "        max_q: 148.30270385742188\n",
      "        mean_q: 36.484832763671875\n",
      "        mean_td_error: -4.186033248901367\n",
      "        min_q: 0.4689549207687378\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.4063680171966553\n",
      "        max_q: 129.90335083007812\n",
      "        mean_q: 35.41945266723633\n",
      "        mean_td_error: -3.4428293704986572\n",
      "        min_q: 2.9004392623901367\n",
      "    num_agent_steps_sampled: 174000\n",
      "    num_agent_steps_trained: 1360128\n",
      "    num_steps_sampled: 87000\n",
      "    num_steps_trained: 680064\n",
      "    num_target_updates: 170\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.546153846153842\n",
      "    ram_util_percent: 70.94615384615385\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 131.0\n",
      "    agent-1: 135.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 105.03\n",
      "    agent-1: 108.28\n",
      "  policy_reward_min:\n",
      "    agent-0: 95.0\n",
      "    agent-1: 98.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07510229061539916\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05812292123680141\n",
      "    mean_inference_ms: 3.1003084784709665\n",
      "    mean_raw_obs_processing_ms: 0.23313972243252906\n",
      "  time_since_restore: 365.907399892807\n",
      "  time_this_iter_s: 9.39435863494873\n",
      "  time_total_s: 857.5468213558197\n",
      "  timers:\n",
      "    learn_throughput: 1964.217\n",
      "    learn_time_ms: 16.291\n",
      "  timestamp: 1628740852\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87000\n",
      "  training_iteration: 87\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         857.547</td><td style=\"text-align: right;\">87000</td><td style=\"text-align: right;\">  213.31</td><td style=\"text-align: right;\">                 232</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 176000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-01-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 232.0\n",
      "  episode_reward_mean: 213.51\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 880\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 87792\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 5.054337024688721\n",
      "        max_q: 219.22036743164062\n",
      "        mean_q: 33.82368087768555\n",
      "        mean_td_error: -4.489906311035156\n",
      "        min_q: 9.47700309753418\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.389730930328369\n",
      "        max_q: 57.09405517578125\n",
      "        mean_q: 26.41089630126953\n",
      "        mean_td_error: 1.183753490447998\n",
      "        min_q: 1.4275232553482056\n",
      "    num_agent_steps_sampled: 176000\n",
      "    num_agent_steps_trained: 1376128\n",
      "    num_steps_sampled: 88000\n",
      "    num_steps_trained: 688064\n",
      "    num_target_updates: 172\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.635714285714283\n",
      "    ram_util_percent: 71.02142857142857\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 119.0\n",
      "    agent-1: 135.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 105.23\n",
      "    agent-1: 108.28\n",
      "  policy_reward_min:\n",
      "    agent-0: 95.0\n",
      "    agent-1: 98.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07503763431624608\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05807350912984186\n",
      "    mean_inference_ms: 3.0970010156900063\n",
      "    mean_raw_obs_processing_ms: 0.23295093783970305\n",
      "  time_since_restore: 375.2148790359497\n",
      "  time_this_iter_s: 9.3074791431427\n",
      "  time_total_s: 866.8543004989624\n",
      "  timers:\n",
      "    learn_throughput: 1990.927\n",
      "    learn_time_ms: 16.073\n",
      "  timestamp: 1628740862\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 88000\n",
      "  training_iteration: 88\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 10.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">         866.854</td><td style=\"text-align: right;\">88000</td><td style=\"text-align: right;\">  213.51</td><td style=\"text-align: right;\">                 232</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 178000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-01-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 232.0\n",
      "  episode_reward_mean: 214.3\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 890\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 88800\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.8190783858299255\n",
      "        max_q: 130.22772216796875\n",
      "        mean_q: 34.881080627441406\n",
      "        mean_td_error: -5.634006500244141\n",
      "        min_q: 1.5977555513381958\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 4.764205455780029\n",
      "        max_q: 128.9954833984375\n",
      "        mean_q: 38.25137710571289\n",
      "        mean_td_error: 1.8349202871322632\n",
      "        min_q: 8.681471824645996\n",
      "    num_agent_steps_sampled: 178000\n",
      "    num_agent_steps_trained: 1392128\n",
      "    num_steps_sampled: 89000\n",
      "    num_steps_trained: 696064\n",
      "    num_target_updates: 174\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.642857142857146\n",
      "    ram_util_percent: 71.36428571428571\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 126.0\n",
      "    agent-1: 135.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 106.05\n",
      "    agent-1: 108.25\n",
      "  policy_reward_min:\n",
      "    agent-0: 95.0\n",
      "    agent-1: 98.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07495439190891583\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.058010423766445725\n",
      "    mean_inference_ms: 3.0926699296042757\n",
      "    mean_raw_obs_processing_ms: 0.23269610827361645\n",
      "  time_since_restore: 385.0868957042694\n",
      "  time_this_iter_s: 9.872016668319702\n",
      "  time_total_s: 876.7263171672821\n",
      "  timers:\n",
      "    learn_throughput: 1988.894\n",
      "    learn_time_ms: 16.089\n",
      "  timestamp: 1628740871\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 89000\n",
      "  training_iteration: 89\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         876.726</td><td style=\"text-align: right;\">89000</td><td style=\"text-align: right;\">   214.3</td><td style=\"text-align: right;\">                 232</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 180000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-01-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 232.0\n",
      "  episode_reward_mean: 215.09\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 900\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 89808\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.4898500442504883\n",
      "        max_q: 143.78036499023438\n",
      "        mean_q: 33.98148727416992\n",
      "        mean_td_error: -0.5751743316650391\n",
      "        min_q: 1.7691075801849365\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.797267436981201\n",
      "        max_q: 86.18450927734375\n",
      "        mean_q: 27.34015464782715\n",
      "        mean_td_error: -0.91390460729599\n",
      "        min_q: 6.243974208831787\n",
      "    num_agent_steps_sampled: 180000\n",
      "    num_agent_steps_trained: 1408128\n",
      "    num_steps_sampled: 90000\n",
      "    num_steps_trained: 704064\n",
      "    num_target_updates: 176\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 24.450000000000006\n",
      "    ram_util_percent: 71.60000000000001\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 126.0\n",
      "    agent-1: 135.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 106.67\n",
      "    agent-1: 108.42\n",
      "  policy_reward_min:\n",
      "    agent-0: 95.0\n",
      "    agent-1: 98.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0748597971213872\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05793826888104823\n",
      "    mean_inference_ms: 3.0877910062395393\n",
      "    mean_raw_obs_processing_ms: 0.23241373714130423\n",
      "  time_since_restore: 394.8867127895355\n",
      "  time_this_iter_s: 9.799817085266113\n",
      "  time_total_s: 886.5261342525482\n",
      "  timers:\n",
      "    learn_throughput: 1730.584\n",
      "    learn_time_ms: 18.491\n",
      "  timestamp: 1628740881\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 90000\n",
      "  training_iteration: 90\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         886.526</td><td style=\"text-align: right;\">90000</td><td style=\"text-align: right;\">  215.09</td><td style=\"text-align: right;\">                 232</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 182000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-01-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 232.0\n",
      "  episode_reward_mean: 215.17\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 910\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 90816\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.8989654183387756\n",
      "        max_q: 118.7923583984375\n",
      "        mean_q: 32.70889663696289\n",
      "        mean_td_error: -1.4976218938827515\n",
      "        min_q: 4.399672508239746\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.955301284790039\n",
      "        max_q: 139.76101684570312\n",
      "        mean_q: 41.18635940551758\n",
      "        mean_td_error: -0.3852348327636719\n",
      "        min_q: 6.03568696975708\n",
      "    num_agent_steps_sampled: 182000\n",
      "    num_agent_steps_trained: 1424128\n",
      "    num_steps_sampled: 91000\n",
      "    num_steps_trained: 712064\n",
      "    num_target_updates: 178\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.192307692307686\n",
      "    ram_util_percent: 71.4076923076923\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 126.0\n",
      "    agent-1: 125.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 107.61\n",
      "    agent-1: 107.56\n",
      "  policy_reward_min:\n",
      "    agent-0: 96.0\n",
      "    agent-1: 98.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07475601407631056\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05785609121710488\n",
      "    mean_inference_ms: 3.082456894092872\n",
      "    mean_raw_obs_processing_ms: 0.2321132432901883\n",
      "  time_since_restore: 404.34320759773254\n",
      "  time_this_iter_s: 9.456494808197021\n",
      "  time_total_s: 895.9826290607452\n",
      "  timers:\n",
      "    learn_throughput: 1942.392\n",
      "    learn_time_ms: 16.475\n",
      "  timestamp: 1628740891\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 91000\n",
      "  training_iteration: 91\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         895.983</td><td style=\"text-align: right;\">91000</td><td style=\"text-align: right;\">  215.17</td><td style=\"text-align: right;\">                 232</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 184000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-01-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 232.0\n",
      "  episode_reward_mean: 215.47\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 920\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 91824\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.411404848098755\n",
      "        max_q: 204.5537567138672\n",
      "        mean_q: 38.97987365722656\n",
      "        mean_td_error: -0.9471063017845154\n",
      "        min_q: 1.9032855033874512\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 3.346809148788452\n",
      "        max_q: 80.70204162597656\n",
      "        mean_q: 28.231142044067383\n",
      "        mean_td_error: 0.1400282382965088\n",
      "        min_q: 1.5578160285949707\n",
      "    num_agent_steps_sampled: 184000\n",
      "    num_agent_steps_trained: 1440128\n",
      "    num_steps_sampled: 92000\n",
      "    num_steps_trained: 720064\n",
      "    num_target_updates: 180\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 23.657142857142855\n",
      "    ram_util_percent: 71.44285714285714\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 126.0\n",
      "    agent-1: 122.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 108.31\n",
      "    agent-1: 107.16\n",
      "  policy_reward_min:\n",
      "    agent-0: 96.0\n",
      "    agent-1: 98.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07464266741713858\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05776544059294115\n",
      "    mean_inference_ms: 3.076736686568873\n",
      "    mean_raw_obs_processing_ms: 0.2317858309418103\n",
      "  time_since_restore: 413.74605321884155\n",
      "  time_this_iter_s: 9.402845621109009\n",
      "  time_total_s: 905.3854746818542\n",
      "  timers:\n",
      "    learn_throughput: 1951.738\n",
      "    learn_time_ms: 16.396\n",
      "  timestamp: 1628740900\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 92000\n",
      "  training_iteration: 92\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         905.385</td><td style=\"text-align: right;\">92000</td><td style=\"text-align: right;\">  215.47</td><td style=\"text-align: right;\">                 232</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 186000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-01-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 232.0\n",
      "  episode_reward_mean: 215.35\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 930\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 92832\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.0652692317962646\n",
      "        max_q: 119.22238159179688\n",
      "        mean_q: 32.23122024536133\n",
      "        mean_td_error: -0.6330007314682007\n",
      "        min_q: 1.825600504875183\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.6752800941467285\n",
      "        max_q: 138.2056427001953\n",
      "        mean_q: 39.303443908691406\n",
      "        mean_td_error: 0.5945680737495422\n",
      "        min_q: 9.642833709716797\n",
      "    num_agent_steps_sampled: 186000\n",
      "    num_agent_steps_trained: 1456128\n",
      "    num_steps_sampled: 93000\n",
      "    num_steps_trained: 728064\n",
      "    num_target_updates: 182\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.215384615384615\n",
      "    ram_util_percent: 71.47692307692309\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 126.0\n",
      "    agent-1: 122.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 108.75\n",
      "    agent-1: 106.6\n",
      "  policy_reward_min:\n",
      "    agent-0: 96.0\n",
      "    agent-1: 96.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07453027605946787\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.057675514330254904\n",
      "    mean_inference_ms: 3.0711841583169246\n",
      "    mean_raw_obs_processing_ms: 0.23145508513800778\n",
      "  time_since_restore: 423.11123037338257\n",
      "  time_this_iter_s: 9.365177154541016\n",
      "  time_total_s: 914.7506518363953\n",
      "  timers:\n",
      "    learn_throughput: 1965.538\n",
      "    learn_time_ms: 16.281\n",
      "  timestamp: 1628740910\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 93000\n",
      "  training_iteration: 93\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         914.751</td><td style=\"text-align: right;\">93000</td><td style=\"text-align: right;\">  215.35</td><td style=\"text-align: right;\">                 232</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 188000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-01-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 238.0\n",
      "  episode_reward_mean: 215.54\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 940\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 93840\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.8204607367515564\n",
      "        max_q: 214.59991455078125\n",
      "        mean_q: 41.46837615966797\n",
      "        mean_td_error: -2.4327759742736816\n",
      "        min_q: 1.4725393056869507\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.4209084510803223\n",
      "        max_q: 56.94523620605469\n",
      "        mean_q: 26.446277618408203\n",
      "        mean_td_error: -1.1903005838394165\n",
      "        min_q: 1.0724742412567139\n",
      "    num_agent_steps_sampled: 188000\n",
      "    num_agent_steps_trained: 1472128\n",
      "    num_steps_sampled: 94000\n",
      "    num_steps_trained: 736064\n",
      "    num_target_updates: 184\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.74285714285714\n",
      "    ram_util_percent: 71.62142857142858\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 126.0\n",
      "    agent-1: 121.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 109.27\n",
      "    agent-1: 106.27\n",
      "  policy_reward_min:\n",
      "    agent-0: 96.0\n",
      "    agent-1: 96.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0744201783950711\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.057588826105484665\n",
      "    mean_inference_ms: 3.065834933274118\n",
      "    mean_raw_obs_processing_ms: 0.23113568176125934\n",
      "  time_since_restore: 432.48422384262085\n",
      "  time_this_iter_s: 9.372993469238281\n",
      "  time_total_s: 924.1236453056335\n",
      "  timers:\n",
      "    learn_throughput: 1856.624\n",
      "    learn_time_ms: 17.236\n",
      "  timestamp: 1628740919\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 94000\n",
      "  training_iteration: 94\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         924.124</td><td style=\"text-align: right;\">94000</td><td style=\"text-align: right;\">  215.54</td><td style=\"text-align: right;\">                 238</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 190000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-02-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 238.0\n",
      "  episode_reward_mean: 215.67\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 950\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 94848\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.3438595533370972\n",
      "        max_q: 129.94366455078125\n",
      "        mean_q: 32.15988540649414\n",
      "        mean_td_error: -1.7636768817901611\n",
      "        min_q: 6.398571014404297\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.1469272375106812\n",
      "        max_q: 130.1552734375\n",
      "        mean_q: 34.43882751464844\n",
      "        mean_td_error: 0.21497873961925507\n",
      "        min_q: 2.2054781913757324\n",
      "    num_agent_steps_sampled: 190000\n",
      "    num_agent_steps_trained: 1488128\n",
      "    num_steps_sampled: 95000\n",
      "    num_steps_trained: 744064\n",
      "    num_target_updates: 186\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.93076923076923\n",
      "    ram_util_percent: 71.66923076923078\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 126.0\n",
      "    agent-1: 119.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 109.56\n",
      "    agent-1: 106.11\n",
      "  policy_reward_min:\n",
      "    agent-0: 96.0\n",
      "    agent-1: 96.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07431232216103516\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05750391619585697\n",
      "    mean_inference_ms: 3.060540961425099\n",
      "    mean_raw_obs_processing_ms: 0.23082577325765696\n",
      "  time_since_restore: 441.89355874061584\n",
      "  time_this_iter_s: 9.409334897994995\n",
      "  time_total_s: 933.5329802036285\n",
      "  timers:\n",
      "    learn_throughput: 1975.368\n",
      "    learn_time_ms: 16.2\n",
      "  timestamp: 1628740928\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95000\n",
      "  training_iteration: 95\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         933.533</td><td style=\"text-align: right;\">95000</td><td style=\"text-align: right;\">  215.67</td><td style=\"text-align: right;\">                 238</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 192000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-02-18\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 238.0\n",
      "  episode_reward_mean: 215.36\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 960\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 95856\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.721657156944275\n",
      "        max_q: 150.03616333007812\n",
      "        mean_q: 38.84830856323242\n",
      "        mean_td_error: 1.3133370876312256\n",
      "        min_q: 3.9927053451538086\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.149637222290039\n",
      "        max_q: 131.48330688476562\n",
      "        mean_q: 34.634193420410156\n",
      "        mean_td_error: -1.8569059371948242\n",
      "        min_q: 2.1472744941711426\n",
      "    num_agent_steps_sampled: 192000\n",
      "    num_agent_steps_trained: 1504128\n",
      "    num_steps_sampled: 96000\n",
      "    num_steps_trained: 752064\n",
      "    num_target_updates: 188\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.178571428571427\n",
      "    ram_util_percent: 71.83571428571426\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 126.0\n",
      "    agent-1: 119.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 109.73\n",
      "    agent-1: 105.63\n",
      "  policy_reward_min:\n",
      "    agent-0: 96.0\n",
      "    agent-1: 96.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07421094735379535\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05742427009673214\n",
      "    mean_inference_ms: 3.055565197647361\n",
      "    mean_raw_obs_processing_ms: 0.23052937498102932\n",
      "  time_since_restore: 451.3148455619812\n",
      "  time_this_iter_s: 9.421286821365356\n",
      "  time_total_s: 942.9542670249939\n",
      "  timers:\n",
      "    learn_throughput: 1797.358\n",
      "    learn_time_ms: 17.804\n",
      "  timestamp: 1628740938\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 96000\n",
      "  training_iteration: 96\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         942.954</td><td style=\"text-align: right;\">96000</td><td style=\"text-align: right;\">  215.36</td><td style=\"text-align: right;\">                 238</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 194000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-02-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 238.0\n",
      "  episode_reward_mean: 215.53\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 970\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 96864\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 2.302494764328003\n",
      "        max_q: 120.34375762939453\n",
      "        mean_q: 31.657394409179688\n",
      "        mean_td_error: -5.5752177238464355\n",
      "        min_q: 1.5178375244140625\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.3471643924713135\n",
      "        max_q: 139.83425903320312\n",
      "        mean_q: 35.24257278442383\n",
      "        mean_td_error: -3.9618189334869385\n",
      "        min_q: 5.859855651855469\n",
      "    num_agent_steps_sampled: 194000\n",
      "    num_agent_steps_trained: 1520128\n",
      "    num_steps_sampled: 97000\n",
      "    num_steps_trained: 760064\n",
      "    num_target_updates: 190\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.892307692307693\n",
      "    ram_util_percent: 71.85384615384613\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 126.0\n",
      "    agent-1: 119.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 110.29\n",
      "    agent-1: 105.24\n",
      "  policy_reward_min:\n",
      "    agent-0: 99.0\n",
      "    agent-1: 96.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07411208322843654\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05734821017420833\n",
      "    mean_inference_ms: 3.0508029709845825\n",
      "    mean_raw_obs_processing_ms: 0.23024737392154332\n",
      "  time_since_restore: 460.72303771972656\n",
      "  time_this_iter_s: 9.408192157745361\n",
      "  time_total_s: 952.3624591827393\n",
      "  timers:\n",
      "    learn_throughput: 1969.319\n",
      "    learn_time_ms: 16.249\n",
      "  timestamp: 1628740947\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 97000\n",
      "  training_iteration: 97\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         952.362</td><td style=\"text-align: right;\">97000</td><td style=\"text-align: right;\">  215.53</td><td style=\"text-align: right;\">                 238</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 196000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-02-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 238.0\n",
      "  episode_reward_mean: 215.12\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 980\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 97872\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.9039307832717896\n",
      "        max_q: 71.30823516845703\n",
      "        mean_q: 33.387733459472656\n",
      "        mean_td_error: -0.08983629941940308\n",
      "        min_q: 12.614275932312012\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.3067407608032227\n",
      "        max_q: 91.1012954711914\n",
      "        mean_q: 32.14789581298828\n",
      "        mean_td_error: -4.350718021392822\n",
      "        min_q: 0.8777596354484558\n",
      "    num_agent_steps_sampled: 196000\n",
      "    num_agent_steps_trained: 1536128\n",
      "    num_steps_sampled: 98000\n",
      "    num_steps_trained: 768064\n",
      "    num_target_updates: 192\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.084615384615383\n",
      "    ram_util_percent: 71.96153846153847\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 126.0\n",
      "    agent-1: 119.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 110.16\n",
      "    agent-1: 104.96\n",
      "  policy_reward_min:\n",
      "    agent-0: 99.0\n",
      "    agent-1: 96.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07401898996431727\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.057274884128744825\n",
      "    mean_inference_ms: 3.0462673452529954\n",
      "    mean_raw_obs_processing_ms: 0.22997993915884457\n",
      "  time_since_restore: 470.0599548816681\n",
      "  time_this_iter_s: 9.336917161941528\n",
      "  time_total_s: 961.6993763446808\n",
      "  timers:\n",
      "    learn_throughput: 1981.062\n",
      "    learn_time_ms: 16.153\n",
      "  timestamp: 1628740957\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 98000\n",
      "  training_iteration: 98\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         961.699</td><td style=\"text-align: right;\">98000</td><td style=\"text-align: right;\">  215.12</td><td style=\"text-align: right;\">                 238</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 198000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-02-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 238.0\n",
      "  episode_reward_mean: 215.14\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 990\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 98880\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.8804929852485657\n",
      "        max_q: 198.46212768554688\n",
      "        mean_q: 39.62709426879883\n",
      "        mean_td_error: 0.08575981855392456\n",
      "        min_q: 1.2375596761703491\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.221503734588623\n",
      "        max_q: 114.0846176147461\n",
      "        mean_q: 35.968955993652344\n",
      "        mean_td_error: -4.533133029937744\n",
      "        min_q: 11.219254493713379\n",
      "    num_agent_steps_sampled: 198000\n",
      "    num_agent_steps_trained: 1552128\n",
      "    num_steps_sampled: 99000\n",
      "    num_steps_trained: 776064\n",
      "    num_target_updates: 194\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.728571428571428\n",
      "    ram_util_percent: 72.02857142857142\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 122.0\n",
      "    agent-1: 119.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 110.27\n",
      "    agent-1: 104.87\n",
      "  policy_reward_min:\n",
      "    agent-0: 99.0\n",
      "    agent-1: 96.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07391934070002222\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.05719284771585123\n",
      "    mean_inference_ms: 3.0414878980158875\n",
      "    mean_raw_obs_processing_ms: 0.22970041992287193\n",
      "  time_since_restore: 479.4684114456177\n",
      "  time_this_iter_s: 9.408456563949585\n",
      "  time_total_s: 971.1078329086304\n",
      "  timers:\n",
      "    learn_throughput: 1940.996\n",
      "    learn_time_ms: 16.486\n",
      "  timestamp: 1628740966\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 99000\n",
      "  training_iteration: 99\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         971.108</td><td style=\"text-align: right;\">99000</td><td style=\"text-align: right;\">  215.14</td><td style=\"text-align: right;\">                 238</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_MG_env_06046_00000:\n",
      "  agent_timesteps_total: 200000\n",
      "  custom_metrics:\n",
      "    agent-0: {}\n",
      "    agent-1: {}\n",
      "  date: 2021-08-12_16-02-55\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 238.0\n",
      "  episode_reward_mean: 214.89\n",
      "  episode_reward_min: 203.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 1000\n",
      "  experiment_id: fe9d180d9dcf4544b62ec7ccc2e5caca\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 99888\n",
      "    learner:\n",
      "      agent-0:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 1.8093993663787842\n",
      "        max_q: 149.7140655517578\n",
      "        mean_q: 33.36493682861328\n",
      "        mean_td_error: 2.048804759979248\n",
      "        min_q: 0.9183366298675537\n",
      "      agent-1:\n",
      "        allreduce_latency: 0.0\n",
      "        cur_lr: 0.0005\n",
      "        grad_gnorm: 0.7457124590873718\n",
      "        max_q: 127.22663879394531\n",
      "        mean_q: 35.550045013427734\n",
      "        mean_td_error: 2.2260630130767822\n",
      "        min_q: 0.9635365009307861\n",
      "    num_agent_steps_sampled: 200000\n",
      "    num_agent_steps_trained: 1568128\n",
      "    num_steps_sampled: 100000\n",
      "    num_steps_trained: 784064\n",
      "    num_target_updates: 196\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.9\n",
      "    ram_util_percent: 72.0923076923077\n",
      "  pid: 127629\n",
      "  policy_reward_max:\n",
      "    agent-0: 121.0\n",
      "    agent-1: 119.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 110.42\n",
      "    agent-1: 104.47\n",
      "  policy_reward_min:\n",
      "    agent-0: 99.0\n",
      "    agent-1: 96.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.07381911641842744\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.057110162523905404\n",
      "    mean_inference_ms: 3.0367384088209275\n",
      "    mean_raw_obs_processing_ms: 0.22941244037933456\n",
      "  time_since_restore: 488.7536587715149\n",
      "  time_this_iter_s: 9.285247325897217\n",
      "  time_total_s: 980.3930802345276\n",
      "  timers:\n",
      "    learn_throughput: 2001.547\n",
      "    learn_time_ms: 15.988\n",
      "  timestamp: 1628740975\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 100000\n",
      "  training_iteration: 100\n",
      "  trial_id: '06046_00000'\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>RUNNING </td><td>192.168.1.21:127629</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         980.393</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  214.89</td><td style=\"text-align: right;\">                 238</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.9 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_2c49e5ba926b7d1f96092d711ff6302f, 0.0/1.0 CPU_group_0_2c49e5ba926b7d1f96092d711ff6302f)<br>Result logdir: /home/peter/ray_results/TA_TEST2<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                      </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_MG_env_06046_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         980.393</td><td style=\"text-align: right;\">100000</td><td style=\"text-align: right;\">  214.89</td><td style=\"text-align: right;\">                 238</td><td style=\"text-align: right;\">                 203</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-12 16:02:56,339\tINFO tune.py:549 -- Total run time: 494.50 seconds (494.15 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# doing 1-v-1 training\n",
    "# with pretrained agents\n",
    "checkpoint = MA_result.get_last_checkpoint()\n",
    "checkpoint = '/home/peter/ray_results/TA_TEST2/DQN_two_agent_t4t_MG_env_30405_00000_0_2021-08-12_15-41-33/checkpoint_000050/checkpoint-50'\n",
    "exp_name = 'TA_TEST2'\n",
    "exp_dict = {\n",
    "        'name': exp_name,\n",
    "        'run_or_experiment': 'DQN',\n",
    "        \"stop\": {\n",
    "            \"training_iteration\": 100\n",
    "        },\n",
    "        'checkpoint_freq': 0,\n",
    "        'checkpoint_at_end': True,\n",
    "        \"config\": config,\n",
    "        \"restore\": checkpoint\n",
    "\n",
    "}\n",
    "# ray.init()\n",
    "MA_result=tune.run(**exp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "095764b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'06046_00000': {'episode_reward_max': 238.0,\n",
       "  'episode_reward_min': 203.0,\n",
       "  'episode_reward_mean': 214.89,\n",
       "  'episode_len_mean': 100.0,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 10,\n",
       "  'policy_reward_min': {'agent-0': 99.0, 'agent-1': 96.0},\n",
       "  'policy_reward_max': {'agent-0': 121.0, 'agent-1': 119.0},\n",
       "  'policy_reward_mean': {'agent-0': 110.42, 'agent-1': 104.47},\n",
       "  'custom_metrics': {'agent-0': {}, 'agent-1': {}},\n",
       "  'hist_stats': {'episode_reward': [216.0,\n",
       "    220.0,\n",
       "    216.0,\n",
       "    215.0,\n",
       "    213.0,\n",
       "    219.0,\n",
       "    214.0,\n",
       "    219.0,\n",
       "    228.0,\n",
       "    207.0,\n",
       "    217.0,\n",
       "    211.0,\n",
       "    220.0,\n",
       "    220.0,\n",
       "    220.0,\n",
       "    220.0,\n",
       "    216.0,\n",
       "    216.0,\n",
       "    229.0,\n",
       "    213.0,\n",
       "    206.0,\n",
       "    212.0,\n",
       "    209.0,\n",
       "    212.0,\n",
       "    222.0,\n",
       "    209.0,\n",
       "    206.0,\n",
       "    210.0,\n",
       "    213.0,\n",
       "    212.0,\n",
       "    217.0,\n",
       "    214.0,\n",
       "    203.0,\n",
       "    225.0,\n",
       "    206.0,\n",
       "    217.0,\n",
       "    215.0,\n",
       "    211.0,\n",
       "    238.0,\n",
       "    220.0,\n",
       "    220.0,\n",
       "    226.0,\n",
       "    213.0,\n",
       "    213.0,\n",
       "    216.0,\n",
       "    214.0,\n",
       "    214.0,\n",
       "    210.0,\n",
       "    216.0,\n",
       "    216.0,\n",
       "    212.0,\n",
       "    210.0,\n",
       "    227.0,\n",
       "    203.0,\n",
       "    209.0,\n",
       "    216.0,\n",
       "    206.0,\n",
       "    206.0,\n",
       "    210.0,\n",
       "    212.0,\n",
       "    218.0,\n",
       "    212.0,\n",
       "    206.0,\n",
       "    203.0,\n",
       "    209.0,\n",
       "    210.0,\n",
       "    214.0,\n",
       "    210.0,\n",
       "    222.0,\n",
       "    215.0,\n",
       "    212.0,\n",
       "    215.0,\n",
       "    219.0,\n",
       "    219.0,\n",
       "    206.0,\n",
       "    207.0,\n",
       "    216.0,\n",
       "    216.0,\n",
       "    216.0,\n",
       "    207.0,\n",
       "    215.0,\n",
       "    222.0,\n",
       "    216.0,\n",
       "    232.0,\n",
       "    213.0,\n",
       "    219.0,\n",
       "    209.0,\n",
       "    223.0,\n",
       "    220.0,\n",
       "    213.0,\n",
       "    208.0,\n",
       "    217.0,\n",
       "    223.0,\n",
       "    211.0,\n",
       "    217.0,\n",
       "    221.0,\n",
       "    217.0,\n",
       "    217.0,\n",
       "    218.0,\n",
       "    211.0],\n",
       "   'episode_lengths': [100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100],\n",
       "   'policy_agent-0_reward': [118.0,\n",
       "    115.0,\n",
       "    118.0,\n",
       "    115.0,\n",
       "    104.0,\n",
       "    117.0,\n",
       "    112.0,\n",
       "    107.0,\n",
       "    119.0,\n",
       "    106.0,\n",
       "    111.0,\n",
       "    108.0,\n",
       "    115.0,\n",
       "    120.0,\n",
       "    110.0,\n",
       "    115.0,\n",
       "    108.0,\n",
       "    118.0,\n",
       "    117.0,\n",
       "    109.0,\n",
       "    103.0,\n",
       "    116.0,\n",
       "    102.0,\n",
       "    111.0,\n",
       "    106.0,\n",
       "    107.0,\n",
       "    108.0,\n",
       "    110.0,\n",
       "    109.0,\n",
       "    106.0,\n",
       "    111.0,\n",
       "    112.0,\n",
       "    104.0,\n",
       "    115.0,\n",
       "    103.0,\n",
       "    111.0,\n",
       "    115.0,\n",
       "    103.0,\n",
       "    119.0,\n",
       "    115.0,\n",
       "    110.0,\n",
       "    113.0,\n",
       "    114.0,\n",
       "    114.0,\n",
       "    108.0,\n",
       "    107.0,\n",
       "    112.0,\n",
       "    105.0,\n",
       "    108.0,\n",
       "    118.0,\n",
       "    106.0,\n",
       "    105.0,\n",
       "    116.0,\n",
       "    104.0,\n",
       "    107.0,\n",
       "    108.0,\n",
       "    103.0,\n",
       "    103.0,\n",
       "    110.0,\n",
       "    111.0,\n",
       "    109.0,\n",
       "    101.0,\n",
       "    103.0,\n",
       "    99.0,\n",
       "    107.0,\n",
       "    110.0,\n",
       "    112.0,\n",
       "    110.0,\n",
       "    111.0,\n",
       "    115.0,\n",
       "    106.0,\n",
       "    105.0,\n",
       "    117.0,\n",
       "    117.0,\n",
       "    108.0,\n",
       "    106.0,\n",
       "    103.0,\n",
       "    113.0,\n",
       "    108.0,\n",
       "    106.0,\n",
       "    110.0,\n",
       "    121.0,\n",
       "    108.0,\n",
       "    121.0,\n",
       "    114.0,\n",
       "    102.0,\n",
       "    112.0,\n",
       "    109.0,\n",
       "    120.0,\n",
       "    114.0,\n",
       "    104.0,\n",
       "    111.0,\n",
       "    114.0,\n",
       "    108.0,\n",
       "    116.0,\n",
       "    113.0,\n",
       "    111.0,\n",
       "    111.0,\n",
       "    119.0,\n",
       "    108.0],\n",
       "   'policy_agent-1_reward': [98.0,\n",
       "    105.0,\n",
       "    98.0,\n",
       "    100.0,\n",
       "    109.0,\n",
       "    102.0,\n",
       "    102.0,\n",
       "    112.0,\n",
       "    109.0,\n",
       "    101.0,\n",
       "    106.0,\n",
       "    103.0,\n",
       "    105.0,\n",
       "    100.0,\n",
       "    110.0,\n",
       "    105.0,\n",
       "    108.0,\n",
       "    98.0,\n",
       "    112.0,\n",
       "    104.0,\n",
       "    103.0,\n",
       "    96.0,\n",
       "    107.0,\n",
       "    101.0,\n",
       "    116.0,\n",
       "    102.0,\n",
       "    98.0,\n",
       "    100.0,\n",
       "    104.0,\n",
       "    106.0,\n",
       "    106.0,\n",
       "    102.0,\n",
       "    99.0,\n",
       "    110.0,\n",
       "    103.0,\n",
       "    106.0,\n",
       "    100.0,\n",
       "    108.0,\n",
       "    119.0,\n",
       "    105.0,\n",
       "    110.0,\n",
       "    113.0,\n",
       "    99.0,\n",
       "    99.0,\n",
       "    108.0,\n",
       "    107.0,\n",
       "    102.0,\n",
       "    105.0,\n",
       "    108.0,\n",
       "    98.0,\n",
       "    106.0,\n",
       "    105.0,\n",
       "    111.0,\n",
       "    99.0,\n",
       "    102.0,\n",
       "    108.0,\n",
       "    103.0,\n",
       "    103.0,\n",
       "    100.0,\n",
       "    101.0,\n",
       "    109.0,\n",
       "    111.0,\n",
       "    103.0,\n",
       "    104.0,\n",
       "    102.0,\n",
       "    100.0,\n",
       "    102.0,\n",
       "    100.0,\n",
       "    111.0,\n",
       "    100.0,\n",
       "    106.0,\n",
       "    110.0,\n",
       "    102.0,\n",
       "    102.0,\n",
       "    98.0,\n",
       "    101.0,\n",
       "    113.0,\n",
       "    103.0,\n",
       "    108.0,\n",
       "    101.0,\n",
       "    105.0,\n",
       "    101.0,\n",
       "    108.0,\n",
       "    111.0,\n",
       "    99.0,\n",
       "    117.0,\n",
       "    97.0,\n",
       "    114.0,\n",
       "    100.0,\n",
       "    99.0,\n",
       "    104.0,\n",
       "    106.0,\n",
       "    109.0,\n",
       "    103.0,\n",
       "    101.0,\n",
       "    108.0,\n",
       "    106.0,\n",
       "    106.0,\n",
       "    99.0,\n",
       "    103.0]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.22941244037933456,\n",
       "   'mean_inference_ms': 3.0367384088209275,\n",
       "   'mean_action_processing_ms': 0.07381911641842744,\n",
       "   'mean_env_wait_ms': 0.057110162523905404,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'off_policy_estimator': {},\n",
       "  'num_healthy_workers': 0,\n",
       "  'timesteps_total': 100000,\n",
       "  'agent_timesteps_total': 200000,\n",
       "  'timers': {'learn_time_ms': 15.988, 'learn_throughput': 2001.547},\n",
       "  'info': {'learner': {'agent-0': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': array(1.8093994, dtype=float32),\n",
       "     'cur_lr': 0.0005,\n",
       "     'mean_q': 33.36493682861328,\n",
       "     'min_q': 0.9183366298675537,\n",
       "     'max_q': 149.7140655517578,\n",
       "     'mean_td_error': 2.048804759979248},\n",
       "    'agent-1': {'allreduce_latency': 0.0,\n",
       "     'grad_gnorm': array(0.74571246, dtype=float32),\n",
       "     'cur_lr': 0.0005,\n",
       "     'mean_q': 35.550045013427734,\n",
       "     'min_q': 0.9635365009307861,\n",
       "     'max_q': 127.22663879394531,\n",
       "     'mean_td_error': 2.2260630130767822}},\n",
       "   'num_steps_sampled': 100000,\n",
       "   'num_agent_steps_sampled': 200000,\n",
       "   'num_steps_trained': 784064,\n",
       "   'num_agent_steps_trained': 1568128,\n",
       "   'last_target_update_ts': 99888,\n",
       "   'num_target_updates': 196},\n",
       "  'done': True,\n",
       "  'episodes_total': 1000,\n",
       "  'training_iteration': 100,\n",
       "  'experiment_id': 'fe9d180d9dcf4544b62ec7ccc2e5caca',\n",
       "  'date': '2021-08-12_16-02-55',\n",
       "  'timestamp': 1628740975,\n",
       "  'time_this_iter_s': 9.285247325897217,\n",
       "  'time_total_s': 980.3930802345276,\n",
       "  'pid': 127629,\n",
       "  'hostname': 'coolo-computer',\n",
       "  'node_ip': '192.168.1.21',\n",
       "  'config': {'num_workers': 0,\n",
       "   'num_envs_per_worker': 1,\n",
       "   'create_env_on_driver': False,\n",
       "   'rollout_fragment_length': 4,\n",
       "   'batch_mode': 'truncate_episodes',\n",
       "   'train_batch_size': 32,\n",
       "   'model': {'_use_default_native_models': False,\n",
       "    'fcnet_hiddens': [256, 256],\n",
       "    'fcnet_activation': 'tanh',\n",
       "    'conv_filters': None,\n",
       "    'conv_activation': 'relu',\n",
       "    'post_fcnet_hiddens': [],\n",
       "    'post_fcnet_activation': 'relu',\n",
       "    'free_log_std': False,\n",
       "    'no_final_linear': False,\n",
       "    'vf_share_layers': True,\n",
       "    'use_lstm': False,\n",
       "    'max_seq_len': 20,\n",
       "    'lstm_cell_size': 256,\n",
       "    'lstm_use_prev_action': False,\n",
       "    'lstm_use_prev_reward': False,\n",
       "    '_time_major': False,\n",
       "    'use_attention': False,\n",
       "    'attention_num_transformer_units': 1,\n",
       "    'attention_dim': 64,\n",
       "    'attention_num_heads': 1,\n",
       "    'attention_head_dim': 32,\n",
       "    'attention_memory_inference': 50,\n",
       "    'attention_memory_training': 50,\n",
       "    'attention_position_wise_mlp_dim': 32,\n",
       "    'attention_init_gru_gate_bias': 2.0,\n",
       "    'attention_use_n_prev_actions': 0,\n",
       "    'attention_use_n_prev_rewards': 0,\n",
       "    'num_framestacks': 'auto',\n",
       "    'dim': 84,\n",
       "    'grayscale': False,\n",
       "    'zero_mean': True,\n",
       "    'custom_model': None,\n",
       "    'custom_model_config': {},\n",
       "    'custom_action_dist': None,\n",
       "    'custom_preprocessor': None,\n",
       "    'lstm_use_prev_action_reward': -1,\n",
       "    'framestack': True},\n",
       "   'optimizer': {},\n",
       "   'gamma': 0.99,\n",
       "   'horizon': None,\n",
       "   'soft_horizon': False,\n",
       "   'no_done_at_end': False,\n",
       "   'env': 'two_agent_MG_env',\n",
       "   'env_config': {},\n",
       "   'env_task_fn': None,\n",
       "   'render_env': False,\n",
       "   'record_env': False,\n",
       "   'normalize_actions': False,\n",
       "   'clip_rewards': None,\n",
       "   'clip_actions': True,\n",
       "   'preprocessor_pref': 'deepmind',\n",
       "   'lr': 0.0005,\n",
       "   'log_level': 'WARN',\n",
       "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "   'ignore_worker_failures': False,\n",
       "   'log_sys_usage': True,\n",
       "   'fake_sampler': False,\n",
       "   'framework': 'torch',\n",
       "   'eager_tracing': False,\n",
       "   'explore': True,\n",
       "   'exploration_config': {'type': 'EpsilonGreedy',\n",
       "    'initial_epsilon': 1.0,\n",
       "    'final_epsilon': 0.02,\n",
       "    'epsilon_timesteps': 10000},\n",
       "   'evaluation_interval': None,\n",
       "   'evaluation_num_episodes': 10,\n",
       "   'evaluation_parallel_to_training': False,\n",
       "   'in_evaluation': False,\n",
       "   'evaluation_config': {'explore': False},\n",
       "   'evaluation_num_workers': 0,\n",
       "   'custom_eval_function': None,\n",
       "   'sample_async': False,\n",
       "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "   'observation_filter': 'NoFilter',\n",
       "   'synchronize_filters': True,\n",
       "   'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "    'inter_op_parallelism_threads': 2,\n",
       "    'gpu_options': {'allow_growth': True},\n",
       "    'log_device_placement': False,\n",
       "    'device_count': {'CPU': 1},\n",
       "    'allow_soft_placement': True},\n",
       "   'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "    'inter_op_parallelism_threads': 8},\n",
       "   'compress_observations': False,\n",
       "   'collect_metrics_timeout': 180,\n",
       "   'metrics_smoothing_episodes': 100,\n",
       "   'remote_worker_envs': False,\n",
       "   'remote_env_batch_wait_ms': 0,\n",
       "   'min_iter_time_s': 1,\n",
       "   'timesteps_per_iteration': 1000,\n",
       "   'seed': None,\n",
       "   'extra_python_environs_for_driver': {},\n",
       "   'extra_python_environs_for_worker': {},\n",
       "   'num_gpus': 0,\n",
       "   '_fake_gpus': False,\n",
       "   'num_cpus_per_worker': 1,\n",
       "   'num_gpus_per_worker': 0,\n",
       "   'custom_resources_per_worker': {},\n",
       "   'num_cpus_for_driver': 1,\n",
       "   'placement_strategy': 'PACK',\n",
       "   'input': 'sampler',\n",
       "   'input_evaluation': ['is', 'wis'],\n",
       "   'postprocess_inputs': False,\n",
       "   'shuffle_buffer_size': 0,\n",
       "   'output': None,\n",
       "   'output_compress_columns': ['obs', 'new_obs'],\n",
       "   'output_max_file_size': 67108864,\n",
       "   'multiagent': {'policies': {'agent-0': (ray.rllib.policy.policy_template.DQNTorchPolicy,\n",
       "      Box(0.0, 1.0, (400,), float32),\n",
       "      Discrete(2),\n",
       "      {'num_workers': 0,\n",
       "       'num_envs_per_worker': 1,\n",
       "       'create_env_on_driver': False,\n",
       "       'rollout_fragment_length': 4,\n",
       "       'batch_mode': 'truncate_episodes',\n",
       "       'train_batch_size': 32,\n",
       "       'model': {'_use_default_native_models': False,\n",
       "        'fcnet_hiddens': [256, 32, 8],\n",
       "        'fcnet_activation': 'tanh',\n",
       "        'conv_filters': None,\n",
       "        'conv_activation': 'relu',\n",
       "        'post_fcnet_hiddens': [],\n",
       "        'post_fcnet_activation': 'relu',\n",
       "        'free_log_std': False,\n",
       "        'no_final_linear': False,\n",
       "        'vf_share_layers': True,\n",
       "        'use_lstm': False,\n",
       "        'max_seq_len': 20,\n",
       "        'lstm_cell_size': 256,\n",
       "        'lstm_use_prev_action': False,\n",
       "        'lstm_use_prev_reward': False,\n",
       "        '_time_major': False,\n",
       "        'use_attention': False,\n",
       "        'attention_num_transformer_units': 1,\n",
       "        'attention_dim': 64,\n",
       "        'attention_num_heads': 1,\n",
       "        'attention_head_dim': 32,\n",
       "        'attention_memory_inference': 50,\n",
       "        'attention_memory_training': 50,\n",
       "        'attention_position_wise_mlp_dim': 32,\n",
       "        'attention_init_gru_gate_bias': 2.0,\n",
       "        'attention_use_n_prev_actions': 0,\n",
       "        'attention_use_n_prev_rewards': 0,\n",
       "        'num_framestacks': 'auto',\n",
       "        'dim': 84,\n",
       "        'grayscale': False,\n",
       "        'zero_mean': True,\n",
       "        'custom_model': None,\n",
       "        'custom_model_config': {},\n",
       "        'custom_action_dist': None,\n",
       "        'custom_preprocessor': None,\n",
       "        'lstm_use_prev_action_reward': -1,\n",
       "        'framestack': True},\n",
       "       'optimizer': {},\n",
       "       'gamma': 0.99,\n",
       "       'horizon': None,\n",
       "       'soft_horizon': False,\n",
       "       'no_done_at_end': False,\n",
       "       'env': None,\n",
       "       'env_config': {},\n",
       "       'env_task_fn': None,\n",
       "       'render_env': False,\n",
       "       'record_env': False,\n",
       "       'normalize_actions': False,\n",
       "       'clip_rewards': None,\n",
       "       'clip_actions': True,\n",
       "       'preprocessor_pref': 'deepmind',\n",
       "       'lr': 0.0005,\n",
       "       'log_level': 'WARN',\n",
       "       'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "       'ignore_worker_failures': False,\n",
       "       'log_sys_usage': True,\n",
       "       'fake_sampler': False,\n",
       "       'framework': 'torch',\n",
       "       'eager_tracing': False,\n",
       "       'explore': True,\n",
       "       'exploration_config': {'type': 'EpsilonGreedy',\n",
       "        'initial_epsilon': 1.0,\n",
       "        'final_epsilon': 0.02,\n",
       "        'epsilon_timesteps': 10000},\n",
       "       'evaluation_interval': None,\n",
       "       'evaluation_num_episodes': 10,\n",
       "       'evaluation_parallel_to_training': False,\n",
       "       'in_evaluation': False,\n",
       "       'evaluation_config': {'explore': False},\n",
       "       'evaluation_num_workers': 0,\n",
       "       'custom_eval_function': None,\n",
       "       'sample_async': False,\n",
       "       'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "       'observation_filter': 'NoFilter',\n",
       "       'synchronize_filters': True,\n",
       "       'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "        'inter_op_parallelism_threads': 2,\n",
       "        'gpu_options': {'allow_growth': True},\n",
       "        'log_device_placement': False,\n",
       "        'device_count': {'CPU': 1},\n",
       "        'allow_soft_placement': True},\n",
       "       'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "        'inter_op_parallelism_threads': 8},\n",
       "       'compress_observations': False,\n",
       "       'collect_metrics_timeout': 180,\n",
       "       'metrics_smoothing_episodes': 100,\n",
       "       'remote_worker_envs': False,\n",
       "       'remote_env_batch_wait_ms': 0,\n",
       "       'min_iter_time_s': 1,\n",
       "       'timesteps_per_iteration': 1000,\n",
       "       'seed': None,\n",
       "       'extra_python_environs_for_driver': {},\n",
       "       'extra_python_environs_for_worker': {},\n",
       "       'num_gpus': 0,\n",
       "       '_fake_gpus': False,\n",
       "       'num_cpus_per_worker': 1,\n",
       "       'num_gpus_per_worker': 0,\n",
       "       'custom_resources_per_worker': {},\n",
       "       'num_cpus_for_driver': 1,\n",
       "       'placement_strategy': 'PACK',\n",
       "       'input': 'sampler',\n",
       "       'input_evaluation': ['is', 'wis'],\n",
       "       'postprocess_inputs': False,\n",
       "       'shuffle_buffer_size': 0,\n",
       "       'output': None,\n",
       "       'output_compress_columns': ['obs', 'new_obs'],\n",
       "       'output_max_file_size': 67108864,\n",
       "       'multiagent': {'policies': {},\n",
       "        'policy_mapping_fn': None,\n",
       "        'policies_to_train': None,\n",
       "        'observation_fn': None,\n",
       "        'replay_mode': 'independent',\n",
       "        'count_steps_by': 'env_steps'},\n",
       "       'logger_config': None,\n",
       "       'simple_optimizer': -1,\n",
       "       'monitor': -1,\n",
       "       'num_atoms': 1,\n",
       "       'v_min': -10.0,\n",
       "       'v_max': 10.0,\n",
       "       'noisy': True,\n",
       "       'sigma0': 0.5,\n",
       "       'dueling': True,\n",
       "       'hiddens': [256],\n",
       "       'double_q': True,\n",
       "       'n_step': 3,\n",
       "       'target_network_update_freq': 500,\n",
       "       'buffer_size': 50000,\n",
       "       'replay_sequence_length': 1,\n",
       "       'prioritized_replay': True,\n",
       "       'prioritized_replay_alpha': 0.6,\n",
       "       'prioritized_replay_beta': 0.4,\n",
       "       'final_prioritized_replay_beta': 0.4,\n",
       "       'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       "       'prioritized_replay_eps': 1e-06,\n",
       "       'before_learn_on_batch': None,\n",
       "       'training_intensity': None,\n",
       "       'lr_schedule': None,\n",
       "       'adam_epsilon': 1e-08,\n",
       "       'grad_clip': 40,\n",
       "       'learning_starts': 1000,\n",
       "       'worker_side_prioritization': False}),\n",
       "     'agent-1': (ray.rllib.policy.policy_template.DQNTorchPolicy,\n",
       "      Box(0.0, 1.0, (400,), float32),\n",
       "      Discrete(2),\n",
       "      {'num_workers': 0,\n",
       "       'num_envs_per_worker': 1,\n",
       "       'create_env_on_driver': False,\n",
       "       'rollout_fragment_length': 4,\n",
       "       'batch_mode': 'truncate_episodes',\n",
       "       'train_batch_size': 32,\n",
       "       'model': {'_use_default_native_models': False,\n",
       "        'fcnet_hiddens': [256, 32, 8],\n",
       "        'fcnet_activation': 'tanh',\n",
       "        'conv_filters': None,\n",
       "        'conv_activation': 'relu',\n",
       "        'post_fcnet_hiddens': [],\n",
       "        'post_fcnet_activation': 'relu',\n",
       "        'free_log_std': False,\n",
       "        'no_final_linear': False,\n",
       "        'vf_share_layers': True,\n",
       "        'use_lstm': False,\n",
       "        'max_seq_len': 20,\n",
       "        'lstm_cell_size': 256,\n",
       "        'lstm_use_prev_action': False,\n",
       "        'lstm_use_prev_reward': False,\n",
       "        '_time_major': False,\n",
       "        'use_attention': False,\n",
       "        'attention_num_transformer_units': 1,\n",
       "        'attention_dim': 64,\n",
       "        'attention_num_heads': 1,\n",
       "        'attention_head_dim': 32,\n",
       "        'attention_memory_inference': 50,\n",
       "        'attention_memory_training': 50,\n",
       "        'attention_position_wise_mlp_dim': 32,\n",
       "        'attention_init_gru_gate_bias': 2.0,\n",
       "        'attention_use_n_prev_actions': 0,\n",
       "        'attention_use_n_prev_rewards': 0,\n",
       "        'num_framestacks': 'auto',\n",
       "        'dim': 84,\n",
       "        'grayscale': False,\n",
       "        'zero_mean': True,\n",
       "        'custom_model': None,\n",
       "        'custom_model_config': {},\n",
       "        'custom_action_dist': None,\n",
       "        'custom_preprocessor': None,\n",
       "        'lstm_use_prev_action_reward': -1,\n",
       "        'framestack': True},\n",
       "       'optimizer': {},\n",
       "       'gamma': 0.99,\n",
       "       'horizon': None,\n",
       "       'soft_horizon': False,\n",
       "       'no_done_at_end': False,\n",
       "       'env': None,\n",
       "       'env_config': {},\n",
       "       'env_task_fn': None,\n",
       "       'render_env': False,\n",
       "       'record_env': False,\n",
       "       'normalize_actions': False,\n",
       "       'clip_rewards': None,\n",
       "       'clip_actions': True,\n",
       "       'preprocessor_pref': 'deepmind',\n",
       "       'lr': 0.0005,\n",
       "       'log_level': 'WARN',\n",
       "       'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "       'ignore_worker_failures': False,\n",
       "       'log_sys_usage': True,\n",
       "       'fake_sampler': False,\n",
       "       'framework': 'torch',\n",
       "       'eager_tracing': False,\n",
       "       'explore': True,\n",
       "       'exploration_config': {'type': 'EpsilonGreedy',\n",
       "        'initial_epsilon': 1.0,\n",
       "        'final_epsilon': 0.02,\n",
       "        'epsilon_timesteps': 10000},\n",
       "       'evaluation_interval': None,\n",
       "       'evaluation_num_episodes': 10,\n",
       "       'evaluation_parallel_to_training': False,\n",
       "       'in_evaluation': False,\n",
       "       'evaluation_config': {'explore': False},\n",
       "       'evaluation_num_workers': 0,\n",
       "       'custom_eval_function': None,\n",
       "       'sample_async': False,\n",
       "       'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "       'observation_filter': 'NoFilter',\n",
       "       'synchronize_filters': True,\n",
       "       'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "        'inter_op_parallelism_threads': 2,\n",
       "        'gpu_options': {'allow_growth': True},\n",
       "        'log_device_placement': False,\n",
       "        'device_count': {'CPU': 1},\n",
       "        'allow_soft_placement': True},\n",
       "       'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "        'inter_op_parallelism_threads': 8},\n",
       "       'compress_observations': False,\n",
       "       'collect_metrics_timeout': 180,\n",
       "       'metrics_smoothing_episodes': 100,\n",
       "       'remote_worker_envs': False,\n",
       "       'remote_env_batch_wait_ms': 0,\n",
       "       'min_iter_time_s': 1,\n",
       "       'timesteps_per_iteration': 1000,\n",
       "       'seed': None,\n",
       "       'extra_python_environs_for_driver': {},\n",
       "       'extra_python_environs_for_worker': {},\n",
       "       'num_gpus': 0,\n",
       "       '_fake_gpus': False,\n",
       "       'num_cpus_per_worker': 1,\n",
       "       'num_gpus_per_worker': 0,\n",
       "       'custom_resources_per_worker': {},\n",
       "       'num_cpus_for_driver': 1,\n",
       "       'placement_strategy': 'PACK',\n",
       "       'input': 'sampler',\n",
       "       'input_evaluation': ['is', 'wis'],\n",
       "       'postprocess_inputs': False,\n",
       "       'shuffle_buffer_size': 0,\n",
       "       'output': None,\n",
       "       'output_compress_columns': ['obs', 'new_obs'],\n",
       "       'output_max_file_size': 67108864,\n",
       "       'multiagent': {'policies': {},\n",
       "        'policy_mapping_fn': None,\n",
       "        'policies_to_train': None,\n",
       "        'observation_fn': None,\n",
       "        'replay_mode': 'independent',\n",
       "        'count_steps_by': 'env_steps'},\n",
       "       'logger_config': None,\n",
       "       'simple_optimizer': -1,\n",
       "       'monitor': -1,\n",
       "       'num_atoms': 1,\n",
       "       'v_min': -10.0,\n",
       "       'v_max': 10.0,\n",
       "       'noisy': True,\n",
       "       'sigma0': 0.5,\n",
       "       'dueling': True,\n",
       "       'hiddens': [256],\n",
       "       'double_q': True,\n",
       "       'n_step': 3,\n",
       "       'target_network_update_freq': 500,\n",
       "       'buffer_size': 50000,\n",
       "       'replay_sequence_length': 1,\n",
       "       'prioritized_replay': True,\n",
       "       'prioritized_replay_alpha': 0.6,\n",
       "       'prioritized_replay_beta': 0.4,\n",
       "       'final_prioritized_replay_beta': 0.4,\n",
       "       'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       "       'prioritized_replay_eps': 1e-06,\n",
       "       'before_learn_on_batch': None,\n",
       "       'training_intensity': None,\n",
       "       'lr_schedule': None,\n",
       "       'adam_epsilon': 1e-08,\n",
       "       'grad_clip': 40,\n",
       "       'learning_starts': 1000,\n",
       "       'worker_side_prioritization': False})},\n",
       "    'policy_mapping_fn': <function __main__.policy_mapping_fn(agent_id)>,\n",
       "    'policies_to_train': None,\n",
       "    'observation_fn': None,\n",
       "    'replay_mode': 'independent',\n",
       "    'count_steps_by': 'env_steps'},\n",
       "   'logger_config': None,\n",
       "   'simple_optimizer': True,\n",
       "   'monitor': -1,\n",
       "   'num_atoms': 1,\n",
       "   'v_min': -10.0,\n",
       "   'v_max': 10.0,\n",
       "   'noisy': False,\n",
       "   'sigma0': 0.5,\n",
       "   'dueling': True,\n",
       "   'hiddens': [256],\n",
       "   'double_q': True,\n",
       "   'n_step': 1,\n",
       "   'target_network_update_freq': 500,\n",
       "   'buffer_size': 50000,\n",
       "   'replay_sequence_length': 1,\n",
       "   'prioritized_replay': True,\n",
       "   'prioritized_replay_alpha': 0.6,\n",
       "   'prioritized_replay_beta': 0.4,\n",
       "   'final_prioritized_replay_beta': 0.4,\n",
       "   'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       "   'prioritized_replay_eps': 1e-06,\n",
       "   'before_learn_on_batch': None,\n",
       "   'training_intensity': None,\n",
       "   'lr_schedule': None,\n",
       "   'adam_epsilon': 1e-08,\n",
       "   'grad_clip': 40,\n",
       "   'learning_starts': 1000,\n",
       "   'worker_side_prioritization': False},\n",
       "  'time_since_restore': 488.7536587715149,\n",
       "  'timesteps_since_restore': 0,\n",
       "  'iterations_since_restore': 50,\n",
       "  'perf': {'cpu_util_percent': 21.9, 'ram_util_percent': 72.0923076923077},\n",
       "  'trial_id': '06046_00000',\n",
       "  'experiment_tag': '0'}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MA_result.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659aff88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
