{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "88039f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "from gym import spaces\n",
    "\n",
    "import players\n",
    "import envs\n",
    "from evaluation import generate_history, is_t4t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "372561dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 17:28:56,436\tINFO worker.py:745 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.examples.env.multi_agent import MultiAgentCartPole\n",
    "from ray.rllib.examples.models.shared_weights_model import \\\n",
    "    SharedWeightsModel1, SharedWeightsModel2, TF2SharedWeightsModel, \\\n",
    "    TorchSharedWeightsModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "# from ray.rllib.policy import PolicySpec\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, PPOTorchPolicy\n",
    "from ray.rllib.agents.ppo import DEFAULT_CONFIG as DEFAULT_CONFIG_PPO\n",
    "\n",
    "from ray.rllib.agents.dqn import DQNTrainer, DQNTorchPolicy\n",
    "from ray.rllib.agents.dqn import  DEFAULT_CONFIG as DEFAULT_CONFIG_DQN\n",
    "\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6c1691b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env('two_agent_MG_env', lambda c: envs.TwoAgentMatrixGameEnv(c))\n",
    "register_env('MG_t4tTD_env', lambda c: envs.MatrixGameEnv(player2=players.TitForTatThenDefect()))\n",
    "# register_env('MG_t4t_env', lambda c: envs.MatrixGameEnv(player2=players.TitForTat()))\n",
    "register_env('MG_t4t_env', lambda c: envs.MatrixGameEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "9b584d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_dqn0 = DEFAULT_CONFIG_DQN.copy()\n",
    "# trainer_config_dqn0['num_workers'] = 3\n",
    "trainer_config_dqn0['n_step'] = 3\n",
    "trainer_config_dqn0['noisy'] = True\n",
    "trainer_config_dqn0['v_min'] = -10.\n",
    "trainer_config_dqn0['v_max'] = 10.\n",
    "trainer_config_dqn0['model']['fcnet_hiddens'] = [256,32,8]\n",
    "trainer_config_dqn0['framework'] = 'torch'\n",
    "\n",
    "trainer_config_dqn0['env'] = 'MG_t4t_env'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bf39b94b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/2.72 GiB heap, 0.0/1.36 GiB objects<br>Result logdir: /home/peter/ray_results/test_pretrain<br>Number of trials: 1/1 (1 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 17:29:06,949\tINFO tune.py:549 -- Total run time: 9.29 seconds (9.04 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# ray.init(ignore_reinit_error=True)\n",
    "results = tune.run(\n",
    "    \"DQN\",\n",
    "    stop={\"training_iteration\":2},\n",
    "    name='test_pretrain',\n",
    "    config=trainer_config_dqn0,\n",
    "#     config={\n",
    "# #         \"num_gpus\": 0,\n",
    "# #         \"num_workers\": 1,\n",
    "# #         \"lr\": tune.grid_search([0.01, 0.001, 0.0001]),\n",
    "#         \"config\":trainer_config_ppo\n",
    "#     },\n",
    "    verbose=1,\n",
    "    checkpoint_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3b9f2e30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment_id': 'b8c51c90ee0642ce8c0ddc8510d0bbf1',\n",
       " 'iteration': 2,\n",
       " 'timesteps_total': None,\n",
       " 'time_total': 7.438932657241821,\n",
       " 'episodes_total': 20,\n",
       " 'ray_version': '1.4.1'}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent0.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a70e9056",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'e' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-6433ab5a13a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQNTorchPolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'e' is not defined"
     ]
    }
   ],
   "source": [
    "hi = DQNTorchPolicy(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de20a710",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:52:13,743\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "2021-08-10 16:52:13,840\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "agent0 = DQNTrainer(config=trainer_config_dqn0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "334b10b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 16:52:16,882\tINFO trainable.py:377 -- Restored on 192.168.1.21 from checkpoint: /home/peter/ray_results/test_pretrain/DQN_MG_t4t_env_b0c4e_00000_0_2021-08-10_16-51-57/checkpoint_000002/checkpoint-2\n",
      "2021-08-10 16:52:16,883\tINFO trainable.py:385 -- Current state after restoring: {'_iteration': 2, '_timesteps_total': None, '_time_total': 7.438932657241821, '_episodes_total': 20}\n"
     ]
    }
   ],
   "source": [
    "agent0.restore(results.get_last_checkpoint())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f429352e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent0.get_policy().get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbf8267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, stop_criteria):\n",
    "    \"\"\"\n",
    "    Train an RLlib PPO agent using tune until any of the configured stopping criteria is met.\n",
    "    :param stop_criteria: Dict with stopping criteria.\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/execution.html#tune-run\n",
    "    :return: Return the path to the saved agent (checkpoint) and tune's ExperimentAnalysis object\n",
    "        See https://docs.ray.io/en/latest/tune/api_docs/analysis.html#experimentanalysis-tune-experimentanalysis\n",
    "    \"\"\"\n",
    "    analysis = ray.tune.run(ppo.PPOTrainer, config=self.config, local_dir=self.save_dir, stop=stop_criteria,\n",
    "                            checkpoint_at_end=True)\n",
    "    # list of lists: one list per checkpoint; each checkpoint list contains 1st the path, 2nd the metric value\n",
    "    checkpoints = analysis.get_trial_checkpoints_paths(trial=analysis.get_best_trial('episode_reward_mean'),\n",
    "                                                       metric='episode_reward_mean')\n",
    "    # retriev the checkpoint path; we only have a single checkpoint, so take the first one\n",
    "    checkpoint_path = checkpoints[0][0]\n",
    "    return checkpoint_path, analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "40d76276",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_dqn0 = DEFAULT_CONFIG_DQN.copy()\n",
    "# trainer_config_dqn0['num_workers'] = 3\n",
    "trainer_config_dqn0['n_step'] = 3\n",
    "trainer_config_dqn0['noisy'] = True\n",
    "trainer_config_dqn0['v_min'] = -10.\n",
    "trainer_config_dqn0['v_max'] = 10.\n",
    "trainer_config_dqn0['model']['fcnet_hiddens'] = [256,32,8]\n",
    "trainer_config_dqn1['framework'] = 'torch'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "52ac55d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_dqn1 = DEFAULT_CONFIG_DQN.copy()\n",
    "# trainer_config_dqn1['num_workers'] = 3\n",
    "trainer_config_dqn1['n_step'] = 3\n",
    "trainer_config_dqn1['noisy'] = True\n",
    "trainer_config_dqn1['v_min'] = -10.\n",
    "trainer_config_dqn1['v_max'] = 10.\n",
    "trainer_config_dqn1['model']['fcnet_hiddens'] = [256,32,8]\n",
    "trainer_config_dqn1['framework'] = 'torch'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c2705616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0faacb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def env_creator(_):\n",
    "    return envs.TwoAgentMatrixGameEnv()\n",
    "single_env = envs.TwoAgentMatrixGameEnv()\n",
    "env_name = \"two_agent_MG_env\"\n",
    "register_env(env_name, env_creator)\n",
    "\n",
    "\n",
    "obs_space = single_env.observation_space\n",
    "act_space = single_env.action_space\n",
    "num_agents = single_env.num_agents\n",
    "# DQNTorchPolicy0 = DQNTorchPolicy.copy()\n",
    "\n",
    "def gen_policy(i):\n",
    "#     return (PPOTorchPolicy, obs_space, act_space, {})\n",
    "    if i == 0:\n",
    "        conf = trainer_config_dqn0\n",
    "#         return (DQNTorchPolicy0, obs_space, act_space, conf)\n",
    "#         policy0 = copy.copy(agent0.get_policy())\n",
    "#         policy0 = DQNTorchPolicy(obs_space, act_space, conf)\n",
    "        policy0 = DQNTorchPolicy\n",
    "#         policy0.set_weights(agent0.get_weights()['default_policy'])\n",
    "#         policy0 = lambda c: policy0\n",
    "        return (policy0, obs_space, act_space, conf)\n",
    "\n",
    "    elif i == 1:\n",
    "        conf = trainer_config_dqn1\n",
    "        return (DQNTorchPolicy, obs_space, act_space, conf)\n",
    "    else:\n",
    "        print('NO CONF!')\n",
    "    \n",
    "    return (DQNTorchPolicy, obs_space, act_space, conf)\n",
    "\n",
    "policy_graphs = {}\n",
    "\n",
    "\n",
    "\n",
    "for i in range(num_agents):\n",
    "    policy_graphs['agent-' + str(i)] = gen_policy(i)\n",
    "def policy_mapping_fn(agent_id):\n",
    "        return 'agent-' + str(agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f8fcc793",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_dqn1['model']['fcnet_hiddens'] = [3]\n",
    "policy0 = DQNTorchPolicy(obs_space, act_space, trainer_config_dqn0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "14e24547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment_id': 'b8c51c90ee0642ce8c0ddc8510d0bbf1',\n",
       " 'iteration': 2,\n",
       " 'timesteps_total': None,\n",
       " 'time_total': 7.438932657241821,\n",
       " 'episodes_total': 20,\n",
       " 'ray_version': '1.4.1'}"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent0.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5b80654e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(ray.rllib.policy.policy_template.DQNTorchPolicy,\n",
       " Box(0.0, 1.0, (400,), float32),\n",
       " Discrete(2),\n",
       " {'num_workers': 0,\n",
       "  'num_envs_per_worker': 1,\n",
       "  'create_env_on_driver': False,\n",
       "  'rollout_fragment_length': 4,\n",
       "  'batch_mode': 'truncate_episodes',\n",
       "  'train_batch_size': 32,\n",
       "  'model': {'_use_default_native_models': False,\n",
       "   'fcnet_hiddens': [3],\n",
       "   'fcnet_activation': 'tanh',\n",
       "   'conv_filters': None,\n",
       "   'conv_activation': 'relu',\n",
       "   'post_fcnet_hiddens': [],\n",
       "   'post_fcnet_activation': 'relu',\n",
       "   'free_log_std': False,\n",
       "   'no_final_linear': True,\n",
       "   'vf_share_layers': True,\n",
       "   'use_lstm': False,\n",
       "   'max_seq_len': 20,\n",
       "   'lstm_cell_size': 256,\n",
       "   'lstm_use_prev_action': False,\n",
       "   'lstm_use_prev_reward': False,\n",
       "   '_time_major': False,\n",
       "   'use_attention': False,\n",
       "   'attention_num_transformer_units': 1,\n",
       "   'attention_dim': 64,\n",
       "   'attention_num_heads': 1,\n",
       "   'attention_head_dim': 32,\n",
       "   'attention_memory_inference': 50,\n",
       "   'attention_memory_training': 50,\n",
       "   'attention_position_wise_mlp_dim': 32,\n",
       "   'attention_init_gru_gate_bias': 2.0,\n",
       "   'attention_use_n_prev_actions': 0,\n",
       "   'attention_use_n_prev_rewards': 0,\n",
       "   'num_framestacks': 0,\n",
       "   'dim': 84,\n",
       "   'grayscale': False,\n",
       "   'zero_mean': True,\n",
       "   'custom_model': None,\n",
       "   'custom_model_config': {},\n",
       "   'custom_action_dist': None,\n",
       "   'custom_preprocessor': None,\n",
       "   'lstm_use_prev_action_reward': -1,\n",
       "   'framestack': True},\n",
       "  'optimizer': {},\n",
       "  'gamma': 0.99,\n",
       "  'horizon': None,\n",
       "  'soft_horizon': False,\n",
       "  'no_done_at_end': False,\n",
       "  'env': 'MG_t4t_env',\n",
       "  'env_config': {},\n",
       "  'env_task_fn': None,\n",
       "  'render_env': False,\n",
       "  'record_env': False,\n",
       "  'normalize_actions': False,\n",
       "  'clip_rewards': None,\n",
       "  'clip_actions': True,\n",
       "  'preprocessor_pref': 'deepmind',\n",
       "  'lr': 0.0005,\n",
       "  'log_level': 'WARN',\n",
       "  'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "  'ignore_worker_failures': False,\n",
       "  'log_sys_usage': True,\n",
       "  'fake_sampler': False,\n",
       "  'framework': 'torch',\n",
       "  'eager_tracing': False,\n",
       "  'explore': True,\n",
       "  'exploration_config': {'type': 'EpsilonGreedy',\n",
       "   'initial_epsilon': 1.0,\n",
       "   'final_epsilon': 0.02,\n",
       "   'epsilon_timesteps': 10000},\n",
       "  'evaluation_interval': None,\n",
       "  'evaluation_num_episodes': 10,\n",
       "  'evaluation_parallel_to_training': False,\n",
       "  'in_evaluation': False,\n",
       "  'evaluation_config': {'explore': False},\n",
       "  'evaluation_num_workers': 0,\n",
       "  'custom_eval_function': None,\n",
       "  'sample_async': False,\n",
       "  'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "  'observation_filter': 'NoFilter',\n",
       "  'synchronize_filters': True,\n",
       "  'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "   'inter_op_parallelism_threads': 2,\n",
       "   'gpu_options': {'allow_growth': True},\n",
       "   'log_device_placement': False,\n",
       "   'device_count': {'CPU': 1},\n",
       "   'allow_soft_placement': True},\n",
       "  'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "   'inter_op_parallelism_threads': 8},\n",
       "  'compress_observations': False,\n",
       "  'collect_metrics_timeout': 180,\n",
       "  'metrics_smoothing_episodes': 100,\n",
       "  'remote_worker_envs': False,\n",
       "  'remote_env_batch_wait_ms': 0,\n",
       "  'min_iter_time_s': 1,\n",
       "  'timesteps_per_iteration': 1000,\n",
       "  'seed': None,\n",
       "  'extra_python_environs_for_driver': {},\n",
       "  'extra_python_environs_for_worker': {},\n",
       "  'num_gpus': 0,\n",
       "  '_fake_gpus': False,\n",
       "  'num_cpus_per_worker': 1,\n",
       "  'num_gpus_per_worker': 0,\n",
       "  'custom_resources_per_worker': {},\n",
       "  'num_cpus_for_driver': 1,\n",
       "  'placement_strategy': 'PACK',\n",
       "  'input': 'sampler',\n",
       "  'input_evaluation': ['is', 'wis'],\n",
       "  'postprocess_inputs': False,\n",
       "  'shuffle_buffer_size': 0,\n",
       "  'output': None,\n",
       "  'output_compress_columns': ['obs', 'new_obs'],\n",
       "  'output_max_file_size': 67108864,\n",
       "  'multiagent': {'policies': {},\n",
       "   'policy_mapping_fn': None,\n",
       "   'policies_to_train': None,\n",
       "   'observation_fn': None,\n",
       "   'replay_mode': 'independent',\n",
       "   'count_steps_by': 'env_steps'},\n",
       "  'logger_config': None,\n",
       "  'simple_optimizer': -1,\n",
       "  'monitor': -1,\n",
       "  'num_atoms': 1,\n",
       "  'v_min': -10.0,\n",
       "  'v_max': 10.0,\n",
       "  'noisy': True,\n",
       "  'sigma0': 0.5,\n",
       "  'dueling': True,\n",
       "  'hiddens': [256],\n",
       "  'double_q': True,\n",
       "  'n_step': 3,\n",
       "  'target_network_update_freq': 500,\n",
       "  'buffer_size': 50000,\n",
       "  'replay_sequence_length': 1,\n",
       "  'prioritized_replay': True,\n",
       "  'prioritized_replay_alpha': 0.6,\n",
       "  'prioritized_replay_beta': 0.4,\n",
       "  'final_prioritized_replay_beta': 0.4,\n",
       "  'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       "  'prioritized_replay_eps': 1e-06,\n",
       "  'before_learn_on_batch': None,\n",
       "  'training_intensity': None,\n",
       "  'lr_schedule': None,\n",
       "  'adam_epsilon': 1e-08,\n",
       "  'grad_clip': 40,\n",
       "  'learning_starts': 1000,\n",
       "  'worker_side_prioritization': False})"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_graphs['agent-0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "07fb38f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"log_level\": \"WARN\",\n",
    "#     \"num_workers\": 3,\n",
    "#     \"num_cpus_for_driver\": 1,\n",
    "#     \"num_cpus_per_worker\": 1,\n",
    "#     \"lr\": 5e-3,\n",
    "#     \"model\":{\"fcnet_hiddens\": [1024, 512,256,32,8]},\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policy_graphs,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "    },\n",
    "    \"env\": \"two_agent_MG_env\",\n",
    "    'framework': 'torch'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fa11f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "exp_name = 'TA_TEST2'\n",
    "exp_dict = {\n",
    "        'name': exp_name,\n",
    "        'run_or_experiment': 'DQN',\n",
    "        \"stop\": {\n",
    "            \"training_iteration\": 200\n",
    "        },\n",
    "        'checkpoint_freq': 0,\n",
    "        'checkpoint_at_end': True,\n",
    "        \"config\": config,\n",
    "}\n",
    "# ray.init()\n",
    "tune.run(**exp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "793f167f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'experiment_id': 'b8c51c90ee0642ce8c0ddc8510d0bbf1',\n",
       " 'iteration': 2,\n",
       " 'timesteps_total': None,\n",
       " 'time_total': 7.438932657241821,\n",
       " 'episodes_total': 20,\n",
       " 'ray_version': '1.4.1'}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dd64c4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy0 = DQNTorchPolicy(obs_space, act_space, trainer_config_dqn0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "45b703c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'obs': <ray.rllib.policy.view_requirement.ViewRequirement at 0x7fa138505d90>,\n",
       " 'new_obs': <ray.rllib.policy.view_requirement.ViewRequirement at 0x7fa0aa8e2190>,\n",
       " 'actions': <ray.rllib.policy.view_requirement.ViewRequirement at 0x7fa0aa8e2340>,\n",
       " 'rewards': <ray.rllib.policy.view_requirement.ViewRequirement at 0x7fa0aa8e2dc0>,\n",
       " 'dones': <ray.rllib.policy.view_requirement.ViewRequirement at 0x7fa0aa8e23a0>,\n",
       " 'infos': <ray.rllib.policy.view_requirement.ViewRequirement at 0x7fa0aa8e2580>,\n",
       " 'eps_id': <ray.rllib.policy.view_requirement.ViewRequirement at 0x7fa0aa8e2a90>,\n",
       " 'unroll_id': <ray.rllib.policy.view_requirement.ViewRequirement at 0x7fa0aa8e20d0>,\n",
       " 'agent_index': <ray.rllib.policy.view_requirement.ViewRequirement at 0x7fa0aa8e27c0>,\n",
       " 'weights': <ray.rllib.policy.view_requirement.ViewRequirement at 0x7fa0ab09ba30>}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy0.view_requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "70059ba7",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for FullyConnectedNetwork_as_DQNTorchModel:\n\tMissing key(s) in state_dict: \"_hidden_layers.0._model.0.weight\", \"_hidden_layers.0._model.0.bias\", \"_hidden_layers.1._model.0.weight\", \"_hidden_layers.1._model.0.bias\", \"_hidden_layers.2._model.0.weight\", \"_hidden_layers.2._model.0.bias\", \"_value_branch._model.0.weight\", \"_value_branch._model.0.bias\", \"advantage_module.dueling_A_0.sigma_w\", \"advantage_module.dueling_A_0.sigma_b\", \"advantage_module.dueling_A_0.w\", \"advantage_module.dueling_A_0.b\", \"advantage_module.A.sigma_w\", \"advantage_module.A.sigma_b\", \"advantage_module.A.w\", \"advantage_module.A.b\", \"value_module.dueling_V_0.sigma_w\", \"value_module.dueling_V_0.sigma_b\", \"value_module.dueling_V_0.w\", \"value_module.dueling_V_0.b\", \"value_module.V._model.0.weight\", \"value_module.V._model.0.bias\". \n\tUnexpected key(s) in state_dict: \"default_policy\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-4dc570a5e172>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicy0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/dqn/simple_q_torch_policy.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(weights)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;31m# model, we sync the target network (from the main model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;31m# at the same time.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mTorchPolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelWeights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_torch_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPolicy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0m\u001b[1;32m   1224\u001b[0m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[1;32m   1225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for FullyConnectedNetwork_as_DQNTorchModel:\n\tMissing key(s) in state_dict: \"_hidden_layers.0._model.0.weight\", \"_hidden_layers.0._model.0.bias\", \"_hidden_layers.1._model.0.weight\", \"_hidden_layers.1._model.0.bias\", \"_hidden_layers.2._model.0.weight\", \"_hidden_layers.2._model.0.bias\", \"_value_branch._model.0.weight\", \"_value_branch._model.0.bias\", \"advantage_module.dueling_A_0.sigma_w\", \"advantage_module.dueling_A_0.sigma_b\", \"advantage_module.dueling_A_0.w\", \"advantage_module.dueling_A_0.b\", \"advantage_module.A.sigma_w\", \"advantage_module.A.sigma_b\", \"advantage_module.A.w\", \"advantage_module.A.b\", \"value_module.dueling_V_0.sigma_w\", \"value_module.dueling_V_0.sigma_b\", \"value_module.dueling_V_0.w\", \"value_module.dueling_V_0.b\", \"value_module.V._model.0.weight\", \"value_module.V._model.0.bias\". \n\tUnexpected key(s) in state_dict: \"default_policy\". "
     ]
    }
   ],
   "source": [
    "policy0.set_weights(agent0.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "7ddbbece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_hidden_layers.0._model.0.weight', '_hidden_layers.0._model.0.bias', '_hidden_layers.1._model.0.weight', '_hidden_layers.1._model.0.bias', '_hidden_layers.2._model.0.weight', '_hidden_layers.2._model.0.bias', '_value_branch._model.0.weight', '_value_branch._model.0.bias', 'advantage_module.dueling_A_0.sigma_w', 'advantage_module.dueling_A_0.sigma_b', 'advantage_module.dueling_A_0.w', 'advantage_module.dueling_A_0.b', 'advantage_module.A.sigma_w', 'advantage_module.A.sigma_b', 'advantage_module.A.w', 'advantage_module.A.b', 'value_module.dueling_V_0.sigma_w', 'value_module.dueling_V_0.sigma_b', 'value_module.dueling_V_0.w', 'value_module.dueling_V_0.b', 'value_module.V._model.0.weight', 'value_module.V._model.0.bias'])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent0.get_weights()['default_policy'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "59bef8a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_hidden_layers.0._model.0.weight', '_hidden_layers.0._model.0.bias', '_hidden_layers.1._model.0.weight', '_hidden_layers.1._model.0.bias', '_hidden_layers.2._model.0.weight', '_hidden_layers.2._model.0.bias', '_value_branch._model.0.weight', '_value_branch._model.0.bias', 'advantage_module.dueling_A_0.sigma_w', 'advantage_module.dueling_A_0.sigma_b', 'advantage_module.dueling_A_0.w', 'advantage_module.dueling_A_0.b', 'advantage_module.A.sigma_w', 'advantage_module.A.sigma_b', 'advantage_module.A.w', 'advantage_module.A.b', 'value_module.dueling_V_0.sigma_w', 'value_module.dueling_V_0.sigma_b', 'value_module.dueling_V_0.w', 'value_module.dueling_V_0.b', 'value_module.V._model.0.weight', 'value_module.V._model.0.bias'])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy0.get_weights().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4a7cd6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy0.set_weights(agent0.get_weights()['default_policy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e6a34f1a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (400x1 and 400x256)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py\u001b[0m in \u001b[0;36m_compute_action_helper\u001b[0;34m(self, input_dict, state_batches, seq_lens, explore, timestep)\u001b[0m\n\u001b[1;32m    298\u001b[0m                     \u001b[0mdist_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_out\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                         self.action_distribution_fn(\n\u001b[0m\u001b[1;32m    300\u001b[0m                             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: get_distribution_inputs_and_class() missing 1 required positional argument: 'obs_batch'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-146-cbb04f71adc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpolicy0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py\u001b[0m in \u001b[0;36mcompute_actions\u001b[0;34m(self, obs_batch, state_batches, prev_action_batch, prev_reward_batch, info_batch, episodes, explore, timestep, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstate_batches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m             ]\n\u001b[0;32m--> 238\u001b[0;31m             return self._compute_action_helper(input_dict, state_batches,\n\u001b[0m\u001b[1;32m    239\u001b[0m                                                seq_lens, explore, timestep)\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/utils/threading.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m\"has no attribute '_lock'\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/policy/torch_policy.py\u001b[0m in \u001b[0;36m_compute_action_helper\u001b[0;34m(self, input_dict, state_batches, seq_lens, explore, timestep)\u001b[0m\n\u001b[1;32m    312\u001b[0m                             \u001b[0;34m\"unexpected keyword argument\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                         \u001b[0mdist_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_out\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m                             self.action_distribution_fn(\n\u001b[0m\u001b[1;32m    315\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\u001b[0m in \u001b[0;36mget_distribution_inputs_and_class\u001b[0;34m(policy, model, obs_batch, explore, is_training, **kwargs)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mis_training\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         **kwargs) -> Tuple[TensorType, type, List[TensorType]]:\n\u001b[0;32m--> 213\u001b[0;31m     q_vals = compute_q_values(\n\u001b[0m\u001b[1;32m    214\u001b[0m         \u001b[0mpolicy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"obs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobs_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/dqn/dqn_torch_policy.py\u001b[0m in \u001b[0;36mcompute_q_values\u001b[0;34m(policy, model, input_dict, state_batches, seq_lens, explore, is_training)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"is_training\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m     \u001b[0mmodel_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate_batches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_atoms\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/models/modelv2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m    232\u001b[0m             \u001b[0mrestored\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"obs_flat\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"obs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestored\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_lens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         if ((not isinstance(res, list) and not isinstance(res, tuple))\n\u001b[1;32m    236\u001b[0m                 or len(res) != 2):\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/models/torch/fcnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_dict, state, seq_lens)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"obs_flat\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_flat_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hidden_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_last_flat_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_features\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_logits\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/models/torch/misc.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensorType\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (400x1 and 400x256)"
     ]
    }
   ],
   "source": [
    "policy0.compute_actions(np.random.rand(400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "dea32d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_t4t(agent0, n_samples=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382b5691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "d6b5fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_t4t_2(agent, n_samples):\n",
    "    \n",
    "    agent_actions = []\n",
    "    t4t_actions = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        history, n_turn = generate_history()\n",
    "        action = agent.compute_action(history)\n",
    "        if n_turn == 0:\n",
    "            opponents_last_action = 0\n",
    "        else:\n",
    "            # opponents_last_action = np.where(history[1,:,n_turn-1])\n",
    "            opponents_last_action = np.where(history[1,:,0])\n",
    "            \n",
    "        agent_actions.append(action)\n",
    "        t4t_actions.append(opponents_last_action)\n",
    "        \n",
    "    same_action = [int(agent_actions[i]==t4t_actions[i]) for i in range(n_samples)]\n",
    "    t4t_fraction = np.sum(same_action)/n_samples\n",
    "    \n",
    "    return t4t_fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "8b21b55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5023"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_t4t_2(agent0, n_samples=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1638db4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent0.compute_action(np.random.rand(400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6481cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
