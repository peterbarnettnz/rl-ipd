{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "from gym import spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-27 10:31:56,321\tINFO worker.py:745 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.examples.env.multi_agent import MultiAgentCartPole\n",
    "from ray.rllib.examples.models.shared_weights_model import \\\n",
    "    SharedWeightsModel1, SharedWeightsModel2, TF2SharedWeightsModel, \\\n",
    "    TorchSharedWeightsModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "# from ray.rllib.policy import PolicySpec\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, PPOTorchPolicy\n",
    "from ray.rllib.agents.ppo import DEFAULT_CONFIG as DEFAULT_CONFIG_PPO\n",
    "\n",
    "from ray.rllib.agents.dqn import DQNTrainer, DQNTorchPolicy\n",
    "from ray.rllib.agents.dqn import  DEFAULT_CONFIG as DEFAULT_CONFIG_DQN\n",
    "\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixGame():\n",
    "    \n",
    "    def __init__(self, RPST=(3,1,0,5)):\n",
    "        \n",
    "        self.RPST = RPST\n",
    "        \n",
    "        self.payoff_mat = np.empty((2,2), dtype=np.object)\n",
    "        \n",
    "        self.payoff_mat[0, 0] = (RPST[0], RPST[0])\n",
    "        self.payoff_mat[1, 1] = (RPST[1], RPST[1])\n",
    "        self.payoff_mat[0, 1] = (RPST[2], RPST[3])\n",
    "        self.payoff_mat[1, 0] = (RPST[3], RPST[2])\n",
    "        \n",
    "    def play(self, a_row, a_col):\n",
    "        # for ease of things 0 is coooperate\n",
    "#                            1 is defect\n",
    "        \n",
    "        \n",
    "#         if a_row == 'c':\n",
    "#             row = 0\n",
    "#         else:\n",
    "#             row = 1\n",
    "            \n",
    "#         if a_col == 'c':\n",
    "#             col = 0\n",
    "#         else:\n",
    "#             col = 1\n",
    "            \n",
    "        return self.payoff_mat[a_row, a_col]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoAgentMatrixGameEnv(MultiAgentEnv):\n",
    "    \n",
    "    def __init__(self, RPST=(3,1,0,5), history_n=100):\n",
    "        \n",
    "        self.num_agents = 2\n",
    "        \n",
    "        self.RPST = RPST\n",
    "        self.history_n = history_n\n",
    "        self.history = np.zeros((2,2,self.history_n))\n",
    "        \n",
    "        self._counter = 0\n",
    "        self._setup_spaces()\n",
    "        self.game = MatrixGame(RPST=self.RPST)\n",
    "    \n",
    "    \n",
    "    def _setup_spaces(self):\n",
    "        \n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        self.observation_space = spaces.Box(0, 1,\n",
    "                                           shape=(self.history_n * 4,))\n",
    "        \n",
    "        \n",
    "    def history_to_states(self, history=None):\n",
    "        \n",
    "        if history is None:\n",
    "            history = self.history\n",
    "            \n",
    "        state1 = history.flatten()\n",
    "        state2 = history[::-1,:,:].flatten()\n",
    "        \n",
    "        states = {0: state1, 1:state2}\n",
    "        \n",
    "        return states\n",
    "            \n",
    "        \n",
    "        \n",
    "    def step(self, action_dict):\n",
    "        \n",
    "        print((action_dict[0], action_dict[1]))\n",
    "        rewards = self.game.play(action_dict[0], action_dict[1])\n",
    "        rew = {i: rewards[i] for i in [0, 1]}\n",
    "        \n",
    "        self.history[0, action_dict[0], self._counter] = 1\n",
    "        self.history[1, action_dict[1], self._counter] = 1\n",
    "        \n",
    "        obs = self.history_to_states(self.history)\n",
    "        \n",
    "        self._counter += 1\n",
    "        \n",
    "        is_done = self._counter >= self.history_n\n",
    "        done = {i: is_done for i in [0, 1, \"__all__\"]}\n",
    "        \n",
    "        info = {0:{}, 1:{}}\n",
    "        \n",
    "        return obs, rew, done, info\n",
    "        \n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.history = np.zeros((2,2,self.history_n))\n",
    "        obs = self.history_to_states(self.history)\n",
    "\n",
    "        self._counter = 0\n",
    "        \n",
    "        return obs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TwoAgentMatrixGameEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4, 1: 3}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = (4,3)\n",
    "{i:aa[i] for i in [0,1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({0: array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  1: array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.])},\n",
       " {0: 3, 1: 3},\n",
       " {0: False, 1: False, '__all__': False},\n",
       " {0: {}, 1: {}})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step({0:0,1:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "info ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[0] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[1] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {}, 1: {}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env('twoagent_PD', lambda c: TwoAgentMatrixGameEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_ppo = DEFAULT_CONFIG_PPO.copy()\n",
    "trainer_config_ppo['num_workers'] = 1\n",
    "trainer_config_ppo['num_sgd_iter'] = 20\n",
    "trainer_config_ppo['sgd_minibatch_size'] = 32\n",
    "# trainer_config_ppo['model']['fcnet_hiddens'] = [1024, 512,512, 256,256,32,8]\n",
    "trainer_config_ppo['model']['fcnet_hiddens'] = [256,256,32,8]\n",
    "\n",
    "trainer_config_ppo['num_cpus_per_worker'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-26 15:52:54,124\tINFO trainable.py:101 -- Trainable.setup took 13.018 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-07-26 15:52:54,125\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "trainer = PPOTrainer(trainer_config_ppo, env=\"twoagent_PD\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration 0...\n",
      "agent_timesteps_total: 4000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-26_15-53-24\n",
      "done: false\n",
      "episode_len_mean: 100.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 478.0\n",
      "episode_reward_mean: 455.8\n",
      "episode_reward_min: 428.0\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 20\n",
      "experiment_id: 39b34ab8c778431db4c7d7363eca7f2b\n",
      "hostname: coolo-computer\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6630105972290039\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.031176701188087463\n",
      "        model: {}\n",
      "        policy_loss: -0.036966897547245026\n",
      "        total_loss: 8868.671875\n",
      "        vf_explained_var: -7.157325967455108e-07\n",
      "        vf_loss: 8868.7041015625\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 192.168.1.21\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.47272727272727\n",
      "  ram_util_percent: 90.11363636363636\n",
      "pid: 31107\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.18910751647796706\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.20377746765045215\n",
      "  mean_inference_ms: 2.210391872468917\n",
      "  mean_raw_obs_processing_ms: 0.2931082981458489\n",
      "time_since_restore: 30.39619755744934\n",
      "time_this_iter_s: 30.39619755744934\n",
      "time_total_s: 30.39619755744934\n",
      "timers:\n",
      "  learn_throughput: 164.283\n",
      "  learn_time_ms: 24348.216\n",
      "  load_throughput: 50540.329\n",
      "  load_time_ms: 79.145\n",
      "  sample_throughput: 679.394\n",
      "  sample_time_ms: 5887.602\n",
      "  update_time_ms: 6.434\n",
      "timestamp: 1627271604\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "\n",
      "Training iteration 1...\n",
      "agent_timesteps_total: 8000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-26_15-53-58\n",
      "done: false\n",
      "episode_len_mean: 100.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 478.0\n",
      "episode_reward_mean: 435.475\n",
      "episode_reward_min: 387.0\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 40\n",
      "experiment_id: 39b34ab8c778431db4c7d7363eca7f2b\n",
      "hostname: coolo-computer\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6342822313308716\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011929323896765709\n",
      "        model: {}\n",
      "        policy_loss: -0.026292547583580017\n",
      "        total_loss: 7051.43798828125\n",
      "        vf_explained_var: -9.441375681262798e-08\n",
      "        vf_loss: 7051.4609375\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_steps_sampled: 8000\n",
      "  num_steps_trained: 8000\n",
      "iterations_since_restore: 2\n",
      "node_ip: 192.168.1.21\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.70208333333333\n",
      "  ram_util_percent: 90.23958333333333\n",
      "pid: 31107\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19724825582773636\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.21430118699969797\n",
      "  mean_inference_ms: 2.310685709354304\n",
      "  mean_raw_obs_processing_ms: 0.30467423266134136\n",
      "time_since_restore: 64.54147911071777\n",
      "time_this_iter_s: 34.14528155326843\n",
      "time_total_s: 64.54147911071777\n",
      "timers:\n",
      "  learn_throughput: 155.268\n",
      "  learn_time_ms: 25761.966\n",
      "  load_throughput: 88602.619\n",
      "  load_time_ms: 45.145\n",
      "  sample_throughput: 623.774\n",
      "  sample_time_ms: 6412.578\n",
      "  update_time_ms: 6.202\n",
      "timestamp: 1627271638\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(\"Training iteration {}...\".format(i))\n",
    "    result=trainer.train()\n",
    "    print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TwoAgentMatrixGameEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-93e06fe5a326>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtotal_defect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "n_samples = 50\n",
    "\n",
    "\n",
    "defects = []\n",
    "rewards = []\n",
    "for i in range(n_samples):\n",
    "    state = env.reset()\n",
    "    \n",
    "    total_defect = 0\n",
    "    cum_reward = 0\n",
    "    done = False\n",
    "    i = 0\n",
    "    while not done:\n",
    "        print('===========')\n",
    "        print(i)\n",
    "        print('===========')\n",
    "        \n",
    "        i+=1\n",
    "        action = trainer.compute_actions(state)\n",
    "        print(action)\n",
    "#         total_defect += action\n",
    "        state, reward, done, results = env.step(action)\n",
    "#         cum_reward += reward\n",
    "    defects.append(total_defect)\n",
    "    rewards.append(cum_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def env_creator(_):\n",
    "    return TwoAgentMatrixGameEnv()\n",
    "single_env = TwoAgentMatrixGameEnv()\n",
    "env_name = \"TwoAgent_PD\"\n",
    "register_env(env_name, env_creator)\n",
    "\n",
    "\n",
    "obs_space = single_env.observation_space\n",
    "act_space = single_env.action_space\n",
    "num_agents = single_env.num_agents\n",
    "\n",
    "def gen_policy():\n",
    "    return (PPOTorchPolicy, obs_space, act_space, {})\n",
    "policy_graphs = {}\n",
    "\n",
    "for i in range(num_agents):\n",
    "    policy_graphs['agent-' + str(i)] = gen_policy()\n",
    "def policy_mapping_fn(agent_id):\n",
    "        return 'agent-' + str(agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"log_level\": \"WARN\",\n",
    "    \"num_workers\": 3,\n",
    "    \"num_cpus_for_driver\": 1,\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "    \"lr\": 5e-3,\n",
    "    \"model\":{\"fcnet_hiddens\": [1024, 512,256,32,8]},\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policy_graphs,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "    },\n",
    "    \"env\": \"TwoAgent_PD\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.4/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 15996\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_10-42-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 487.0\n",
      "  episode_reward_mean: 448.8974358974359\n",
      "  episode_reward_min: 410.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 78\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1999999999999999\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.6898711702180287\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0032961564551774315\n",
      "          policy_loss: 0.0025286045930688343\n",
      "          total_loss: 6878.073854476686\n",
      "          vf_explained_var: -1.5137688436084318e-08\n",
      "          vf_loss: 6878.070707775298\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1999999999999999\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.688524255676875\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004566053411176582\n",
      "          policy_loss: 0.00013849109647765993\n",
      "          total_loss: 6878.232049851191\n",
      "          vf_explained_var: 6.622738357719982e-09\n",
      "          vf_loss: 6878.230941530258\n",
      "    num_agent_steps_sampled: 15996\n",
      "    num_agent_steps_trained: 15996\n",
      "    num_steps_sampled: 7998\n",
      "    num_steps_trained: 7998\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.658387096774195\n",
      "    ram_util_percent: 80.71483870967741\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 277.0\n",
      "    agent-1: 277.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 223.23076923076923\n",
      "    agent-1: 225.66666666666666\n",
      "  policy_reward_min:\n",
      "    agent-0: 162.0\n",
      "    agent-1: 176.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11314452998177174\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.12411038408635808\n",
      "    mean_inference_ms: 6.544050254340234\n",
      "    mean_raw_obs_processing_ms: 0.18536995357460506\n",
      "  time_since_restore: 217.2679419517517\n",
      "  time_this_iter_s: 217.2679419517517\n",
      "  time_total_s: 217.2679419517517\n",
      "  timers:\n",
      "    learn_throughput: 40.291\n",
      "    learn_time_ms: 198504.481\n",
      "    sample_throughput: 426.852\n",
      "    sample_time_ms: 18737.156\n",
      "    update_time_ms: 6.206\n",
      "  timestamp: 1627339377\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7998\n",
      "  training_iteration: 1\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         217.268</td><td style=\"text-align: right;\">7998</td><td style=\"text-align: right;\"> 448.897</td><td style=\"text-align: right;\">                 487</td><td style=\"text-align: right;\">                 410</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 31992\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_10-46-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 487.0\n",
      "  episode_reward_mean: 444.45\n",
      "  episode_reward_min: 405.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 159\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.09999999999999995\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.6820998570275685\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010965851231640766\n",
      "          policy_loss: 0.000675432997504397\n",
      "          total_loss: 4987.425509982639\n",
      "          vf_explained_var: -1.8922110545105397e-09\n",
      "          vf_loss: 4987.423719618056\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.09999999999999995\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.6793103066701738\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005452657706983801\n",
      "          policy_loss: -0.0005386829937970828\n",
      "          total_loss: 6232.069560701885\n",
      "          vf_explained_var: -1.3245476715439963e-08\n",
      "          vf_loss: 6232.069545200893\n",
      "    num_agent_steps_sampled: 31992\n",
      "    num_agent_steps_trained: 31992\n",
      "    num_steps_sampled: 15996\n",
      "    num_steps_trained: 15996\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.6\n",
      "    ram_util_percent: 82.91384615384615\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 257.0\n",
      "    agent-1: 280.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 214.25\n",
      "    agent-1: 230.2\n",
      "  policy_reward_min:\n",
      "    agent-0: 160.0\n",
      "    agent-1: 176.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11843321483019988\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13023475676740368\n",
      "    mean_inference_ms: 6.972874926995087\n",
      "    mean_raw_obs_processing_ms: 0.1948964157519777\n",
      "  time_since_restore: 444.83506298065186\n",
      "  time_this_iter_s: 227.56712102890015\n",
      "  time_total_s: 444.83506298065186\n",
      "  timers:\n",
      "    learn_throughput: 39.573\n",
      "    learn_time_ms: 202109.394\n",
      "    sample_throughput: 394.331\n",
      "    sample_time_ms: 20282.435\n",
      "    update_time_ms: 6.398\n",
      "  timestamp: 1627339605\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15996\n",
      "  training_iteration: 2\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         444.835</td><td style=\"text-align: right;\">15996</td><td style=\"text-align: right;\">  444.45</td><td style=\"text-align: right;\">                 487</td><td style=\"text-align: right;\">                 405</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 47988\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_10-50-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 476.0\n",
      "  episode_reward_mean: 430.9\n",
      "  episode_reward_min: 382.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 237\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.09999999999999995\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.6442135998180935\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027008375419037684\n",
      "          policy_loss: -0.006746554311128363\n",
      "          total_loss: 3417.8270205543154\n",
      "          vf_explained_var: 1.8922110545105397e-09\n",
      "          vf_loss: 3417.831077938988\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.09999999999999995\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.6675832243192763\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010252879533716565\n",
      "          policy_loss: -0.0019877577599670205\n",
      "          total_loss: 4411.511924138144\n",
      "          vf_explained_var: -2.838316470743507e-09\n",
      "          vf_loss: 4411.512830946181\n",
      "    num_agent_steps_sampled: 47988\n",
      "    num_agent_steps_trained: 47988\n",
      "    num_steps_sampled: 23994\n",
      "    num_steps_trained: 23994\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.713782051282053\n",
      "    ram_util_percent: 83.69967948717948\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 250.0\n",
      "    agent-1: 270.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 206.4\n",
      "    agent-1: 224.5\n",
      "  policy_reward_min:\n",
      "    agent-0: 168.0\n",
      "    agent-1: 188.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12156148939297186\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13418476038590396\n",
      "    mean_inference_ms: 7.235516054626526\n",
      "    mean_raw_obs_processing_ms: 0.20036474032069043\n",
      "  time_since_restore: 663.2938711643219\n",
      "  time_this_iter_s: 218.45880818367004\n",
      "  time_total_s: 663.2938711643219\n",
      "  timers:\n",
      "    learn_throughput: 39.948\n",
      "    learn_time_ms: 200209.379\n",
      "    sample_throughput: 383.339\n",
      "    sample_time_ms: 20864.051\n",
      "    update_time_ms: 6.223\n",
      "  timestamp: 1627339823\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23994\n",
      "  training_iteration: 3\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         663.294</td><td style=\"text-align: right;\">23994</td><td style=\"text-align: right;\">   430.9</td><td style=\"text-align: right;\">                 476</td><td style=\"text-align: right;\">                 382</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 63984\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_10-54-02\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 456.0\n",
      "  episode_reward_mean: 386.56\n",
      "  episode_reward_min: 342.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 318\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.15000000000000008\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.6472189994085402\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0050972449513094955\n",
      "          policy_loss: 2.3004632177097456e-05\n",
      "          total_loss: 1755.48923843626\n",
      "          vf_explained_var: -7.568844218042159e-09\n",
      "          vf_loss: 1755.4884052579366\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.09999999999999995\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.5457764561214145\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013230698580469286\n",
      "          policy_loss: -0.0038203304663063987\n",
      "          total_loss: 2403.383686368428\n",
      "          vf_explained_var: 9.461055272552699e-10\n",
      "          vf_loss: 2403.3861936538938\n",
      "    num_agent_steps_sampled: 63984\n",
      "    num_agent_steps_trained: 63984\n",
      "    num_steps_sampled: 31992\n",
      "    num_steps_trained: 31992\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.06655948553055\n",
      "    ram_util_percent: 83.4128617363344\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 246.0\n",
      "    agent-1: 266.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 184.38\n",
      "    agent-1: 202.18\n",
      "  policy_reward_min:\n",
      "    agent-0: 138.0\n",
      "    agent-1: 153.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12055454919677942\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13656449456285014\n",
      "    mean_inference_ms: 7.170730446144769\n",
      "    mean_raw_obs_processing_ms: 0.19875849305144316\n",
      "  time_since_restore: 881.6052298545837\n",
      "  time_this_iter_s: 218.31135869026184\n",
      "  time_total_s: 881.6052298545837\n",
      "  timers:\n",
      "    learn_throughput: 40.005\n",
      "    learn_time_ms: 199922.711\n",
      "    sample_throughput: 391.015\n",
      "    sample_time_ms: 20454.448\n",
      "    update_time_ms: 6.119\n",
      "  timestamp: 1627340042\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31992\n",
      "  training_iteration: 4\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         881.605</td><td style=\"text-align: right;\">31992</td><td style=\"text-align: right;\">  386.56</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">                 342</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 79980\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_10-57-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 403.0\n",
      "  episode_reward_mean: 353.05\n",
      "  episode_reward_min: 320.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 399\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.15000000000000008\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.5248797119609894\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004781771783850023\n",
      "          policy_loss: 0.0005746063820662952\n",
      "          total_loss: 1466.4229038783483\n",
      "          vf_explained_var: -4.730527525254047e-09\n",
      "          vf_loss: 1466.4216095455108\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.09999999999999995\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.5248234470685323\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012193220263898616\n",
      "          policy_loss: -0.002295680580631135\n",
      "          total_loss: 1136.7879793681795\n",
      "          vf_explained_var: 9.461055272552699e-10\n",
      "          vf_loss: 1136.789063468812\n",
      "    num_agent_steps_sampled: 79980\n",
      "    num_agent_steps_trained: 79980\n",
      "    num_steps_sampled: 39990\n",
      "    num_steps_trained: 39990\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.213373860182372\n",
      "    ram_util_percent: 83.70699088145896\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 231.0\n",
      "    agent-1: 237.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 179.55\n",
      "    agent-1: 173.5\n",
      "  policy_reward_min:\n",
      "    agent-0: 141.0\n",
      "    agent-1: 124.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12025906950139807\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1363549573370307\n",
      "    mean_inference_ms: 7.146832985625142\n",
      "    mean_raw_obs_processing_ms: 0.19894665545684986\n",
      "  time_since_restore: 1112.0878207683563\n",
      "  time_this_iter_s: 230.48259091377258\n",
      "  time_total_s: 1112.0878207683563\n",
      "  timers:\n",
      "    learn_throughput: 39.613\n",
      "    learn_time_ms: 201903.92\n",
      "    sample_throughput: 390.349\n",
      "    sample_time_ms: 20489.376\n",
      "    update_time_ms: 5.967\n",
      "  timestamp: 1627340272\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39990\n",
      "  training_iteration: 5\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 12.9/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         1112.09</td><td style=\"text-align: right;\">39990</td><td style=\"text-align: right;\">  353.05</td><td style=\"text-align: right;\">                 403</td><td style=\"text-align: right;\">                 320</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 95976\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_11-02-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 383.0\n",
      "  episode_reward_mean: 319.43\n",
      "  episode_reward_min: 260.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 477\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.07500000000000004\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.4641875534776657\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0036923268072559805\n",
      "          policy_loss: 0.0015884676120347446\n",
      "          total_loss: 892.5746159629216\n",
      "          vf_explained_var: 9.461055272552699e-10\n",
      "          vf_loss: 892.57274421813\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.09999999999999995\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.46568439877222456\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0024786427529761568\n",
      "          policy_loss: -0.0025490372190399777\n",
      "          total_loss: 858.2071465386284\n",
      "          vf_explained_var: 2.838316470743507e-09\n",
      "          vf_loss: 858.2094339037699\n",
      "    num_agent_steps_sampled: 95976\n",
      "    num_agent_steps_trained: 95976\n",
      "    num_steps_sampled: 47988\n",
      "    num_steps_trained: 47988\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.340489130434786\n",
      "    ram_util_percent: 84.57146739130435\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 231.0\n",
      "    agent-1: 209.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 163.94\n",
      "    agent-1: 155.49\n",
      "  policy_reward_min:\n",
      "    agent-0: 119.0\n",
      "    agent-1: 118.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11969998826825794\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13549364510624895\n",
      "    mean_inference_ms: 7.1177682040458485\n",
      "    mean_raw_obs_processing_ms: 0.19853769265421645\n",
      "  time_since_restore: 1370.1650683879852\n",
      "  time_this_iter_s: 258.0772476196289\n",
      "  time_total_s: 1370.1650683879852\n",
      "  timers:\n",
      "    learn_throughput: 38.459\n",
      "    learn_time_ms: 207959.963\n",
      "    sample_throughput: 392.502\n",
      "    sample_time_ms: 20376.974\n",
      "    update_time_ms: 5.875\n",
      "  timestamp: 1627340530\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47988\n",
      "  training_iteration: 6\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.0/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         1370.17</td><td style=\"text-align: right;\">47988</td><td style=\"text-align: right;\">  319.43</td><td style=\"text-align: right;\">                 383</td><td style=\"text-align: right;\">                 260</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 111972\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_11-06-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 354.0\n",
      "  episode_reward_mean: 297.5\n",
      "  episode_reward_min: 259.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 558\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.03750000000000002\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.4032939303488958\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0034499725436020643\n",
      "          policy_loss: -0.0006354218644518701\n",
      "          total_loss: 869.8221900576636\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 869.8226899646577\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.049999999999999975\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.42890283938438173\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0036589588562203554\n",
      "          policy_loss: -0.000339436921335402\n",
      "          total_loss: 694.2995208255828\n",
      "          vf_explained_var: 3.7844221090210795e-09\n",
      "          vf_loss: 694.2996884300595\n",
      "    num_agent_steps_sampled: 111972\n",
      "    num_agent_steps_trained: 111972\n",
      "    num_steps_sampled: 55986\n",
      "    num_steps_trained: 55986\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 43.45049504950495\n",
      "    ram_util_percent: 84.72227722772278\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 191.0\n",
      "    agent-1: 179.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 154.95\n",
      "    agent-1: 142.55\n",
      "  policy_reward_min:\n",
      "    agent-0: 115.0\n",
      "    agent-1: 115.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11891588610921047\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1342978058252298\n",
      "    mean_inference_ms: 7.076498486066814\n",
      "    mean_raw_obs_processing_ms: 0.19708616124409242\n",
      "  time_since_restore: 1653.5292789936066\n",
      "  time_this_iter_s: 283.36421060562134\n",
      "  time_total_s: 1653.5292789936066\n",
      "  timers:\n",
      "    learn_throughput: 37.042\n",
      "    learn_time_ms: 215919.475\n",
      "    sample_throughput: 394.481\n",
      "    sample_time_ms: 20274.728\n",
      "    update_time_ms: 6.113\n",
      "  timestamp: 1627340814\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55986\n",
      "  training_iteration: 7\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         1653.53</td><td style=\"text-align: right;\">55986</td><td style=\"text-align: right;\">   297.5</td><td style=\"text-align: right;\">                 354</td><td style=\"text-align: right;\">                 259</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 127968\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_11-11-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 326.0\n",
      "  episode_reward_mean: 282.32\n",
      "  episode_reward_min: 242.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 639\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.01875000000000001\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.34030963977177936\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0017513137333538444\n",
      "          policy_loss: 0.0013848175959927695\n",
      "          total_loss: 874.3887619745163\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 874.3873475089906\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.024999999999999988\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.4355497814360119\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002375537713132438\n",
      "          policy_loss: 0.00020620830002285185\n",
      "          total_loss: 590.95558820452\n",
      "          vf_explained_var: 3.7844221090210795e-09\n",
      "          vf_loss: 590.955314515129\n",
      "    num_agent_steps_sampled: 127968\n",
      "    num_agent_steps_trained: 127968\n",
      "    num_steps_sampled: 63984\n",
      "    num_steps_trained: 63984\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.23275862068966\n",
      "    ram_util_percent: 89.72906403940887\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 195.0\n",
      "    agent-1: 173.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 151.76\n",
      "    agent-1: 130.56\n",
      "  policy_reward_min:\n",
      "    agent-0: 121.0\n",
      "    agent-1: 105.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12070789201853685\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13558716954991934\n",
      "    mean_inference_ms: 7.200478824477711\n",
      "    mean_raw_obs_processing_ms: 0.199106388374525\n",
      "  time_since_restore: 1937.918953180313\n",
      "  time_this_iter_s: 284.38967418670654\n",
      "  time_total_s: 1937.918953180313\n",
      "  timers:\n",
      "    learn_throughput: 36.111\n",
      "    learn_time_ms: 221481.499\n",
      "    sample_throughput: 385.749\n",
      "    sample_time_ms: 20733.667\n",
      "    update_time_ms: 6.256\n",
      "  timestamp: 1627341098\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63984\n",
      "  training_iteration: 8\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         1937.92</td><td style=\"text-align: right;\">63984</td><td style=\"text-align: right;\">  282.32</td><td style=\"text-align: right;\">                 326</td><td style=\"text-align: right;\">                 242</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 143964\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_11-16-21\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 318.0\n",
      "  episode_reward_mean: 274.58\n",
      "  episode_reward_min: 240.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 717\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.009375000000000005\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.34834953480296665\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0011223369196707766\n",
      "          policy_loss: 0.0004992450750063337\n",
      "          total_loss: 784.6564146980406\n",
      "          vf_explained_var: 6.622738357719982e-09\n",
      "          vf_loss: 784.6559002588666\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.012499999999999994\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.3585754831631978\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0030413601595524046\n",
      "          policy_loss: 0.00023852878560622534\n",
      "          total_loss: 579.6238107832652\n",
      "          vf_explained_var: 1.8922110545105397e-09\n",
      "          vf_loss: 579.623533218626\n",
      "    num_agent_steps_sampled: 143964\n",
      "    num_agent_steps_trained: 143964\n",
      "    num_steps_sampled: 71982\n",
      "    num_steps_trained: 71982\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.981637717121586\n",
      "    ram_util_percent: 91.9803970223325\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 183.0\n",
      "    agent-1: 171.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 145.99\n",
      "    agent-1: 128.59\n",
      "  policy_reward_min:\n",
      "    agent-0: 109.0\n",
      "    agent-1: 94.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12262562778594216\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1373613601358166\n",
      "    mean_inference_ms: 7.335346581899495\n",
      "    mean_raw_obs_processing_ms: 0.2016051737374376\n",
      "  time_since_restore: 2220.484981060028\n",
      "  time_this_iter_s: 282.56602787971497\n",
      "  time_total_s: 2220.484981060028\n",
      "  timers:\n",
      "    learn_throughput: 35.456\n",
      "    learn_time_ms: 225575.779\n",
      "    sample_throughput: 378.693\n",
      "    sample_time_ms: 21120.035\n",
      "    update_time_ms: 6.214\n",
      "  timestamp: 1627341381\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71982\n",
      "  training_iteration: 9\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         2220.48</td><td style=\"text-align: right;\">71982</td><td style=\"text-align: right;\">  274.58</td><td style=\"text-align: right;\">                 318</td><td style=\"text-align: right;\">                 240</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 159960\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_11-20-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 318.0\n",
      "  episode_reward_mean: 271.52\n",
      "  episode_reward_min: 243.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 798\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.004687500000000002\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.32558965777593946\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.003628143756132987\n",
      "          policy_loss: -0.000336638374608897\n",
      "          total_loss: 635.5340023949033\n",
      "          vf_explained_var: 9.461055272552699e-10\n",
      "          vf_loss: 635.5343211340526\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.006249999999999997\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.2984038884677584\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006474849524065142\n",
      "          policy_loss: -0.003232782200304052\n",
      "          total_loss: 637.1022304958767\n",
      "          vf_explained_var: 6.622738357719982e-09\n",
      "          vf_loss: 637.1054227314298\n",
      "    num_agent_steps_sampled: 159960\n",
      "    num_agent_steps_trained: 159960\n",
      "    num_steps_sampled: 79980\n",
      "    num_steps_trained: 79980\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.310439560439562\n",
      "    ram_util_percent: 91.90219780219779\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 164.0\n",
      "    agent-1: 162.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 137.91\n",
      "    agent-1: 133.61\n",
      "  policy_reward_min:\n",
      "    agent-0: 114.0\n",
      "    agent-1: 94.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1226138399536184\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1376974798474837\n",
      "    mean_inference_ms: 7.347777051144935\n",
      "    mean_raw_obs_processing_ms: 0.20140405060040276\n",
      "  time_since_restore: 2475.6524443626404\n",
      "  time_this_iter_s: 255.1674633026123\n",
      "  time_total_s: 2475.6524443626404\n",
      "  timers:\n",
      "    learn_throughput: 35.312\n",
      "    learn_time_ms: 226492.125\n",
      "    sample_throughput: 379.98\n",
      "    sample_time_ms: 21048.483\n",
      "    update_time_ms: 6.214\n",
      "  timestamp: 1627341636\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79980\n",
      "  training_iteration: 10\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         2475.65</td><td style=\"text-align: right;\">79980</td><td style=\"text-align: right;\">  271.52</td><td style=\"text-align: right;\">                 318</td><td style=\"text-align: right;\">                 243</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 175956\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_11-25-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 296.0\n",
      "  episode_reward_mean: 261.0\n",
      "  episode_reward_min: 227.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 879\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.002343750000000001\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.33420158757103813\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0008314444835016889\n",
      "          policy_loss: 0.0002732199306289355\n",
      "          total_loss: 563.1373416961186\n",
      "          vf_explained_var: 2.838316470743507e-09\n",
      "          vf_loss: 563.1370670379155\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.006249999999999997\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.2776357190949576\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0016989135944770947\n",
      "          policy_loss: -0.0004925886353862191\n",
      "          total_loss: 642.3555355980283\n",
      "          vf_explained_var: 2.838316470743507e-09\n",
      "          vf_loss: 642.3560006277902\n",
      "    num_agent_steps_sampled: 175956\n",
      "    num_agent_steps_trained: 175956\n",
      "    num_steps_sampled: 87978\n",
      "    num_steps_trained: 87978\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 30.62124352331606\n",
      "    ram_util_percent: 92.33238341968912\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 163.0\n",
      "    agent-1: 160.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 126.4\n",
      "    agent-1: 134.6\n",
      "  policy_reward_min:\n",
      "    agent-0: 96.0\n",
      "    agent-1: 112.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12229045041882049\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13726666251131725\n",
      "    mean_inference_ms: 7.330815619250282\n",
      "    mean_raw_obs_processing_ms: 0.20079153999080282\n",
      "  time_since_restore: 2745.8193974494934\n",
      "  time_this_iter_s: 270.166953086853\n",
      "  time_total_s: 2745.8193974494934\n",
      "  timers:\n",
      "    learn_throughput: 34.534\n",
      "    learn_time_ms: 231598.42\n",
      "    sample_throughput: 376.69\n",
      "    sample_time_ms: 21232.331\n",
      "    update_time_ms: 6.117\n",
      "  timestamp: 1627341906\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87978\n",
      "  training_iteration: 11\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">         2745.82</td><td style=\"text-align: right;\">87978</td><td style=\"text-align: right;\">     261</td><td style=\"text-align: right;\">                 296</td><td style=\"text-align: right;\">                 227</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 191952\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_11-29-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 291.0\n",
      "  episode_reward_mean: 250.6\n",
      "  episode_reward_min: 221.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 957\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0011718750000000006\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.24454979669480098\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00340556198849328\n",
      "          policy_loss: 0.0004153317284016382\n",
      "          total_loss: 496.73692103794644\n",
      "          vf_explained_var: 2.838316470743507e-09\n",
      "          vf_loss: 496.7365010579427\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0031249999999999984\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.22398647902503846\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0029082134863399134\n",
      "          policy_loss: -0.002556703718645232\n",
      "          total_loss: 611.5494152250744\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 611.5519607786148\n",
      "    num_agent_steps_sampled: 191952\n",
      "    num_agent_steps_trained: 191952\n",
      "    num_steps_sampled: 95976\n",
      "    num_steps_trained: 95976\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 34.467331670822944\n",
      "    ram_util_percent: 92.68728179551123\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 154.0\n",
      "    agent-1: 160.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 119.9\n",
      "    agent-1: 130.7\n",
      "  policy_reward_min:\n",
      "    agent-0: 96.0\n",
      "    agent-1: 97.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12223796942728636\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13747083821394712\n",
      "    mean_inference_ms: 7.329398091548373\n",
      "    mean_raw_obs_processing_ms: 0.20069476944618983\n",
      "  time_since_restore: 3027.426348924637\n",
      "  time_this_iter_s: 281.60695147514343\n",
      "  time_total_s: 3027.426348924637\n",
      "  timers:\n",
      "    learn_throughput: 33.739\n",
      "    learn_time_ms: 237054.78\n",
      "    sample_throughput: 377.642\n",
      "    sample_time_ms: 21178.813\n",
      "    update_time_ms: 6.546\n",
      "  timestamp: 1627342188\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95976\n",
      "  training_iteration: 12\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         3027.43</td><td style=\"text-align: right;\">95976</td><td style=\"text-align: right;\">   250.6</td><td style=\"text-align: right;\">                 291</td><td style=\"text-align: right;\">                 221</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 207948\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_11-34-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 273.0\n",
      "  episode_reward_mean: 236.03\n",
      "  episode_reward_min: 215.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 1038\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0005859375000000003\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.19134178521141174\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002215550879814795\n",
      "          policy_loss: -0.0005817777506770596\n",
      "          total_loss: 438.0082760765439\n",
      "          vf_explained_var: 3.7844221090210795e-09\n",
      "          vf_loss: 438.0088563949343\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0015624999999999992\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.16203989277756403\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002407418956632711\n",
      "          policy_loss: -0.00033466753342913254\n",
      "          total_loss: 502.8490726531498\n",
      "          vf_explained_var: 2.838316470743507e-09\n",
      "          vf_loss: 502.8494030180432\n",
      "    num_agent_steps_sampled: 207948\n",
      "    num_agent_steps_trained: 207948\n",
      "    num_steps_sampled: 103974\n",
      "    num_steps_trained: 103974\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.13797468354431\n",
      "    ram_util_percent: 94.32455696202533\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 141.0\n",
      "    agent-1: 153.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 113.84\n",
      "    agent-1: 122.19\n",
      "  policy_reward_min:\n",
      "    agent-0: 97.0\n",
      "    agent-1: 103.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12319911644540037\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13811062598134236\n",
      "    mean_inference_ms: 7.388148139551979\n",
      "    mean_raw_obs_processing_ms: 0.20211951690012162\n",
      "  time_since_restore: 3304.0278210639954\n",
      "  time_this_iter_s: 276.6014721393585\n",
      "  time_total_s: 3304.0278210639954\n",
      "  timers:\n",
      "    learn_throughput: 32.953\n",
      "    learn_time_ms: 242712.932\n",
      "    sample_throughput: 374.897\n",
      "    sample_time_ms: 21333.856\n",
      "    update_time_ms: 6.937\n",
      "  timestamp: 1627342464\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103974\n",
      "  training_iteration: 13\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         3304.03</td><td style=\"text-align: right;\">103974</td><td style=\"text-align: right;\">  236.03</td><td style=\"text-align: right;\">                 273</td><td style=\"text-align: right;\">                 215</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 223944\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_11-38-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 258.0\n",
      "  episode_reward_mean: 227.56\n",
      "  episode_reward_min: 209.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 1119\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.00029296875000000015\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.14984841479195488\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0019087931658658716\n",
      "          policy_loss: -0.0024501881192600917\n",
      "          total_loss: 441.38036189003594\n",
      "          vf_explained_var: 1.8922110545105397e-09\n",
      "          vf_loss: 441.38280620272195\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0007812499999999996\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.14970713238867503\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.001729080897608323\n",
      "          policy_loss: -0.0015473630841052722\n",
      "          total_loss: 467.5741751534598\n",
      "          vf_explained_var: 7.568844218042159e-09\n",
      "          vf_loss: 467.5757281591022\n",
      "    num_agent_steps_sampled: 223944\n",
      "    num_agent_steps_trained: 223944\n",
      "    num_steps_sampled: 111972\n",
      "    num_steps_trained: 111972\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.735978835978838\n",
      "    ram_util_percent: 94.10608465608468\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 139.0\n",
      "    agent-1: 144.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 110.98\n",
      "    agent-1: 116.58\n",
      "  policy_reward_min:\n",
      "    agent-0: 97.0\n",
      "    agent-1: 98.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12399336562892238\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13856165642688223\n",
      "    mean_inference_ms: 7.437967164286339\n",
      "    mean_raw_obs_processing_ms: 0.20338934023381178\n",
      "  time_since_restore: 3569.145492553711\n",
      "  time_this_iter_s: 265.1176714897156\n",
      "  time_total_s: 3569.145492553711\n",
      "  timers:\n",
      "    learn_throughput: 32.377\n",
      "    learn_time_ms: 247024.688\n",
      "    sample_throughput: 368.524\n",
      "    sample_time_ms: 21702.796\n",
      "    update_time_ms: 6.931\n",
      "  timestamp: 1627342729\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111972\n",
      "  training_iteration: 14\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         3569.15</td><td style=\"text-align: right;\">111972</td><td style=\"text-align: right;\">  227.56</td><td style=\"text-align: right;\">                 258</td><td style=\"text-align: right;\">                 209</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 239940\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_11-43-05\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 242.0\n",
      "  episode_reward_mean: 222.77\n",
      "  episode_reward_min: 206.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 1197\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.00014648437500000008\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.14873459048214413\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0008780278390437911\n",
      "          policy_loss: 0.0006045169670075651\n",
      "          total_loss: 415.55897110227556\n",
      "          vf_explained_var: 4.730527525254047e-09\n",
      "          vf_loss: 415.55836898561506\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0003906249999999998\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.15741729121359568\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00039688505629393537\n",
      "          policy_loss: 5.186461503543551e-05\n",
      "          total_loss: 441.68660336449034\n",
      "          vf_explained_var: 6.622738357719982e-09\n",
      "          vf_loss: 441.68655589270213\n",
      "    num_agent_steps_sampled: 239940\n",
      "    num_agent_steps_trained: 239940\n",
      "    num_steps_sampled: 119970\n",
      "    num_steps_trained: 119970\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.987362637362637\n",
      "    ram_util_percent: 94.13543956043956\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 124.0\n",
      "    agent-1: 133.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 109.11\n",
      "    agent-1: 113.66\n",
      "  policy_reward_min:\n",
      "    agent-0: 92.0\n",
      "    agent-1: 95.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1240547810127712\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13866175096152816\n",
      "    mean_inference_ms: 7.445436152941052\n",
      "    mean_raw_obs_processing_ms: 0.20354696344211667\n",
      "  time_since_restore: 3824.16246342659\n",
      "  time_this_iter_s: 255.01697087287903\n",
      "  time_total_s: 3824.16246342659\n",
      "  timers:\n",
      "    learn_throughput: 32.069\n",
      "    learn_time_ms: 249397.544\n",
      "    sample_throughput: 367.169\n",
      "    sample_time_ms: 21782.866\n",
      "    update_time_ms: 6.958\n",
      "  timestamp: 1627342985\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119970\n",
      "  training_iteration: 15\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         3824.16</td><td style=\"text-align: right;\">119970</td><td style=\"text-align: right;\">  222.77</td><td style=\"text-align: right;\">                 242</td><td style=\"text-align: right;\">                 206</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 255936\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_11-47-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 239.0\n",
      "  episode_reward_mean: 220.57\n",
      "  episode_reward_min: 206.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 1278\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 7.324218750000004e-05\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.12838779709168843\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.001104504105711161\n",
      "          policy_loss: -0.0017437147981827221\n",
      "          total_loss: 398.268062531002\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 398.26980978345114\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.0001953124999999999\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.14684294968370407\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00030121182862785645\n",
      "          policy_loss: 0.0010778458700293587\n",
      "          total_loss: 410.9432600717696\n",
      "          vf_explained_var: -1.8922110545105397e-09\n",
      "          vf_loss: 410.94217500232514\n",
      "    num_agent_steps_sampled: 255936\n",
      "    num_agent_steps_trained: 255936\n",
      "    num_steps_sampled: 127968\n",
      "    num_steps_trained: 127968\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.060638297872345\n",
      "    ram_util_percent: 94.08563829787234\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 128.0\n",
      "    agent-1: 127.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 109.56\n",
      "    agent-1: 111.01\n",
      "  policy_reward_min:\n",
      "    agent-0: 97.0\n",
      "    agent-1: 94.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12397179237312533\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13889583332616307\n",
      "    mean_inference_ms: 7.445176467907153\n",
      "    mean_raw_obs_processing_ms: 0.20349635305221228\n",
      "  time_since_restore: 4087.241439342499\n",
      "  time_this_iter_s: 263.0789759159088\n",
      "  time_total_s: 4087.241439342499\n",
      "  timers:\n",
      "    learn_throughput: 32.026\n",
      "    learn_time_ms: 249735.525\n",
      "    sample_throughput: 364.457\n",
      "    sample_time_ms: 21944.996\n",
      "    update_time_ms: 7.041\n",
      "  timestamp: 1627343248\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127968\n",
      "  training_iteration: 16\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         4087.24</td><td style=\"text-align: right;\">127968</td><td style=\"text-align: right;\">  220.57</td><td style=\"text-align: right;\">                 239</td><td style=\"text-align: right;\">                 206</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 271932\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_11-51-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 233.0\n",
      "  episode_reward_mean: 216.04\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 1359\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 3.662109375000002e-05\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.0950216409705934\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0010359105035026986\n",
      "          policy_loss: -0.0008176186580270056\n",
      "          total_loss: 391.35582333519346\n",
      "          vf_explained_var: 4.730527525254047e-09\n",
      "          vf_loss: 391.3566371372768\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.765624999999995e-05\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.08576876977606425\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0012905675333939374\n",
      "          policy_loss: -8.776902206360348e-06\n",
      "          total_loss: 405.25655449761285\n",
      "          vf_explained_var: 1.8922110545105397e-09\n",
      "          vf_loss: 405.2565651545449\n",
      "    num_agent_steps_sampled: 271932\n",
      "    num_agent_steps_trained: 271932\n",
      "    num_steps_sampled: 135966\n",
      "    num_steps_trained: 135966\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 28.80215053763441\n",
      "    ram_util_percent: 94.2395161290323\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 126.0\n",
      "    agent-1: 128.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 107.47\n",
      "    agent-1: 108.57\n",
      "  policy_reward_min:\n",
      "    agent-0: 93.0\n",
      "    agent-1: 95.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1236901686995432\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13854968155480804\n",
      "    mean_inference_ms: 7.42920654572302\n",
      "    mean_raw_obs_processing_ms: 0.20315772967983448\n",
      "  time_since_restore: 4348.388349533081\n",
      "  time_this_iter_s: 261.1469101905823\n",
      "  time_total_s: 4348.388349533081\n",
      "  timers:\n",
      "    learn_throughput: 32.323\n",
      "    learn_time_ms: 247440.841\n",
      "    sample_throughput: 363.246\n",
      "    sample_time_ms: 22018.136\n",
      "    update_time_ms: 6.822\n",
      "  timestamp: 1627343509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135966\n",
      "  training_iteration: 17\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         4348.39</td><td style=\"text-align: right;\">135966</td><td style=\"text-align: right;\">  216.04</td><td style=\"text-align: right;\">                 233</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 287928\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_11-56-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 230.0\n",
      "  episode_reward_mean: 211.77\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 1437\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.831054687500001e-05\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.08146685184467406\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0003159406197872678\n",
      "          policy_loss: 0.0005013222643543803\n",
      "          total_loss: 378.42431010897195\n",
      "          vf_explained_var: 9.461055272552699e-10\n",
      "          vf_loss: 378.42380584232393\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.8828124999999976e-05\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.07408987387778267\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00045088583502792825\n",
      "          policy_loss: 0.0004587604235562067\n",
      "          total_loss: 387.28077625093005\n",
      "          vf_explained_var: 5.676632941487014e-09\n",
      "          vf_loss: 387.2803180028522\n",
      "    num_agent_steps_sampled: 287928\n",
      "    num_agent_steps_trained: 287928\n",
      "    num_steps_sampled: 143964\n",
      "    num_steps_trained: 143964\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 32.63316326530612\n",
      "    ram_util_percent: 95.17602040816327\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 127.0\n",
      "    agent-1: 126.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 106.11\n",
      "    agent-1: 105.66\n",
      "  policy_reward_min:\n",
      "    agent-0: 96.0\n",
      "    agent-1: 94.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.124183467511084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13904130083798324\n",
      "    mean_inference_ms: 7.464073232400306\n",
      "    mean_raw_obs_processing_ms: 0.20427267617194353\n",
      "  time_since_restore: 4622.853736877441\n",
      "  time_this_iter_s: 274.46538734436035\n",
      "  time_total_s: 4622.853736877441\n",
      "  timers:\n",
      "    learn_throughput: 32.454\n",
      "    learn_time_ms: 246442.116\n",
      "    sample_throughput: 363.138\n",
      "    sample_time_ms: 22024.684\n",
      "    update_time_ms: 6.92\n",
      "  timestamp: 1627343783\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 143964\n",
      "  training_iteration: 18\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">         4622.85</td><td style=\"text-align: right;\">143964</td><td style=\"text-align: right;\">  211.77</td><td style=\"text-align: right;\">                 230</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 303924\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_12-00-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 228.0\n",
      "  episode_reward_mean: 210.07\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 1518\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 9.155273437500005e-06\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.0736085135075781\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0008138585941712754\n",
      "          policy_loss: 0.0008497075990788521\n",
      "          total_loss: 353.0207282172309\n",
      "          vf_explained_var: 2.838316470743507e-09\n",
      "          vf_loss: 353.0198814755394\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 2.4414062499999988e-05\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.06269826113231598\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00038935402722378807\n",
      "          policy_loss: 0.0003157841071249947\n",
      "          total_loss: 375.061770484561\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 375.0614522298177\n",
      "    num_agent_steps_sampled: 303924\n",
      "    num_agent_steps_trained: 303924\n",
      "    num_steps_sampled: 151962\n",
      "    num_steps_trained: 151962\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 27.677513227513227\n",
      "    ram_util_percent: 95.06904761904761\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 124.0\n",
      "    agent-1: 126.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 103.41\n",
      "    agent-1: 106.66\n",
      "  policy_reward_min:\n",
      "    agent-0: 95.0\n",
      "    agent-1: 94.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12415293413663404\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13906997331524856\n",
      "    mean_inference_ms: 7.462743898828515\n",
      "    mean_raw_obs_processing_ms: 0.2041762212396172\n",
      "  time_since_restore: 4887.6940751075745\n",
      "  time_this_iter_s: 264.84033823013306\n",
      "  time_total_s: 4887.6940751075745\n",
      "  timers:\n",
      "    learn_throughput: 32.639\n",
      "    learn_time_ms: 245043.765\n",
      "    sample_throughput: 369.411\n",
      "    sample_time_ms: 21650.688\n",
      "    update_time_ms: 6.904\n",
      "  timestamp: 1627344048\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151962\n",
      "  training_iteration: 19\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         4887.69</td><td style=\"text-align: right;\">151962</td><td style=\"text-align: right;\">  210.07</td><td style=\"text-align: right;\">                 228</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_4db32_00000:\n",
      "  agent_timesteps_total: 319920\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-27_12-05-17\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 224.0\n",
      "  episode_reward_mean: 206.51\n",
      "  episode_reward_min: 200.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 1599\n",
      "  experiment_id: 3f8acaa62e554dc3a3d95c19c2898547\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 4.577636718750002e-06\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.07081073487088793\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00016902480977492698\n",
      "          policy_loss: 0.0011319992176833608\n",
      "          total_loss: 355.19950309632316\n",
      "          vf_explained_var: 3.7844221090210795e-09\n",
      "          vf_loss: 355.19837055509055\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 1.2207031249999994e-05\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.04088118601413\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00028936283608403294\n",
      "          policy_loss: -0.0006290687144630485\n",
      "          total_loss: 360.64540124317955\n",
      "          vf_explained_var: 2.838316470743507e-09\n",
      "          vf_loss: 360.64603000217016\n",
      "    num_agent_steps_sampled: 319920\n",
      "    num_agent_steps_trained: 319920\n",
      "    num_steps_sampled: 159960\n",
      "    num_steps_trained: 159960\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 26.9177545691906\n",
      "    ram_util_percent: 94.98590078328984\n",
      "  pid: 105126\n",
      "  policy_reward_max:\n",
      "    agent-0: 116.0\n",
      "    agent-1: 119.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 102.58\n",
      "    agent-1: 103.93\n",
      "  policy_reward_min:\n",
      "    agent-0: 96.0\n",
      "    agent-1: 96.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.12389794798650712\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.13885546961067496\n",
      "    mean_inference_ms: 7.446362307987601\n",
      "    mean_raw_obs_processing_ms: 0.20367366854087635\n",
      "  time_since_restore: 5156.180612325668\n",
      "  time_this_iter_s: 268.4865372180939\n",
      "  time_total_s: 5156.180612325668\n",
      "  timers:\n",
      "    learn_throughput: 32.463\n",
      "    learn_time_ms: 246369.47\n",
      "    sample_throughput: 369.306\n",
      "    sample_time_ms: 21656.853\n",
      "    update_time_ms: 6.856\n",
      "  timestamp: 1627344317\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159960\n",
      "  training_iteration: 20\n",
      "  trial_id: 4db32_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>RUNNING </td><td>192.168.1.21:105126</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5156.18</td><td style=\"text-align: right;\">159960</td><td style=\"text-align: right;\">  206.51</td><td style=\"text-align: right;\">                 224</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/1.31 GiB heap, 0.0/0.66 GiB objects (0.0/1.0 CPU_group_2_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_3_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/4.0 CPU_group_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_0_655c2beea4de92de5c27ac8feb0e5d9f, 0.0/1.0 CPU_group_1_655c2beea4de92de5c27ac8feb0e5d9f)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_4db32_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         5156.18</td><td style=\"text-align: right;\">159960</td><td style=\"text-align: right;\">  206.51</td><td style=\"text-align: right;\">                 224</td><td style=\"text-align: right;\">                 200</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-27 12:05:17,970\tINFO tune.py:549 -- Total run time: 5163.70 seconds (5163.13 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f2cc2dee0a0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'TA_TEST1'\n",
    "exp_dict = {\n",
    "        'name': exp_name,\n",
    "        'run_or_experiment': 'PPO',\n",
    "        \"stop\": {\n",
    "            \"training_iteration\": 20\n",
    "        },\n",
    "        'checkpoint_freq': 20,\n",
    "        \"config\": config,\n",
    "}\n",
    "# ray.init()\n",
    "tune.run(**exp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
