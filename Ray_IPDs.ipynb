{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "from gym import spaces\n",
    "\n",
    "# import axelrod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/.local/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n",
      "2021-07-30 20:45:51,819\tINFO services.py:1272 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.21',\n",
       " 'raylet_ip_address': '192.168.1.21',\n",
       " 'redis_address': '192.168.1.21:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-07-30_20-45-50_536289_66576/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-07-30_20-45-50_536289_66576/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-07-30_20-45-50_536289_66576',\n",
       " 'metrics_export_port': 52646,\n",
       " 'node_id': '4ffe0332e659415059b4a5ff4ede5793585f092b9628baa1b4c5e050'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.ppo import DEFAULT_CONFIG as DEFAULT_CONFIG_PPO\n",
    "\n",
    "from ray.rllib.agents.dqn import DQNTrainer, DEFAULT_CONFIG \n",
    "from ray.rllib.agents.dqn import  DEFAULT_CONFIG as DEFAULT_CONFIG_DQN\n",
    "\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "from ray import tune\n",
    "\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 0,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'create_env_on_driver': False,\n",
       " 'rollout_fragment_length': 4,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'train_batch_size': 32,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  'fcnet_hiddens': [256, 256],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': True,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'num_framestacks': 'auto',\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1,\n",
       "  'framestack': True},\n",
       " 'optimizer': {},\n",
       " 'gamma': 0.99,\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env': None,\n",
       " 'env_config': {},\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'record_env': False,\n",
       " 'normalize_actions': False,\n",
       " 'clip_rewards': None,\n",
       " 'clip_actions': True,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'lr': 0.0005,\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'tf',\n",
       " 'eager_tracing': False,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'EpsilonGreedy',\n",
       "  'initial_epsilon': 1.0,\n",
       "  'final_epsilon': 0.02,\n",
       "  'epsilon_timesteps': 10000},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_num_episodes': 10,\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {'explore': False},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'sample_async': False,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'collect_metrics_timeout': 180,\n",
       " 'metrics_smoothing_episodes': 100,\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'min_iter_time_s': 1,\n",
       " 'timesteps_per_iteration': 1000,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 0,\n",
       " '_fake_gpus': False,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'placement_strategy': 'PACK',\n",
       " 'input': 'sampler',\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent',\n",
       "  'count_steps_by': 'env_steps'},\n",
       " 'logger_config': None,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'num_atoms': 1,\n",
       " 'v_min': -10.0,\n",
       " 'v_max': 10.0,\n",
       " 'noisy': False,\n",
       " 'sigma0': 0.5,\n",
       " 'dueling': True,\n",
       " 'hiddens': [256],\n",
       " 'double_q': True,\n",
       " 'n_step': 1,\n",
       " 'target_network_update_freq': 500,\n",
       " 'buffer_size': 50000,\n",
       " 'replay_sequence_length': 1,\n",
       " 'prioritized_replay': True,\n",
       " 'prioritized_replay_alpha': 0.6,\n",
       " 'prioritized_replay_beta': 0.4,\n",
       " 'final_prioritized_replay_beta': 0.4,\n",
       " 'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       " 'prioritized_replay_eps': 1e-06,\n",
       " 'before_learn_on_batch': None,\n",
       " 'training_intensity': None,\n",
       " 'lr_schedule': None,\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'grad_clip': 40,\n",
       " 'learning_starts': 1000,\n",
       " 'worker_side_prioritization': False}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_CONFIG_DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixGame():\n",
    "    \n",
    "    def __init__(self, RPST=(3,1,0,5)):\n",
    "        \n",
    "        self.RPST = RPST\n",
    "        \n",
    "        self.payoff_mat = np.empty((2,2), dtype=np.object)\n",
    "        \n",
    "        self.payoff_mat[0, 0] = (RPST[0], RPST[0])\n",
    "        self.payoff_mat[1, 1] = (RPST[1], RPST[1])\n",
    "        self.payoff_mat[0, 1] = (RPST[2], RPST[3])\n",
    "        self.payoff_mat[1, 0] = (RPST[3], RPST[2])\n",
    "        \n",
    "    def play(self, a_row, a_col):\n",
    "        # for ease of things 0 is coooperate\n",
    "#                            1 is defect\n",
    "        \n",
    "        \n",
    "#         if a_row == 'c':\n",
    "#             row = 0\n",
    "#         else:\n",
    "#             row = 1\n",
    "            \n",
    "#         if a_col == 'c':\n",
    "#             col = 0\n",
    "#         else:\n",
    "#             col = 1\n",
    "            \n",
    "        return self.payoff_mat[a_row, a_col]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitForTatPlayer():\n",
    "    \"\"\"Starts by cooperating, and then always plays the opponents previous move\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.count = 0\n",
    "        self.opponents_move = 0\n",
    "        \n",
    "    def play(self):\n",
    "        \n",
    "        return self.opponents_move\n",
    "    \n",
    "    def update(self, opponents_move):\n",
    "        self.opponents_move = opponents_move\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.count = 0\n",
    "        self.opponents_move = 0\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitForTatThenDefectPlayer():\n",
    "    \"\"\"Starts by cooperating, and then always plays the opponents previous move\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_defect_turn=50):\n",
    "        \n",
    "        self.count = 0\n",
    "        self.opponents_move = 0\n",
    "        self.n_defect_turn = n_defect_turn\n",
    "        \n",
    "    def play(self):\n",
    "        \n",
    "        if self.count < self.n_defect_turn:\n",
    "            return self.opponents_move\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "    def update(self, opponents_move):\n",
    "        \n",
    "        self.opponents_move = opponents_move\n",
    "        self.count += 1\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.count = 0\n",
    "        self.opponents_move = 0\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(2, dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitForTatOrRandom():\n",
    "    \"\"\"Starts by cooperating, and then always plays the opponents previous move\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.count = 0\n",
    "        self.opponents_move = 0\n",
    "        self.is_random = np.random.randint(2, dtype=np.bool)\n",
    "        \n",
    "    def play(self):\n",
    "        \n",
    "        if self.is_random:\n",
    "            return np.random.randint(2)\n",
    "        else:\n",
    "            return self.opponents_move\n",
    "    \n",
    "    def update(self, opponents_move):\n",
    "        \n",
    "        self.opponents_move = opponents_move\n",
    "        self.count += 1\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.count = 0\n",
    "        self.opponents_move = 0\n",
    "        self.is_random = np.random.randint(2, dtype=np.bool)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitForTatOrDefect():\n",
    "    \"\"\"Starts by cooperating, and then always plays the opponents previous move\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.count = 0\n",
    "        self.opponents_move = 0\n",
    "        self.is_random = np.random.randint(2, dtype=np.bool)\n",
    "        \n",
    "    def play(self):\n",
    "        \n",
    "        if self.is_random:\n",
    "            return 1\n",
    "        else:\n",
    "            return self.opponents_move\n",
    "    \n",
    "    def update(self, opponents_move):\n",
    "        \n",
    "        self.opponents_move = opponents_move\n",
    "        self.count += 1\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.count = 0\n",
    "        self.opponents_move = 0\n",
    "        self.is_random = np.random.randint(2, dtype=np.bool)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TitForTatThenDefect():\n",
    "    \"\"\"Starts by cooperating, and then always plays the opponents previous move\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, min_defect_turn = 50, max_defect_turn=100):\n",
    "        \n",
    "        self.count = 0\n",
    "        self.opponents_move = 0\n",
    "        self.is_random = np.random.randint(2, dtype=np.bool)\n",
    "        self.min_defect_turn = min_defect_turn\n",
    "        self.max_defect_turn = max_defect_turn\n",
    "        \n",
    "        self.defect_turn = np.random.randint(low=self.min_defect_turn, high=self.max_defect_turn)\n",
    "        \n",
    "    def play(self):\n",
    "        \n",
    "        if self.count > self.defect_turn:\n",
    "            return 1\n",
    "        else:\n",
    "            return self.opponents_move\n",
    "    \n",
    "    def update(self, opponents_move):\n",
    "        \n",
    "        self.opponents_move = opponents_move\n",
    "        self.count += 1\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.count = 0\n",
    "        self.opponents_move = 0\n",
    "        self.is_random = np.random.randint(2, dtype=np.bool)\n",
    "        self.defect_turn = np.random.randint(low=self.min_defect_turn, high=self.max_defect_turn)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "2\n",
      "4\n",
      "4\n",
      "2\n",
      "4\n",
      "2\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "2\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for ii in range(20):\n",
    "    print(np.random.randint(low=2,high=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GrudgePlayer():\n",
    "    \"\"\"Starts by cooperating, but defects forever if opponent defects\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.count = 0\n",
    "#         self.opponents_move = 0\n",
    "        self.grudging = False # has the opponent played defect\n",
    "        \n",
    "    def play(self):\n",
    "        \n",
    "        if self.grudging:\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def update(self, opponents_move):\n",
    "#         self.opponents_move = opponents_move\n",
    "        if opponents_move == 1:\n",
    "            self.grudging = True\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.count = 0\n",
    "        self.grudging = False  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer():\n",
    "    \"\"\"Cooperates a with a fixed probability\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, P_coop=0.5):\n",
    "        \n",
    "        self.count = 0\n",
    "        self.P_coop = P_coop\n",
    "        \n",
    "        assert self.P_coop <= 1.\n",
    "    \n",
    "        \n",
    "    def play(self):\n",
    "        \n",
    "        if np.random.rand() < self.P_coop:\n",
    "            return 0 \n",
    "        \n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    \n",
    "    def update(self, opponents_move):\n",
    "        pass\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomProbPlayer():\n",
    "    \"\"\"Cooperates a with a probability P_coop,\n",
    "    P_coop is chosen randomly each round\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.count = 0\n",
    "        self.P_coop = np.random.rand()\n",
    "            \n",
    "        \n",
    "    def play(self):\n",
    "        \n",
    "        if np.random.rand() < self.P_coop:\n",
    "            return 0 \n",
    "        \n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    \n",
    "    def update(self, opponents_move):\n",
    "        pass\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.count = 0\n",
    "        self.P_coop = np.random.rand()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CooperateOrDefectPlayer():\n",
    "    \"\"\"Either always cooperates or always defects each round\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.count = 0\n",
    "        self.P_coop = np.random.randint(2)\n",
    "            \n",
    "        \n",
    "    def play(self):\n",
    "        \n",
    "        if np.random.rand() < self.P_coop:\n",
    "            return 0 \n",
    "        \n",
    "        else:\n",
    "            return 1\n",
    "        \n",
    "    \n",
    "    def update(self, opponents_move):\n",
    "        pass\n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.count = 0\n",
    "        self.P_coop = np.random.randint(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CooperatePlayer(RandomPlayer):\n",
    "    \"\"\" Always cooperates\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__(P_coop=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DefectPlayer(RandomPlayer):\n",
    "    \"\"\"Always defects\n",
    "    \"\"\"\n",
    "     \n",
    "    def __init__(self):\n",
    "        super().__init__(P_coop=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixGameEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self,  RPST=(3,1,0,5), history_n=100, player2=TitForTatPlayer()):\n",
    "        \n",
    "        self.RPST = RPST\n",
    "        self.history_n = history_n\n",
    "        self.history = np.zeros((2,2,self.history_n))\n",
    "        self._counter = 0\n",
    "        self._setup_spaces()\n",
    "#         self.player2 = TitForTatPlayer()\n",
    "#         self.player2 = GrudgePlayer()\n",
    "#         self.player2 = DefectPlayer()\n",
    "#         self.player2 = CooperateOrDefectPlayer()\n",
    "#         self.player2 = TitForTatThenDefectPlayer()\n",
    "#         self.player2 = TitForTatOrDefect()\n",
    "        self.player2 = player2\n",
    "        self.game = MatrixGame(RPST=(3,1,0,5))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _setup_spaces(self):\n",
    "        \n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        self.observation_space = spaces.Box(0, 1,\n",
    "                                           shape=(self.history_n * 4,))\n",
    "        \n",
    "    def history_to_state(self, history=None):\n",
    "        \n",
    "        if history is None:\n",
    "            history = self.history\n",
    "            \n",
    "        state = history.flatten()\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        assert self.action_space.contains(action)\n",
    "        \n",
    "        action2 = self.player2.play()\n",
    "        rewards = self.game.play(action, action2)\n",
    "        \n",
    "        self.player2.update(action)\n",
    "        self.history[0, action, self._counter] = 1\n",
    "        self.history[1, action2, self._counter] = 1\n",
    "        \n",
    "        self.state = self.history_to_state(self.history)\n",
    "        \n",
    "        self._counter += 1\n",
    "        done = self._counter >= self.history_n \n",
    "        \n",
    "        return self.state, rewards[0], done, {}\n",
    "\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "\n",
    "        self.history = np.zeros((2,2,self.history_n))\n",
    "        self.state = self.history.flatten()\n",
    "\n",
    "        self._counter = 0\n",
    "        self.player2.reset()\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixGameRollingHistoryEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self,  RPST=(3,1,0,5), history_n=10, game_length=100,player2=TitForTatPlayer()):\n",
    "        \n",
    "        self.RPST = RPST\n",
    "        self.history_n = history_n\n",
    "        self.history = np.zeros((2,2,self.history_n))\n",
    "        self._counter = 0\n",
    "        self._setup_spaces()\n",
    "#         self.player2 = TitForTatPlayer()\n",
    "#         self.player2 = GrudgePlayer()\n",
    "#         self.player2 = DefectPlayer()\n",
    "#         self.player2 = CooperateOrDefectPlayer()\n",
    "#         self.player2 = TitForTatThenDefectPlayer()\n",
    "#         self.player2 = TitForTatOrDefect()\n",
    "        self.player2 = player2\n",
    "        self.game = MatrixGame(RPST=(3,1,0,5))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _setup_spaces(self):\n",
    "        \n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        self.observation_space = spaces.Box(0, 1,\n",
    "                                           shape=(self.history_n * 4,))\n",
    "        \n",
    "    def history_to_state(self, history=None):\n",
    "        \n",
    "        if history is None:\n",
    "            history = self.history\n",
    "            \n",
    "        state = history.flatten()\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        assert self.action_space.contains(action)\n",
    "        \n",
    "        action2 = self.player2.play()\n",
    "        rewards = self.game.play(action, action2)\n",
    "        \n",
    "        self.player2.update(action)\n",
    "        self.history[0, action, self._counter] = 1\n",
    "        self.history[1, action2, self._counter] = 1\n",
    "        \n",
    "        self.state = self.history_to_state(self.history)\n",
    "        \n",
    "        self._counter += 1\n",
    "        done = self._counter >= self.history_n \n",
    "        \n",
    "        return self.state, rewards[0], done, {}\n",
    "\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "\n",
    "        self.history = np.zeros((2,2,self.history_n))\n",
    "        self.state = self.history.flatten()\n",
    "\n",
    "        self._counter = 0\n",
    "        self.player2.reset()\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env('MG_env', lambda c: MatrixGameEnv(player2=TitForTatOrRandom()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixGameEnv_no_history(gym.Env):\n",
    "    \n",
    "    def __init__(self,  RPST=(3,1,0,5), n_games=100, player2=TitForTatPlayer()):\n",
    "        \n",
    "        self.RPST = RPST\n",
    "        self.last_moves = np.zeros((2,2))\n",
    "        self._counter = 0\n",
    "        self._setup_spaces()\n",
    "        self.player2 = TitForTatPlayer()\n",
    "#         self.player2 = GrudgePlayer()\n",
    "#         self.player2 = DefectPlayer()\n",
    "#         self.player2 = CooperateOrDefectPlayer()\n",
    "    #         self.player2 = TitForTatThenDefectPlayer()\n",
    "#         self.player2 = TitForTatOrRandom()\n",
    "        self.player2 = player2\n",
    "        self.n_games = n_games\n",
    "        self.game = MatrixGame(RPST=(3,1,0,5))\n",
    "        \n",
    "        \n",
    "        \n",
    "    def _setup_spaces(self):\n",
    "        \n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        self.observation_space = spaces.Box(0, 1,\n",
    "                                           shape=(4,))\n",
    "        \n",
    "    def lastmoves_to_state(self, last_moves=None):\n",
    "        \n",
    "        if last_moves is None:\n",
    "            last_moves = self.last_moves\n",
    "            \n",
    "        state = last_moves.flatten()\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        \n",
    "        assert self.action_space.contains(action)\n",
    "        \n",
    "        action2 = self.player2.play()\n",
    "        rewards = self.game.play(action, action2)\n",
    "        \n",
    "        self.player2.update(action)\n",
    "        self.last_moves = np.zeros((2,2))\n",
    "\n",
    "        self.last_moves[0, action] = 1\n",
    "        self.last_moves[1, action2] = 1\n",
    "        \n",
    "        self.state = self.lastmoves_to_state(self.last_moves)\n",
    "        \n",
    "        self._counter += 1\n",
    "        done = self._counter >= self.n_games\n",
    "        \n",
    "        return self.state, rewards[0], done, {}\n",
    "\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "\n",
    "        self.last_moves = np.zeros((2,2))\n",
    "        self.state = self.last_moves.flatten()\n",
    "\n",
    "        self._counter = 0\n",
    "        self.player2.reset()\n",
    "        \n",
    "        return self.state\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env('MG_env', lambda c: MatrixGameEnv(player2=TitForTatOrDefect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_dqn = DEFAULT_CONFIG_DQN.copy()\n",
    "trainer_config_dqn['num_workers'] = 3\n",
    "trainer_config_dqn['n_step'] = 3\n",
    "trainer_config_dqn['noisy'] = True\n",
    "trainer_config_dqn['v_min'] = -10.\n",
    "trainer_config_dqn['v_max'] = 10.\n",
    "# trainer_config_dqn['num_atoms'] = 51\n",
    "\n",
    "# trainer_config_dqn['model']['fcnet_hiddens'] = [1024, 512,512, 256,256,32,8]\n",
    "trainer_config_dqn['model']['fcnet_hiddens'] = [256,256,32,8]\n",
    "\n",
    "trainer_config_dqn['num_cpus_per_worker'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_ppo = DEFAULT_CONFIG_PPO.copy()\n",
    "trainer_config_ppo['num_workers'] = 3\n",
    "trainer_config_ppo['num_sgd_iter'] = 20\n",
    "trainer_config_ppo['sgd_minibatch_size'] = 32\n",
    "trainer_config_ppo['model']['fcnet_hiddens'] = [1024, 512,512, 256,256,32,8]\n",
    "# trainer_config_ppo['model']['fcnet_hiddens'] = [256,256,32,8]\n",
    "# trainer_config_ppo['model']['fcnet_hiddens'] = [20,20]\n",
    "\n",
    "# trainer_config_ppo['lr'] =  0.001\n",
    "\n",
    "trainer_config_ppo['num_cpus_per_worker'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-29 11:20:49,967\tINFO trainable.py:101 -- Trainable.setup took 14.931 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-07-29 11:20:49,968\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration 0...\n",
      "32.0\n",
      "105.33333333333333\n",
      "240.0\n",
      "Training iteration 1...\n",
      "32.0\n",
      "125.72222222222223\n",
      "240.0\n",
      "Training iteration 2...\n",
      "32.0\n",
      "149.0\n",
      "240.0\n",
      "Training iteration 3...\n",
      "32.0\n",
      "163.4102564102564\n",
      "260.0\n",
      "Training iteration 4...\n",
      "32.0\n",
      "161.89583333333334\n",
      "272.0\n",
      "Training iteration 5...\n",
      "32.0\n",
      "166.26666666666668\n",
      "279.0\n",
      "Training iteration 6...\n",
      "32.0\n",
      "172.0144927536232\n",
      "280.0\n",
      "Training iteration 7...\n",
      "32.0\n",
      "176.96153846153845\n",
      "288.0\n",
      "Training iteration 8...\n",
      "32.0\n",
      "169.37777777777777\n",
      "289.0\n",
      "Training iteration 9...\n",
      "32.0\n",
      "170.17171717171718\n",
      "291.0\n",
      "Training iteration 10...\n",
      "49.0\n",
      "181.84\n",
      "297.0\n",
      "Training iteration 11...\n",
      "49.0\n",
      "187.67\n",
      "301.0\n",
      "Training iteration 12...\n",
      "66.0\n",
      "196.94\n",
      "301.0\n",
      "Training iteration 13...\n",
      "67.0\n",
      "191.36\n",
      "301.0\n",
      "Training iteration 14...\n",
      "69.0\n",
      "191.7\n",
      "301.0\n",
      "Training iteration 15...\n",
      "77.0\n",
      "190.42\n",
      "301.0\n",
      "Training iteration 16...\n",
      "81.0\n",
      "186.08\n",
      "301.0\n",
      "Training iteration 17...\n",
      "81.0\n",
      "191.09\n",
      "301.0\n",
      "Training iteration 18...\n",
      "85.0\n",
      "193.62\n",
      "301.0\n",
      "Training iteration 19...\n",
      "88.0\n",
      "196.87\n",
      "301.0\n"
     ]
    }
   ],
   "source": [
    "# trainer = PPOTrainer(trainer_config_ppo, env=\"MG_env\");\n",
    "trainer = DQNTrainer(trainer_config_dqn, env=\"MG_env\");\n",
    "ep_min = []\n",
    "ep_mean = []\n",
    "ep_max = []\n",
    "\n",
    "for i in range(20):\n",
    "    print(\"Training iteration {}...\".format(i))\n",
    "    result=trainer.train()\n",
    "#     print(pretty_print(result))\n",
    "\n",
    "    print(result['episode_reward_min'])\n",
    "    print(result['episode_reward_mean'])\n",
    "    print(result['episode_reward_max'])\n",
    "    \n",
    "    ep_min.append(result['episode_reward_min'])\n",
    "    ep_mean.append(result['episode_reward_mean'])\n",
    "    ep_max.append(result['episode_reward_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration 0...\n",
      "88.0\n",
      "196.12\n",
      "301.0\n",
      "Training iteration 1...\n",
      "88.0\n",
      "195.41\n",
      "301.0\n",
      "Training iteration 2...\n",
      "88.0\n",
      "192.53\n",
      "300.0\n",
      "Training iteration 3...\n",
      "88.0\n",
      "192.16\n",
      "300.0\n",
      "Training iteration 4...\n",
      "88.0\n",
      "195.5\n",
      "300.0\n",
      "Training iteration 5...\n",
      "82.0\n",
      "192.75\n",
      "293.0\n",
      "Training iteration 6...\n",
      "80.0\n",
      "196.05\n",
      "293.0\n",
      "Training iteration 7...\n",
      "79.0\n",
      "187.16\n",
      "293.0\n",
      "Training iteration 8...\n",
      "79.0\n",
      "198.4\n",
      "293.0\n",
      "Training iteration 9...\n",
      "79.0\n",
      "196.84\n",
      "293.0\n",
      "Training iteration 10...\n",
      "79.0\n",
      "196.85\n",
      "293.0\n",
      "Training iteration 11...\n",
      "79.0\n",
      "196.7\n",
      "293.0\n",
      "Training iteration 12...\n",
      "79.0\n",
      "188.48\n",
      "292.0\n",
      "Training iteration 13...\n",
      "79.0\n",
      "190.27\n",
      "292.0\n",
      "Training iteration 14...\n",
      "79.0\n",
      "184.36\n",
      "292.0\n",
      "Training iteration 15...\n",
      "79.0\n",
      "186.22\n",
      "292.0\n",
      "Training iteration 16...\n",
      "79.0\n",
      "191.47\n",
      "291.0\n",
      "Training iteration 17...\n",
      "79.0\n",
      "199.63\n",
      "292.0\n",
      "Training iteration 18...\n",
      "79.0\n",
      "193.32\n",
      "292.0\n",
      "Training iteration 19...\n",
      "79.0\n",
      "186.74\n",
      "292.0\n",
      "Training iteration 20...\n",
      "75.0\n",
      "187.99\n",
      "292.0\n",
      "Training iteration 21...\n",
      "74.0\n",
      "191.33\n",
      "293.0\n",
      "Training iteration 22...\n",
      "74.0\n",
      "195.0\n",
      "293.0\n",
      "Training iteration 23...\n",
      "74.0\n",
      "196.8\n",
      "293.0\n",
      "Training iteration 24...\n",
      "74.0\n",
      "193.11\n",
      "293.0\n",
      "Training iteration 25...\n",
      "74.0\n",
      "193.06\n",
      "293.0\n",
      "Training iteration 26...\n",
      "74.0\n",
      "189.55\n",
      "293.0\n",
      "Training iteration 27...\n",
      "74.0\n",
      "187.5\n",
      "293.0\n",
      "Training iteration 28...\n",
      "74.0\n",
      "187.65\n",
      "295.0\n",
      "Training iteration 29...\n",
      "74.0\n",
      "186.42\n",
      "295.0\n",
      "Training iteration 30...\n",
      "88.0\n",
      "185.24\n",
      "295.0\n",
      "Training iteration 31...\n",
      "88.0\n",
      "175.92\n",
      "295.0\n",
      "Training iteration 32...\n",
      "88.0\n",
      "177.5\n",
      "295.0\n",
      "Training iteration 33...\n",
      "88.0\n",
      "175.74\n",
      "295.0\n",
      "Training iteration 34...\n",
      "88.0\n",
      "179.42\n",
      "295.0\n",
      "Training iteration 35...\n",
      "87.0\n",
      "181.13\n",
      "295.0\n",
      "Training iteration 36...\n",
      "86.0\n",
      "179.04\n",
      "295.0\n",
      "Training iteration 37...\n",
      "86.0\n",
      "179.3\n",
      "295.0\n",
      "Training iteration 38...\n",
      "86.0\n",
      "175.47\n",
      "288.0\n",
      "Training iteration 39...\n",
      "85.0\n",
      "171.67\n",
      "290.0\n",
      "Training iteration 40...\n",
      "81.0\n",
      "177.14\n",
      "290.0\n",
      "Training iteration 41...\n",
      "74.0\n",
      "178.66\n",
      "290.0\n",
      "Training iteration 42...\n",
      "74.0\n",
      "181.12\n",
      "293.0\n",
      "Training iteration 43...\n",
      "59.0\n",
      "190.94\n",
      "299.0\n",
      "Training iteration 44...\n",
      "59.0\n",
      "191.47\n",
      "299.0\n",
      "Training iteration 45...\n",
      "59.0\n",
      "195.43\n",
      "299.0\n",
      "Training iteration 46...\n",
      "59.0\n",
      "196.25\n",
      "299.0\n",
      "Training iteration 47...\n",
      "59.0\n",
      "192.3\n",
      "299.0\n",
      "Training iteration 48...\n",
      "59.0\n",
      "198.3\n",
      "299.0\n",
      "Training iteration 49...\n",
      "59.0\n",
      "201.83\n",
      "299.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(50):\n",
    "    print(\"Training iteration {}...\".format(i))\n",
    "    result=trainer.train()\n",
    "#     print(pretty_print(result))\n",
    "    \n",
    "    print(result['episode_reward_min'])\n",
    "    print(result['episode_reward_mean'])\n",
    "    print(result['episode_reward_max'])\n",
    "    \n",
    "    ep_min.append(result['episode_reward_min'])\n",
    "    ep_mean.append(result['episode_reward_mean'])\n",
    "    ep_max.append(result['episode_reward_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "50*3+1*50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa6a065cc40>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ep_max)\n",
    "plt.plot(ep_mean)\n",
    "plt.plot(ep_min)\n",
    "\n",
    "plt.title('DQN')\n",
    "plt.xlabel('20x games')\n",
    "plt.legend(['max', 'mean', 'min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fa5fddc7e80>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(ep_max)\n",
    "plt.plot(ep_mean)\n",
    "plt.plot(ep_min)\n",
    "\n",
    "plt.title('PPO')\n",
    "plt.xlabel('20x games')\n",
    "plt.legend(['max', 'mean', 'min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer1 = PPOTrainer(trainer_config_ppo, MatrixGameEnv_no_history);\n",
    "trainer2 = PPOTrainer(trainer_config_ppo, MatrixGameEnv_no_history);\n",
    "\n",
    "# trainer = DQNTrainer(trainer_config_dqn, MatrixGameEnv_no_history);\n",
    "ep_min = []\n",
    "ep_mean = []\n",
    "ep_max = []\n",
    "\n",
    "for i in range(20):\n",
    "    print(\"Training iteration {}...\".format(i))\n",
    "    result=trainer.train()\n",
    "#     print(pretty_print(result))\n",
    "\n",
    "    print(result['episode_reward_min'])\n",
    "    print(result['episode_reward_mean'])\n",
    "    print(result['episode_reward_max'])\n",
    "    \n",
    "    ep_min.append(result['episode_reward_min'])\n",
    "    ep_mean.append(result['episode_reward_mean'])\n",
    "    ep_max.append(result['episode_reward_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "done = False\n",
    "cumulative_reward = 0\n",
    "action_save = []\n",
    "while not done:\n",
    "    action = trainer.compute_action(state)\n",
    "    action_save.append(action)\n",
    "    state, reward, done, results = env.step(action)\n",
    "    cumulative_reward += reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "\n",
    "n_samples = 50\n",
    "\n",
    "\n",
    "defects = []\n",
    "rewards = []\n",
    "for i in range(n_samples):\n",
    "    state = env.reset()\n",
    "    \n",
    "    total_defect = 0\n",
    "    cum_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = trainer.compute_action(state)\n",
    "        total_defect += action\n",
    "        state, reward, done, results = env.step(action)\n",
    "        cum_reward += reward\n",
    "    defects.append(total_defect)\n",
    "    rewards.append(cum_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f765109be20>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(defects, rewards,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env('MG_env', lambda c: MatrixGameEnv_no_history(player2=TitForTatOrDefect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env('MG_t4tTR_env', lambda c: MatrixGameEnv(player2=TitForTatThenDefect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_ppo = DEFAULT_CONFIG_PPO.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_ppo = DEFAULT_CONFIG_PPO.copy()\n",
    "trainer_config_ppo['num_workers'] = 5\n",
    "trainer_config_ppo['num_sgd_iter'] = 20\n",
    "trainer_config_ppo['sgd_minibatch_size'] = 32\n",
    "# trainer_config_ppo['model']['fcnet_hiddens'] = [1024, 512,512, 256,256,32,8]\n",
    "trainer_config_ppo['model']['fcnet_hiddens'] = [256,256,32,8]\n",
    "# trainer_config_ppo['model']['fcnet_hiddens'] = tune.grid_search([[32,16], [256, 32,16], [256, 256,32,16]])\n",
    "trainer_config_ppo['model']['use_lstm'] = False\n",
    "# trainer_config_ppo['lr'] =  tune.grid_search([0.01, 0.001, 0.0001])\n",
    "\n",
    "# trainer_config_ppo['num_cpus_per_worker'] = 0\n",
    "trainer_config_ppo['env'] = 'MG_env'\n",
    "\n",
    "# trainer_config_ppo['num_gpus'] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-30 14:08:40,359\tINFO worker.py:745 -- Calling ray.init() again after it has already been called.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 9.4/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.89 GiB heap, 0.0/2.45 GiB objects<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_0ee9d_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_0ee9d_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-30_14-09-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 241.0\n",
      "  episode_reward_mean: 125.05\n",
      "  episode_reward_min: 41.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: a8e539e34582432f84e0398c9b466da6\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6617069244384766\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03220942243933678\n",
      "          model: {}\n",
      "          policy_loss: -0.04173337668180466\n",
      "          total_loss: 3826.31689453125\n",
      "          vf_explained_var: -5.879402351638419e-07\n",
      "          vf_loss: 3826.352294921875\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 10.140.115.95\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.99130434782609\n",
      "    ram_util_percent: 71.93913043478263\n",
      "  pid: 80938\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17338572965281435\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15949435001901918\n",
      "    mean_inference_ms: 2.7289672141962136\n",
      "    mean_raw_obs_processing_ms: 0.2181374029571495\n",
      "  time_since_restore: 15.938587427139282\n",
      "  time_this_iter_s: 15.938587427139282\n",
      "  time_total_s: 15.938587427139282\n",
      "  timers:\n",
      "    learn_throughput: 306.893\n",
      "    learn_time_ms: 13033.874\n",
      "    load_throughput: 70469.409\n",
      "    load_time_ms: 56.762\n",
      "    sample_throughput: 1433.804\n",
      "    sample_time_ms: 2789.782\n",
      "    update_time_ms: 6.09\n",
      "  timestamp: 1627610954\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 0ee9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.89 GiB heap, 0.0/2.45 GiB objects<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_0ee9d_00000</td><td>RUNNING </td><td>10.140.115.95:80938</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         15.9386</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">  125.05</td><td style=\"text-align: right;\">                 241</td><td style=\"text-align: right;\">                  41</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_0ee9d_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-30_14-09-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.0\n",
      "  episode_reward_mean: 132.45\n",
      "  episode_reward_min: 36.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: a8e539e34582432f84e0398c9b466da6\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6572600603103638\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007395122665911913\n",
      "          model: {}\n",
      "          policy_loss: -0.016406115144491196\n",
      "          total_loss: 4635.94140625\n",
      "          vf_explained_var: -6.008148289993187e-08\n",
      "          vf_loss: 4635.9560546875\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 10.140.115.95\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.82142857142857\n",
      "    ram_util_percent: 72.2\n",
      "  pid: 80938\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17645293273459473\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16501559176270306\n",
      "    mean_inference_ms: 2.8758058007323113\n",
      "    mean_raw_obs_processing_ms: 0.2249678148781209\n",
      "  time_since_restore: 35.22434711456299\n",
      "  time_this_iter_s: 19.285759687423706\n",
      "  time_total_s: 35.22434711456299\n",
      "  timers:\n",
      "    learn_throughput: 277.974\n",
      "    learn_time_ms: 14389.851\n",
      "    load_throughput: 98208.285\n",
      "    load_time_ms: 40.73\n",
      "    sample_throughput: 1273.732\n",
      "    sample_time_ms: 3140.378\n",
      "    update_time_ms: 6.777\n",
      "  timestamp: 1627610973\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 0ee9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.89 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_4_61e8be635887a69c812c6ccdaff28072, 0.0/6.0 CPU_group_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_3_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_1_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_0_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_5_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_2_61e8be635887a69c812c6ccdaff28072)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_0ee9d_00000</td><td>RUNNING </td><td>10.140.115.95:80938</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         35.2243</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\">  132.45</td><td style=\"text-align: right;\">                 261</td><td style=\"text-align: right;\">                  36</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_0ee9d_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-30_14-09-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 261.0\n",
      "  episode_reward_mean: 133.12\n",
      "  episode_reward_min: 36.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: a8e539e34582432f84e0398c9b466da6\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.6297959089279175\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012444892898201942\n",
      "          model: {}\n",
      "          policy_loss: -0.02271566540002823\n",
      "          total_loss: 3985.2734375\n",
      "          vf_explained_var: -1.525878978725359e-08\n",
      "          vf_loss: 3985.292724609375\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 10.140.115.95\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.29166666666667\n",
      "    ram_util_percent: 72.44583333333333\n",
      "  pid: 80938\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17896414397830135\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1687037697023802\n",
      "    mean_inference_ms: 2.9612913085144736\n",
      "    mean_raw_obs_processing_ms: 0.2295673951979352\n",
      "  time_since_restore: 52.46797251701355\n",
      "  time_this_iter_s: 17.24362540245056\n",
      "  time_total_s: 52.46797251701355\n",
      "  timers:\n",
      "    learn_throughput: 279.469\n",
      "    learn_time_ms: 14312.853\n",
      "    load_throughput: 129892.146\n",
      "    load_time_ms: 30.795\n",
      "    sample_throughput: 1284.897\n",
      "    sample_time_ms: 3113.09\n",
      "    update_time_ms: 5.745\n",
      "  timestamp: 1627610991\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 0ee9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.89 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_3_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_4_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_1_61e8be635887a69c812c6ccdaff28072, 0.0/6.0 CPU_group_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_5_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_2_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_0_61e8be635887a69c812c6ccdaff28072)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_0ee9d_00000</td><td>RUNNING </td><td>10.140.115.95:80938</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">          52.468</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  133.12</td><td style=\"text-align: right;\">                 261</td><td style=\"text-align: right;\">                  36</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_0ee9d_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-30_14-10-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 263.0\n",
      "  episode_reward_mean: 129.4\n",
      "  episode_reward_min: 32.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: a8e539e34582432f84e0398c9b466da6\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5987403392791748\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014305796474218369\n",
      "          model: {}\n",
      "          policy_loss: -0.02717319130897522\n",
      "          total_loss: 3486.262939453125\n",
      "          vf_explained_var: -4.76837147544984e-09\n",
      "          vf_loss: 3486.2861328125\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 10.140.115.95\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.20384615384616\n",
      "    ram_util_percent: 72.66538461538461\n",
      "  pid: 80938\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17668754438465215\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.17095832340250894\n",
      "    mean_inference_ms: 2.9878593733447913\n",
      "    mean_raw_obs_processing_ms: 0.23017492141154974\n",
      "  time_since_restore: 70.59328722953796\n",
      "  time_this_iter_s: 18.125314712524414\n",
      "  time_total_s: 70.59328722953796\n",
      "  timers:\n",
      "    learn_throughput: 274.626\n",
      "    learn_time_ms: 14565.269\n",
      "    load_throughput: 163889.615\n",
      "    load_time_ms: 24.407\n",
      "    sample_throughput: 1320.549\n",
      "    sample_time_ms: 3029.042\n",
      "    update_time_ms: 5.321\n",
      "  timestamp: 1627611009\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 0ee9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.89 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_5_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_2_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_1_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_3_61e8be635887a69c812c6ccdaff28072, 0.0/6.0 CPU_group_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_4_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_0_61e8be635887a69c812c6ccdaff28072)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_0ee9d_00000</td><td>RUNNING </td><td>10.140.115.95:80938</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         70.5933</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">   129.4</td><td style=\"text-align: right;\">                 263</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_0ee9d_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-30_14-10-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 264.0\n",
      "  episode_reward_mean: 135.26\n",
      "  episode_reward_min: 26.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: a8e539e34582432f84e0398c9b466da6\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5804921984672546\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014489117078483105\n",
      "          model: {}\n",
      "          policy_loss: -0.030173804610967636\n",
      "          total_loss: 4820.7744140625\n",
      "          vf_explained_var: -8.583068478174027e-09\n",
      "          vf_loss: 4820.80029296875\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 10.140.115.95\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 69.84400000000001\n",
      "    ram_util_percent: 72.652\n",
      "  pid: 80938\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.17221539523601145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16865283823295582\n",
      "    mean_inference_ms: 2.905830956262187\n",
      "    mean_raw_obs_processing_ms: 0.22562936178036183\n",
      "  time_since_restore: 87.68491792678833\n",
      "  time_this_iter_s: 17.091630697250366\n",
      "  time_total_s: 87.68491792678833\n",
      "  timers:\n",
      "    learn_throughput: 274.58\n",
      "    learn_time_ms: 14567.689\n",
      "    load_throughput: 192388.235\n",
      "    load_time_ms: 20.791\n",
      "    sample_throughput: 1371.062\n",
      "    sample_time_ms: 2917.447\n",
      "    update_time_ms: 6.442\n",
      "  timestamp: 1627611026\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 0ee9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.1/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.89 GiB heap, 0.0/2.45 GiB objects (0.0/6.0 CPU_group_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_1_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_5_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_3_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_2_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_4_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_0_61e8be635887a69c812c6ccdaff28072)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_0ee9d_00000</td><td>RUNNING </td><td>10.140.115.95:80938</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         87.6849</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  135.26</td><td style=\"text-align: right;\">                 264</td><td style=\"text-align: right;\">                  26</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_0ee9d_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-30_14-10-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 264.0\n",
      "  episode_reward_mean: 135.64\n",
      "  episode_reward_min: 26.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: a8e539e34582432f84e0398c9b466da6\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5704641342163086\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01529738586395979\n",
      "          model: {}\n",
      "          policy_loss: -0.030574331060051918\n",
      "          total_loss: 3542.36962890625\n",
      "          vf_explained_var: -8.583068478174027e-09\n",
      "          vf_loss: 3542.39599609375\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 10.140.115.95\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 72.232\n",
      "    ram_util_percent: 72.668\n",
      "  pid: 80938\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16803807920543817\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16625065798238917\n",
      "    mean_inference_ms: 2.8334268172887986\n",
      "    mean_raw_obs_processing_ms: 0.22151628546618116\n",
      "  time_since_restore: 105.6004900932312\n",
      "  time_this_iter_s: 17.91557216644287\n",
      "  time_total_s: 105.6004900932312\n",
      "  timers:\n",
      "    learn_throughput: 272.367\n",
      "    learn_time_ms: 14686.053\n",
      "    load_throughput: 221332.87\n",
      "    load_time_ms: 18.072\n",
      "    sample_throughput: 1394.886\n",
      "    sample_time_ms: 2867.618\n",
      "    update_time_ms: 6.039\n",
      "  timestamp: 1627611044\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 0ee9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.89 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_3_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_5_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_0_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_1_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_4_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_2_61e8be635887a69c812c6ccdaff28072, 0.0/6.0 CPU_group_61e8be635887a69c812c6ccdaff28072)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_0ee9d_00000</td><td>RUNNING </td><td>10.140.115.95:80938</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">           105.6</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  135.64</td><td style=\"text-align: right;\">                 264</td><td style=\"text-align: right;\">                  26</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_0ee9d_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-30_14-10-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 266.0\n",
      "  episode_reward_mean: 142.11\n",
      "  episode_reward_min: 30.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 280\n",
      "  experiment_id: a8e539e34582432f84e0398c9b466da6\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5551830530166626\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011726786382496357\n",
      "          model: {}\n",
      "          policy_loss: -0.02380739152431488\n",
      "          total_loss: 4389.7998046875\n",
      "          vf_explained_var: -8.106232129989621e-09\n",
      "          vf_loss: 4389.8193359375\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 10.140.115.95\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 65.34090909090908\n",
      "    ram_util_percent: 72.87272727272727\n",
      "  pid: 80938\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16458358833272357\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.16416105427728078\n",
      "    mean_inference_ms: 2.7745685490853282\n",
      "    mean_raw_obs_processing_ms: 0.2181106942739526\n",
      "  time_since_restore: 120.59576678276062\n",
      "  time_this_iter_s: 14.995276689529419\n",
      "  time_total_s: 120.59576678276062\n",
      "  timers:\n",
      "    learn_throughput: 277.949\n",
      "    learn_time_ms: 14391.143\n",
      "    load_throughput: 247001.901\n",
      "    load_time_ms: 16.194\n",
      "    sample_throughput: 1431.458\n",
      "    sample_time_ms: 2794.354\n",
      "    update_time_ms: 5.668\n",
      "  timestamp: 1627611059\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 0ee9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.89 GiB heap, 0.0/2.45 GiB objects (0.0/6.0 CPU_group_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_3_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_0_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_5_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_1_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_2_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_4_61e8be635887a69c812c6ccdaff28072)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_0ee9d_00000</td><td>RUNNING </td><td>10.140.115.95:80938</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         120.596</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  142.11</td><td style=\"text-align: right;\">                 266</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_0ee9d_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-30_14-11-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 266.0\n",
      "  episode_reward_mean: 140.75\n",
      "  episode_reward_min: 30.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 320\n",
      "  experiment_id: a8e539e34582432f84e0398c9b466da6\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5371195673942566\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014624299481511116\n",
      "          model: {}\n",
      "          policy_loss: -0.02725093811750412\n",
      "          total_loss: 4689.15087890625\n",
      "          vf_explained_var: -7.1525572131747595e-09\n",
      "          vf_loss: 4689.17333984375\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 10.140.115.95\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.1590909090909\n",
      "    ram_util_percent: 72.8\n",
      "  pid: 80938\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16123271083409033\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1621345187356009\n",
      "    mean_inference_ms: 2.7116453392346944\n",
      "    mean_raw_obs_processing_ms: 0.2147925514583334\n",
      "  time_since_restore: 136.05114555358887\n",
      "  time_this_iter_s: 15.455378770828247\n",
      "  time_total_s: 136.05114555358887\n",
      "  timers:\n",
      "    learn_throughput: 280.687\n",
      "    learn_time_ms: 14250.757\n",
      "    load_throughput: 271805.298\n",
      "    load_time_ms: 14.716\n",
      "    sample_throughput: 1472.718\n",
      "    sample_time_ms: 2716.066\n",
      "    update_time_ms: 5.424\n",
      "  timestamp: 1627611074\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 0ee9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.89 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_5_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_0_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_3_61e8be635887a69c812c6ccdaff28072, 0.0/6.0 CPU_group_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_2_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_4_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_1_61e8be635887a69c812c6ccdaff28072)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_0ee9d_00000</td><td>RUNNING </td><td>10.140.115.95:80938</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         136.051</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  140.75</td><td style=\"text-align: right;\">                 266</td><td style=\"text-align: right;\">                  30</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_0ee9d_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-30_14-11-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 254.0\n",
      "  episode_reward_mean: 144.58\n",
      "  episode_reward_min: 28.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 360\n",
      "  experiment_id: a8e539e34582432f84e0398c9b466da6\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5342023968696594\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01294561568647623\n",
      "          model: {}\n",
      "          policy_loss: -0.02708115242421627\n",
      "          total_loss: 3896.235107421875\n",
      "          vf_explained_var: -9.536743617033494e-10\n",
      "          vf_loss: 3896.2578125\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 10.140.115.95\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 66.95454545454547\n",
      "    ram_util_percent: 72.85909090909092\n",
      "  pid: 80938\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15854325770925967\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15986354738623376\n",
      "    mean_inference_ms: 2.6538089708250268\n",
      "    mean_raw_obs_processing_ms: 0.2111369056137655\n",
      "  time_since_restore: 151.7832429409027\n",
      "  time_this_iter_s: 15.732097387313843\n",
      "  time_total_s: 151.7832429409027\n",
      "  timers:\n",
      "    learn_throughput: 282.575\n",
      "    learn_time_ms: 14155.544\n",
      "    load_throughput: 295169.12\n",
      "    load_time_ms: 13.552\n",
      "    sample_throughput: 1496.989\n",
      "    sample_time_ms: 2672.031\n",
      "    update_time_ms: 5.234\n",
      "  timestamp: 1627611090\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 0ee9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/4.89 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_4_61e8be635887a69c812c6ccdaff28072, 0.0/6.0 CPU_group_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_1_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_3_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_2_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_0_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_5_61e8be635887a69c812c6ccdaff28072)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_0ee9d_00000</td><td>RUNNING </td><td>10.140.115.95:80938</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         151.783</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  144.58</td><td style=\"text-align: right;\">                 254</td><td style=\"text-align: right;\">                  28</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_0ee9d_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-30_14-11-48\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 252.0\n",
      "  episode_reward_mean: 141.1\n",
      "  episode_reward_min: 28.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 400\n",
      "  experiment_id: a8e539e34582432f84e0398c9b466da6\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 4.999999873689376e-05\n",
      "          entropy: 0.5425683259963989\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014346797950565815\n",
      "          model: {}\n",
      "          policy_loss: -0.028796246275305748\n",
      "          total_loss: 3406.592041015625\n",
      "          vf_explained_var: 4.768371808516747e-10\n",
      "          vf_loss: 3406.61669921875\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 10.140.115.95\n",
      "  num_healthy_workers: 5\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.55384615384615\n",
      "    ram_util_percent: 72.88461538461539\n",
      "  pid: 80938\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.15776395559725157\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.15914626998842796\n",
      "    mean_inference_ms: 2.634681390008853\n",
      "    mean_raw_obs_processing_ms: 0.20972367696965918\n",
      "  time_since_restore: 169.56998872756958\n",
      "  time_this_iter_s: 17.78674578666687\n",
      "  time_total_s: 169.56998872756958\n",
      "  timers:\n",
      "    learn_throughput: 281.312\n",
      "    learn_time_ms: 14219.074\n",
      "    load_throughput: 310739.249\n",
      "    load_time_ms: 12.873\n",
      "    sample_throughput: 1480.213\n",
      "    sample_time_ms: 2702.314\n",
      "    update_time_ms: 5.089\n",
      "  timestamp: 1627611108\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 0ee9d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.89 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_1_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_5_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_3_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_4_61e8be635887a69c812c6ccdaff28072, 0.0/6.0 CPU_group_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_2_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_0_61e8be635887a69c812c6ccdaff28072)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_0ee9d_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">          169.57</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">   141.1</td><td style=\"text-align: right;\">                 252</td><td style=\"text-align: right;\">                  28</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/4.89 GiB heap, 0.0/2.45 GiB objects (0.0/1.0 CPU_group_1_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_5_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_3_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_4_61e8be635887a69c812c6ccdaff28072, 0.0/6.0 CPU_group_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_2_61e8be635887a69c812c6ccdaff28072, 0.0/1.0 CPU_group_0_61e8be635887a69c812c6ccdaff28072)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_0ee9d_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">          169.57</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">   141.1</td><td style=\"text-align: right;\">                 252</td><td style=\"text-align: right;\">                  28</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-30 14:11:49,359\tINFO tune.py:549 -- Total run time: 189.00 seconds (188.32 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f163016bd60>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True)\n",
    "tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\"training_iteration\":10},\n",
    "    \n",
    "    config=trainer_config_ppo,\n",
    "#     config={\n",
    "# #         \"num_gpus\": 0,\n",
    "# #         \"num_workers\": 1,\n",
    "# #         \"lr\": tune.grid_search([0.01, 0.001, 0.0001]),\n",
    "#         \"config\":trainer_config_ppo\n",
    "#     },\n",
    "#     verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 11.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/6.62 GiB heap, 0.0/3.31 GiB objects<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 3/3 (3 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc  </th><th style=\"text-align: right;\">    lr</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_1cc7b_00000</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.01  </td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00001</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.001 </td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00002</td><td>PENDING </td><td>     </td><td style=\"text-align: right;\">0.0001</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_1cc7b_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-40-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 240.0\n",
      "  episode_reward_mean: 120.55\n",
      "  episode_reward_min: 43.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 32588f95e5924ff18cc497c7b633b851\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.6639519333839417\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03095288760960102\n",
      "          model: {}\n",
      "          policy_loss: -0.008811107836663723\n",
      "          total_loss: 2870.7958984375\n",
      "          vf_explained_var: 0.04311968386173248\n",
      "          vf_loss: 2870.798583984375\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.85000000000001\n",
      "    ram_util_percent: 87.825\n",
      "  pid: 97175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08181237304428166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08328066441870367\n",
      "    mean_inference_ms: 1.166619708674516\n",
      "    mean_raw_obs_processing_ms: 0.11062151311308525\n",
      "  time_since_restore: 16.28184676170349\n",
      "  time_this_iter_s: 16.28184676170349\n",
      "  time_total_s: 16.28184676170349\n",
      "  timers:\n",
      "    learn_throughput: 388.935\n",
      "    learn_time_ms: 10284.492\n",
      "    load_throughput: 64980.115\n",
      "    load_time_ms: 61.557\n",
      "    sample_throughput: 681.041\n",
      "    sample_time_ms: 5873.359\n",
      "    update_time_ms: 7.172\n",
      "  timestamp: 1627515626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 1cc7b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/6.62 GiB heap, 0.0/3.31 GiB objects<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_1cc7b_00000</td><td>RUNNING </td><td>192.168.1.21:97175</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         16.2818</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">  120.55</td><td style=\"text-align: right;\">                 240</td><td style=\"text-align: right;\">                  43</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00001</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00002</td><td>RUNNING </td><td>                  </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">      </td><td style=\"text-align: right;\">                </td><td style=\"text-align: right;\">    </td><td style=\"text-align: right;\">        </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                    </td><td style=\"text-align: right;\">                  </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_1cc7b_00001:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-40-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 241.0\n",
      "  episode_reward_mean: 160.225\n",
      "  episode_reward_min: 40.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: 0c6578794e374e0ba692e935dfaa465e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.6662601232528687\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027907950803637505\n",
      "          model: {}\n",
      "          policy_loss: -0.06406012922525406\n",
      "          total_loss: 4495.77001953125\n",
      "          vf_explained_var: -3.382659633643925e-05\n",
      "          vf_loss: 4495.82861328125\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 77.0\n",
      "    ram_util_percent: 87.83333333333333\n",
      "  pid: 97178\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08067838015242894\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08505691560737136\n",
      "    mean_inference_ms: 1.1857670266995934\n",
      "    mean_raw_obs_processing_ms: 0.11453798966716451\n",
      "  time_since_restore: 16.497774839401245\n",
      "  time_this_iter_s: 16.497774839401245\n",
      "  time_total_s: 16.497774839401245\n",
      "  timers:\n",
      "    learn_throughput: 383.86\n",
      "    learn_time_ms: 10420.478\n",
      "    load_throughput: 61344.225\n",
      "    load_time_ms: 65.206\n",
      "    sample_throughput: 670.555\n",
      "    sample_time_ms: 5965.206\n",
      "    update_time_ms: 4.646\n",
      "  timestamp: 1627515626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 1cc7b_00001\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00002:\n",
      "  agent_timesteps_total: 4000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-40-26\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 240.0\n",
      "  episode_reward_mean: 140.2\n",
      "  episode_reward_min: 47.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 40\n",
      "  experiment_id: f4f18905240646649c93157709d0a45e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6730242371559143\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02072157710790634\n",
      "          model: {}\n",
      "          policy_loss: -0.03684104233980179\n",
      "          total_loss: 3716.78466796875\n",
      "          vf_explained_var: -0.0016224903520196676\n",
      "          vf_loss: 3716.817626953125\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 4000\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 4000\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 76.94583333333334\n",
      "    ram_util_percent: 87.82916666666667\n",
      "  pid: 97173\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08081627082777035\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08607012008136881\n",
      "    mean_inference_ms: 1.1868136610218478\n",
      "    mean_raw_obs_processing_ms: 0.11221804162377987\n",
      "  time_since_restore: 16.555179119110107\n",
      "  time_this_iter_s: 16.555179119110107\n",
      "  time_total_s: 16.555179119110107\n",
      "  timers:\n",
      "    learn_throughput: 381.196\n",
      "    learn_time_ms: 10493.295\n",
      "    load_throughput: 65451.118\n",
      "    load_time_ms: 61.114\n",
      "    sample_throughput: 670.846\n",
      "    sample_time_ms: 5962.624\n",
      "    update_time_ms: 3.226\n",
      "  timestamp: 1627515626\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 1\n",
      "  trial_id: 1cc7b_00002\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-40-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 260.0\n",
      "  episode_reward_mean: 137.2125\n",
      "  episode_reward_min: 38.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 32588f95e5924ff18cc497c7b633b851\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.6543769240379333\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02703109011054039\n",
      "          model: {}\n",
      "          policy_loss: -0.033032651990652084\n",
      "          total_loss: 3461.95263671875\n",
      "          vf_explained_var: 0.16419366002082825\n",
      "          vf_loss: 3461.977294921875\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.05652173913045\n",
      "    ram_util_percent: 88.18695652173915\n",
      "  pid: 97175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08232267620230198\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08371286781345996\n",
      "    mean_inference_ms: 1.180054513030663\n",
      "    mean_raw_obs_processing_ms: 0.11102714622822399\n",
      "  time_since_restore: 32.47739839553833\n",
      "  time_this_iter_s: 16.19555163383484\n",
      "  time_total_s: 32.47739839553833\n",
      "  timers:\n",
      "    learn_throughput: 393.664\n",
      "    learn_time_ms: 10160.95\n",
      "    load_throughput: 85368.721\n",
      "    load_time_ms: 46.856\n",
      "    sample_throughput: 667.751\n",
      "    sample_time_ms: 5990.253\n",
      "    update_time_ms: 7.102\n",
      "  timestamp: 1627515642\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 1cc7b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/6.62 GiB heap, 0.0/3.31 GiB objects (0.0/1.0 CPU_group_0_ce6012929ac9623150263bb845cff8e8, 0.0/1.0 CPU_group_0_359db8288e7c09c622134661ba803f8c, 0.0/1.0 CPU_group_0_13c0d4dc4b448e67bac04705c4cf0956, 0.0/1.0 CPU_group_1_ce6012929ac9623150263bb845cff8e8, 0.0/2.0 CPU_group_359db8288e7c09c622134661ba803f8c, 0.0/1.0 CPU_group_1_359db8288e7c09c622134661ba803f8c, 0.0/2.0 CPU_group_ce6012929ac9623150263bb845cff8e8, 0.0/1.0 CPU_group_1_13c0d4dc4b448e67bac04705c4cf0956, 0.0/2.0 CPU_group_13c0d4dc4b448e67bac04705c4cf0956)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_1cc7b_00000</td><td>RUNNING </td><td>192.168.1.21:97175</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         32.4774</td><td style=\"text-align: right;\">8000</td><td style=\"text-align: right;\"> 137.213</td><td style=\"text-align: right;\">                 260</td><td style=\"text-align: right;\">                  38</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00001</td><td>RUNNING </td><td>192.168.1.21:97178</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         16.4978</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 160.225</td><td style=\"text-align: right;\">                 241</td><td style=\"text-align: right;\">                  40</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00002</td><td>RUNNING </td><td>192.168.1.21:97173</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         16.5552</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\"> 140.2  </td><td style=\"text-align: right;\">                 240</td><td style=\"text-align: right;\">                  47</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_1cc7b_00001:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-40-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 252.0\n",
      "  episode_reward_mean: 139.2625\n",
      "  episode_reward_min: 33.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: 0c6578794e374e0ba692e935dfaa465e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.6255355477333069\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.04351194575428963\n",
      "          model: {}\n",
      "          policy_loss: -0.09668713063001633\n",
      "          total_loss: 2530.711669921875\n",
      "          vf_explained_var: -1.610094477655366e-05\n",
      "          vf_loss: 2530.795654296875\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.0608695652174\n",
      "    ram_util_percent: 88.18695652173915\n",
      "  pid: 97178\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08076013841039334\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08566022925606702\n",
      "    mean_inference_ms: 1.1944962497326788\n",
      "    mean_raw_obs_processing_ms: 0.11544761282329671\n",
      "  time_since_restore: 32.736690282821655\n",
      "  time_this_iter_s: 16.23891544342041\n",
      "  time_total_s: 32.736690282821655\n",
      "  timers:\n",
      "    learn_throughput: 390.519\n",
      "    learn_time_ms: 10242.78\n",
      "    load_throughput: 95498.725\n",
      "    load_time_ms: 41.885\n",
      "    sample_throughput: 660.738\n",
      "    sample_time_ms: 6053.839\n",
      "    update_time_ms: 4.581\n",
      "  timestamp: 1627515643\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 1cc7b_00001\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00002:\n",
      "  agent_timesteps_total: 8000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-40-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 254.0\n",
      "  episode_reward_mean: 137.7\n",
      "  episode_reward_min: 32.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 80\n",
      "  experiment_id: f4f18905240646649c93157709d0a45e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6392973065376282\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016313472762703896\n",
      "          model: {}\n",
      "          policy_loss: -0.026447471231222153\n",
      "          total_loss: 2559.197998046875\n",
      "          vf_explained_var: -0.0015709246508777142\n",
      "          vf_loss: 2559.219970703125\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 8000\n",
      "    num_steps_sampled: 8000\n",
      "    num_steps_trained: 8000\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.04782608695653\n",
      "    ram_util_percent: 88.18695652173915\n",
      "  pid: 97173\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0813331866230373\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08677728556274134\n",
      "    mean_inference_ms: 1.1947466862366225\n",
      "    mean_raw_obs_processing_ms: 0.11309240215405372\n",
      "  time_since_restore: 32.79192304611206\n",
      "  time_this_iter_s: 16.236743927001953\n",
      "  time_total_s: 32.79192304611206\n",
      "  timers:\n",
      "    learn_throughput: 388.781\n",
      "    learn_time_ms: 10288.565\n",
      "    load_throughput: 100511.423\n",
      "    load_time_ms: 39.796\n",
      "    sample_throughput: 662.026\n",
      "    sample_time_ms: 6042.063\n",
      "    update_time_ms: 3.294\n",
      "  timestamp: 1627515643\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 8000\n",
      "  training_iteration: 2\n",
      "  trial_id: 1cc7b_00002\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00000:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-40-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 264.0\n",
      "  episode_reward_mean: 143.47\n",
      "  episode_reward_min: 38.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 32588f95e5924ff18cc497c7b633b851\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.649207353591919\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022204404696822166\n",
      "          model: {}\n",
      "          policy_loss: -0.02495741844177246\n",
      "          total_loss: 2158.07958984375\n",
      "          vf_explained_var: 0.415693074464798\n",
      "          vf_loss: 2158.0947265625\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.20416666666667\n",
      "    ram_util_percent: 88.23333333333333\n",
      "  pid: 97175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08287924201367237\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08429108758336064\n",
      "    mean_inference_ms: 1.192094166282459\n",
      "    mean_raw_obs_processing_ms: 0.11148114493480275\n",
      "  time_since_restore: 49.07648062705994\n",
      "  time_this_iter_s: 16.599082231521606\n",
      "  time_total_s: 49.07648062705994\n",
      "  timers:\n",
      "    learn_throughput: 390.268\n",
      "    learn_time_ms: 10249.377\n",
      "    load_throughput: 108619.923\n",
      "    load_time_ms: 36.826\n",
      "    sample_throughput: 662.283\n",
      "    sample_time_ms: 6039.717\n",
      "    update_time_ms: 6.01\n",
      "  timestamp: 1627515659\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 1cc7b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/6.62 GiB heap, 0.0/3.31 GiB objects (0.0/1.0 CPU_group_0_359db8288e7c09c622134661ba803f8c, 0.0/1.0 CPU_group_0_ce6012929ac9623150263bb845cff8e8, 0.0/1.0 CPU_group_1_13c0d4dc4b448e67bac04705c4cf0956, 0.0/2.0 CPU_group_359db8288e7c09c622134661ba803f8c, 0.0/2.0 CPU_group_ce6012929ac9623150263bb845cff8e8, 0.0/1.0 CPU_group_1_359db8288e7c09c622134661ba803f8c, 0.0/2.0 CPU_group_13c0d4dc4b448e67bac04705c4cf0956, 0.0/1.0 CPU_group_1_ce6012929ac9623150263bb845cff8e8, 0.0/1.0 CPU_group_0_13c0d4dc4b448e67bac04705c4cf0956)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_1cc7b_00000</td><td>RUNNING </td><td>192.168.1.21:97175</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         49.0765</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\"> 143.47 </td><td style=\"text-align: right;\">                 264</td><td style=\"text-align: right;\">                  38</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00001</td><td>RUNNING </td><td>192.168.1.21:97178</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         32.7367</td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\"> 139.262</td><td style=\"text-align: right;\">                 252</td><td style=\"text-align: right;\">                  33</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00002</td><td>RUNNING </td><td>192.168.1.21:97173</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         32.7919</td><td style=\"text-align: right;\"> 8000</td><td style=\"text-align: right;\"> 137.7  </td><td style=\"text-align: right;\">                 254</td><td style=\"text-align: right;\">                  32</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_1cc7b_00001:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-40-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 255.0\n",
      "  episode_reward_mean: 137.46\n",
      "  episode_reward_min: 27.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: 0c6578794e374e0ba692e935dfaa465e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5972404479980469\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0455242283642292\n",
      "          model: {}\n",
      "          policy_loss: -0.11455514281988144\n",
      "          total_loss: 2629.490234375\n",
      "          vf_explained_var: -1.1509464457049035e-05\n",
      "          vf_loss: 2629.58447265625\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.19583333333334\n",
      "    ram_util_percent: 88.22916666666667\n",
      "  pid: 97178\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08114483521250765\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08624578702492201\n",
      "    mean_inference_ms: 1.202654250002556\n",
      "    mean_raw_obs_processing_ms: 0.11624324858375683\n",
      "  time_since_restore: 49.237213134765625\n",
      "  time_this_iter_s: 16.50052285194397\n",
      "  time_total_s: 49.237213134765625\n",
      "  timers:\n",
      "    learn_throughput: 389.727\n",
      "    learn_time_ms: 10263.582\n",
      "    load_throughput: 120165.136\n",
      "    load_time_ms: 33.288\n",
      "    sample_throughput: 656.708\n",
      "    sample_time_ms: 6090.989\n",
      "    update_time_ms: 4.298\n",
      "  timestamp: 1627515659\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 1cc7b_00001\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00002:\n",
      "  agent_timesteps_total: 12000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-40-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 254.0\n",
      "  episode_reward_mean: 127.95\n",
      "  episode_reward_min: 29.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 120\n",
      "  experiment_id: f4f18905240646649c93157709d0a45e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6224110722541809\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017085256054997444\n",
      "          model: {}\n",
      "          policy_loss: -0.03096138872206211\n",
      "          total_loss: 1750.92578125\n",
      "          vf_explained_var: -0.0008432692266069353\n",
      "          vf_loss: 1750.9515380859375\n",
      "    num_agent_steps_sampled: 12000\n",
      "    num_agent_steps_trained: 12000\n",
      "    num_steps_sampled: 12000\n",
      "    num_steps_trained: 12000\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.19166666666668\n",
      "    ram_util_percent: 88.22916666666667\n",
      "  pid: 97173\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08181651013371717\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08741493952658413\n",
      "    mean_inference_ms: 1.2023180701082967\n",
      "    mean_raw_obs_processing_ms: 0.11383000392669736\n",
      "  time_since_restore: 49.32488703727722\n",
      "  time_this_iter_s: 16.53296399116516\n",
      "  time_total_s: 49.32488703727722\n",
      "  timers:\n",
      "    learn_throughput: 388.116\n",
      "    learn_time_ms: 10306.189\n",
      "    load_throughput: 124725.301\n",
      "    load_time_ms: 32.07\n",
      "    sample_throughput: 657.609\n",
      "    sample_time_ms: 6082.642\n",
      "    update_time_ms: 3.122\n",
      "  timestamp: 1627515659\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 12000\n",
      "  training_iteration: 3\n",
      "  trial_id: 1cc7b_00002\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00000:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-41-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 264.0\n",
      "  episode_reward_mean: 137.22\n",
      "  episode_reward_min: 38.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 32588f95e5924ff18cc497c7b633b851\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.6287674307823181\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.024377550929784775\n",
      "          model: {}\n",
      "          policy_loss: -0.03856144845485687\n",
      "          total_loss: 1147.799560546875\n",
      "          vf_explained_var: 0.7238659858703613\n",
      "          vf_loss: 1147.82177734375\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.8913043478261\n",
      "    ram_util_percent: 88.32608695652175\n",
      "  pid: 97175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08342394931566785\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0849324973141033\n",
      "    mean_inference_ms: 1.204480016175703\n",
      "    mean_raw_obs_processing_ms: 0.11195610264868018\n",
      "  time_since_restore: 65.63825511932373\n",
      "  time_this_iter_s: 16.561774492263794\n",
      "  time_total_s: 65.63825511932373\n",
      "  timers:\n",
      "    learn_throughput: 389.134\n",
      "    learn_time_ms: 10279.229\n",
      "    load_throughput: 137884.065\n",
      "    load_time_ms: 29.01\n",
      "    sample_throughput: 658.854\n",
      "    sample_time_ms: 6071.146\n",
      "    update_time_ms: 5.303\n",
      "  timestamp: 1627515676\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 1cc7b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/6.62 GiB heap, 0.0/3.31 GiB objects (0.0/1.0 CPU_group_0_359db8288e7c09c622134661ba803f8c, 0.0/1.0 CPU_group_1_13c0d4dc4b448e67bac04705c4cf0956, 0.0/1.0 CPU_group_1_ce6012929ac9623150263bb845cff8e8, 0.0/2.0 CPU_group_ce6012929ac9623150263bb845cff8e8, 0.0/1.0 CPU_group_0_ce6012929ac9623150263bb845cff8e8, 0.0/2.0 CPU_group_13c0d4dc4b448e67bac04705c4cf0956, 0.0/2.0 CPU_group_359db8288e7c09c622134661ba803f8c, 0.0/1.0 CPU_group_0_13c0d4dc4b448e67bac04705c4cf0956, 0.0/1.0 CPU_group_1_359db8288e7c09c622134661ba803f8c)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_1cc7b_00000</td><td>RUNNING </td><td>192.168.1.21:97175</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         65.6383</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  137.22</td><td style=\"text-align: right;\">                 264</td><td style=\"text-align: right;\">                  38</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00001</td><td>RUNNING </td><td>192.168.1.21:97178</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         49.2372</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  137.46</td><td style=\"text-align: right;\">                 255</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00002</td><td>RUNNING </td><td>192.168.1.21:97173</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         49.3249</td><td style=\"text-align: right;\">12000</td><td style=\"text-align: right;\">  127.95</td><td style=\"text-align: right;\">                 254</td><td style=\"text-align: right;\">                  29</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_1cc7b_00001:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-41-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 255.0\n",
      "  episode_reward_mean: 130.76\n",
      "  episode_reward_min: 25.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: 0c6578794e374e0ba692e935dfaa465e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5673757791519165\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.041409656405448914\n",
      "          model: {}\n",
      "          policy_loss: -0.11974494904279709\n",
      "          total_loss: 2068.830322265625\n",
      "          vf_explained_var: -3.116746029263595e-06\n",
      "          vf_loss: 2068.922119140625\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.88695652173915\n",
      "    ram_util_percent: 88.3217391304348\n",
      "  pid: 97178\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0816475398983356\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08689743987679069\n",
      "    mean_inference_ms: 1.2097585875362873\n",
      "    mean_raw_obs_processing_ms: 0.11707814797932864\n",
      "  time_since_restore: 65.8122136592865\n",
      "  time_this_iter_s: 16.575000524520874\n",
      "  time_total_s: 65.8122136592865\n",
      "  timers:\n",
      "    learn_throughput: 388.12\n",
      "    learn_time_ms: 10306.091\n",
      "    load_throughput: 151363.59\n",
      "    load_time_ms: 26.426\n",
      "    sample_throughput: 655.855\n",
      "    sample_time_ms: 6098.91\n",
      "    update_time_ms: 4.12\n",
      "  timestamp: 1627515676\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 1cc7b_00001\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00002:\n",
      "  agent_timesteps_total: 16000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-41-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 255.0\n",
      "  episode_reward_mean: 129.31\n",
      "  episode_reward_min: 20.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 160\n",
      "  experiment_id: f4f18905240646649c93157709d0a45e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6139891147613525\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020770389586687088\n",
      "          model: {}\n",
      "          policy_loss: -0.03199220821261406\n",
      "          total_loss: 1759.123779296875\n",
      "          vf_explained_var: -0.00040992614231072366\n",
      "          vf_loss: 1759.149658203125\n",
      "    num_agent_steps_sampled: 16000\n",
      "    num_agent_steps_trained: 16000\n",
      "    num_steps_sampled: 16000\n",
      "    num_steps_trained: 16000\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.325\n",
      "    ram_util_percent: 88.3125\n",
      "  pid: 97173\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08250157564569154\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08834557793592379\n",
      "    mean_inference_ms: 1.2122109704173374\n",
      "    mean_raw_obs_processing_ms: 0.11482724663757297\n",
      "  time_since_restore: 65.94948840141296\n",
      "  time_this_iter_s: 16.624601364135742\n",
      "  time_total_s: 65.94948840141296\n",
      "  timers:\n",
      "    learn_throughput: 387.973\n",
      "    learn_time_ms: 10309.988\n",
      "    load_throughput: 143946.187\n",
      "    load_time_ms: 27.788\n",
      "    sample_throughput: 652.389\n",
      "    sample_time_ms: 6131.308\n",
      "    update_time_ms: 2.959\n",
      "  timestamp: 1627515676\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 16000\n",
      "  training_iteration: 4\n",
      "  trial_id: 1cc7b_00002\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00000:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-41-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 259.0\n",
      "  episode_reward_mean: 144.9\n",
      "  episode_reward_min: 40.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 32588f95e5924ff18cc497c7b633b851\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.6207384467124939\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01968519203364849\n",
      "          model: {}\n",
      "          policy_loss: -0.036460574716329575\n",
      "          total_loss: 715.7515258789062\n",
      "          vf_explained_var: 0.8963881134986877\n",
      "          vf_loss: 715.76806640625\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.7\n",
      "    ram_util_percent: 88.09999999999998\n",
      "  pid: 97175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08379937147942403\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08531232857725624\n",
      "    mean_inference_ms: 1.2114569946032376\n",
      "    mean_raw_obs_processing_ms: 0.11238213288928016\n",
      "  time_since_restore: 82.466805934906\n",
      "  time_this_iter_s: 16.828550815582275\n",
      "  time_total_s: 82.466805934906\n",
      "  timers:\n",
      "    learn_throughput: 386.566\n",
      "    learn_time_ms: 10347.531\n",
      "    load_throughput: 162297.539\n",
      "    load_time_ms: 24.646\n",
      "    sample_throughput: 656.46\n",
      "    sample_time_ms: 6093.285\n",
      "    update_time_ms: 5.76\n",
      "  timestamp: 1627515693\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 1cc7b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/6.62 GiB heap, 0.0/3.31 GiB objects (0.0/1.0 CPU_group_0_13c0d4dc4b448e67bac04705c4cf0956, 0.0/1.0 CPU_group_1_13c0d4dc4b448e67bac04705c4cf0956, 0.0/1.0 CPU_group_0_ce6012929ac9623150263bb845cff8e8, 0.0/2.0 CPU_group_ce6012929ac9623150263bb845cff8e8, 0.0/1.0 CPU_group_1_ce6012929ac9623150263bb845cff8e8, 0.0/1.0 CPU_group_0_359db8288e7c09c622134661ba803f8c, 0.0/2.0 CPU_group_13c0d4dc4b448e67bac04705c4cf0956, 0.0/2.0 CPU_group_359db8288e7c09c622134661ba803f8c, 0.0/1.0 CPU_group_1_359db8288e7c09c622134661ba803f8c)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_1cc7b_00000</td><td>RUNNING </td><td>192.168.1.21:97175</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         82.4668</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  144.9 </td><td style=\"text-align: right;\">                 259</td><td style=\"text-align: right;\">                  40</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00001</td><td>RUNNING </td><td>192.168.1.21:97178</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         65.8122</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  130.76</td><td style=\"text-align: right;\">                 255</td><td style=\"text-align: right;\">                  25</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00002</td><td>RUNNING </td><td>192.168.1.21:97173</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         65.9495</td><td style=\"text-align: right;\">16000</td><td style=\"text-align: right;\">  129.31</td><td style=\"text-align: right;\">                 255</td><td style=\"text-align: right;\">                  20</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_1cc7b_00001:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-41-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 249.0\n",
      "  episode_reward_mean: 139.3\n",
      "  episode_reward_min: 25.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: 0c6578794e374e0ba692e935dfaa465e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5427832007408142\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03556763008236885\n",
      "          model: {}\n",
      "          policy_loss: -0.12137497216463089\n",
      "          total_loss: 2354.750732421875\n",
      "          vf_explained_var: -1.030583575811761e-06\n",
      "          vf_loss: 2354.835693359375\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.66666666666667\n",
      "    ram_util_percent: 88.09999999999998\n",
      "  pid: 97178\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08196717053724288\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08719863131814128\n",
      "    mean_inference_ms: 1.2145939543725408\n",
      "    mean_raw_obs_processing_ms: 0.11747819116168122\n",
      "  time_since_restore: 82.60151267051697\n",
      "  time_this_iter_s: 16.78929901123047\n",
      "  time_total_s: 82.60151267051697\n",
      "  timers:\n",
      "    learn_throughput: 386.421\n",
      "    learn_time_ms: 10351.399\n",
      "    load_throughput: 173850.675\n",
      "    load_time_ms: 23.008\n",
      "    sample_throughput: 653.056\n",
      "    sample_time_ms: 6125.049\n",
      "    update_time_ms: 4.332\n",
      "  timestamp: 1627515693\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 1cc7b_00001\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00002:\n",
      "  agent_timesteps_total: 20000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-41-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 260.0\n",
      "  episode_reward_mean: 131.7\n",
      "  episode_reward_min: 20.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 200\n",
      "  experiment_id: f4f18905240646649c93157709d0a45e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.6127324104309082\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.020846014842391014\n",
      "          model: {}\n",
      "          policy_loss: -0.04059657081961632\n",
      "          total_loss: 2014.380615234375\n",
      "          vf_explained_var: -9.42599363042973e-05\n",
      "          vf_loss: 2014.4117431640625\n",
      "    num_agent_steps_sampled: 20000\n",
      "    num_agent_steps_trained: 20000\n",
      "    num_steps_sampled: 20000\n",
      "    num_steps_trained: 20000\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.66666666666667\n",
      "    ram_util_percent: 88.09583333333332\n",
      "  pid: 97173\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08305729172037139\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08924166954305876\n",
      "    mean_inference_ms: 1.222805791014114\n",
      "    mean_raw_obs_processing_ms: 0.11564090833096886\n",
      "  time_since_restore: 82.89945077896118\n",
      "  time_this_iter_s: 16.949962377548218\n",
      "  time_total_s: 82.89945077896118\n",
      "  timers:\n",
      "    learn_throughput: 386.608\n",
      "    learn_time_ms: 10346.387\n",
      "    load_throughput: 161358.483\n",
      "    load_time_ms: 24.79\n",
      "    sample_throughput: 646.041\n",
      "    sample_time_ms: 6191.56\n",
      "    update_time_ms: 2.97\n",
      "  timestamp: 1627515693\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 20000\n",
      "  training_iteration: 5\n",
      "  trial_id: 1cc7b_00002\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00000:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-41-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 259.0\n",
      "  episode_reward_mean: 160.15\n",
      "  episode_reward_min: 44.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: 32588f95e5924ff18cc497c7b633b851\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.6027557253837585\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.027560459449887276\n",
      "          model: {}\n",
      "          policy_loss: -0.03080029785633087\n",
      "          total_loss: 158.99375915527344\n",
      "          vf_explained_var: 0.959585428237915\n",
      "          vf_loss: 158.99664306640625\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.39999999999999\n",
      "    ram_util_percent: 88.14583333333333\n",
      "  pid: 97175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08410709555339203\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08550181019649247\n",
      "    mean_inference_ms: 1.215552374874334\n",
      "    mean_raw_obs_processing_ms: 0.1126412501514766\n",
      "  time_since_restore: 99.00859522819519\n",
      "  time_this_iter_s: 16.541789293289185\n",
      "  time_total_s: 99.00859522819519\n",
      "  timers:\n",
      "    learn_throughput: 386.281\n",
      "    learn_time_ms: 10355.154\n",
      "    load_throughput: 184469.07\n",
      "    load_time_ms: 21.684\n",
      "    sample_throughput: 655.901\n",
      "    sample_time_ms: 6098.477\n",
      "    update_time_ms: 5.557\n",
      "  timestamp: 1627515709\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 1cc7b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/6.62 GiB heap, 0.0/3.31 GiB objects (0.0/1.0 CPU_group_1_13c0d4dc4b448e67bac04705c4cf0956, 0.0/2.0 CPU_group_ce6012929ac9623150263bb845cff8e8, 0.0/1.0 CPU_group_0_13c0d4dc4b448e67bac04705c4cf0956, 0.0/2.0 CPU_group_13c0d4dc4b448e67bac04705c4cf0956, 0.0/1.0 CPU_group_1_359db8288e7c09c622134661ba803f8c, 0.0/1.0 CPU_group_1_ce6012929ac9623150263bb845cff8e8, 0.0/1.0 CPU_group_0_359db8288e7c09c622134661ba803f8c, 0.0/2.0 CPU_group_359db8288e7c09c622134661ba803f8c, 0.0/1.0 CPU_group_0_ce6012929ac9623150263bb845cff8e8)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_1cc7b_00000</td><td>RUNNING </td><td>192.168.1.21:97175</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         99.0086</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  160.15</td><td style=\"text-align: right;\">                 259</td><td style=\"text-align: right;\">                  44</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00001</td><td>RUNNING </td><td>192.168.1.21:97178</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         82.6015</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  139.3 </td><td style=\"text-align: right;\">                 249</td><td style=\"text-align: right;\">                  25</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00002</td><td>RUNNING </td><td>192.168.1.21:97173</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         82.8995</td><td style=\"text-align: right;\">20000</td><td style=\"text-align: right;\">  131.7 </td><td style=\"text-align: right;\">                 260</td><td style=\"text-align: right;\">                  20</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_1cc7b_00001:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-41-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 252.0\n",
      "  episode_reward_mean: 144.7\n",
      "  episode_reward_min: 27.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: 0c6578794e374e0ba692e935dfaa465e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5316322445869446\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.023005222901701927\n",
      "          model: {}\n",
      "          policy_loss: -0.10583166033029556\n",
      "          total_loss: 2242.48779296875\n",
      "          vf_explained_var: -4.133870561418007e-07\n",
      "          vf_loss: 2242.558837890625\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.41666666666667\n",
      "    ram_util_percent: 88.1375\n",
      "  pid: 97178\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08207167914612484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08728204497780843\n",
      "    mean_inference_ms: 1.217287364330449\n",
      "    mean_raw_obs_processing_ms: 0.11764958541382925\n",
      "  time_since_restore: 99.03419971466064\n",
      "  time_this_iter_s: 16.432687044143677\n",
      "  time_total_s: 99.03419971466064\n",
      "  timers:\n",
      "    learn_throughput: 386.631\n",
      "    learn_time_ms: 10345.775\n",
      "    load_throughput: 192142.929\n",
      "    load_time_ms: 20.818\n",
      "    sample_throughput: 653.63\n",
      "    sample_time_ms: 6119.671\n",
      "    update_time_ms: 4.43\n",
      "  timestamp: 1627515709\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 1cc7b_00001\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00002:\n",
      "  agent_timesteps_total: 24000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-41-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.0\n",
      "  episode_reward_mean: 135.31\n",
      "  episode_reward_min: 20.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 240\n",
      "  experiment_id: f4f18905240646649c93157709d0a45e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.608590304851532\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01142813079059124\n",
      "          model: {}\n",
      "          policy_loss: -0.03317958489060402\n",
      "          total_loss: 1837.70458984375\n",
      "          vf_explained_var: -3.559743345249444e-05\n",
      "          vf_loss: 1837.7305908203125\n",
      "    num_agent_steps_sampled: 24000\n",
      "    num_agent_steps_trained: 24000\n",
      "    num_steps_sampled: 24000\n",
      "    num_steps_trained: 24000\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 78.9913043478261\n",
      "    ram_util_percent: 88.1391304347826\n",
      "  pid: 97173\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08354315618188939\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08992907922598121\n",
      "    mean_inference_ms: 1.232301193758373\n",
      "    mean_raw_obs_processing_ms: 0.11636662652229575\n",
      "  time_since_restore: 99.4358823299408\n",
      "  time_this_iter_s: 16.536431550979614\n",
      "  time_total_s: 99.4358823299408\n",
      "  timers:\n",
      "    learn_throughput: 387.719\n",
      "    learn_time_ms: 10316.754\n",
      "    load_throughput: 181641.388\n",
      "    load_time_ms: 22.021\n",
      "    sample_throughput: 643.329\n",
      "    sample_time_ms: 6217.659\n",
      "    update_time_ms: 2.907\n",
      "  timestamp: 1627515710\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 24000\n",
      "  training_iteration: 6\n",
      "  trial_id: 1cc7b_00002\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00000:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-42-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.0\n",
      "  episode_reward_mean: 159.45\n",
      "  episode_reward_min: 38.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 280\n",
      "  experiment_id: 32588f95e5924ff18cc497c7b633b851\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.6033374071121216\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017602061852812767\n",
      "          model: {}\n",
      "          policy_loss: -0.022078577429056168\n",
      "          total_loss: 82.56806182861328\n",
      "          vf_explained_var: 0.97269606590271\n",
      "          vf_loss: 82.56339263916016\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.65833333333333\n",
      "    ram_util_percent: 87.99166666666667\n",
      "  pid: 97175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08438013615939695\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0858061699615198\n",
      "    mean_inference_ms: 1.21914826072347\n",
      "    mean_raw_obs_processing_ms: 0.1129931546939736\n",
      "  time_since_restore: 115.87308239936829\n",
      "  time_this_iter_s: 16.864487171173096\n",
      "  time_total_s: 115.87308239936829\n",
      "  timers:\n",
      "    learn_throughput: 385.217\n",
      "    learn_time_ms: 10383.759\n",
      "    load_throughput: 200313.35\n",
      "    load_time_ms: 19.969\n",
      "    sample_throughput: 653.116\n",
      "    sample_time_ms: 6124.482\n",
      "    update_time_ms: 5.658\n",
      "  timestamp: 1627515726\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 1cc7b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.6/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/6.62 GiB heap, 0.0/3.31 GiB objects (0.0/1.0 CPU_group_0_13c0d4dc4b448e67bac04705c4cf0956, 0.0/2.0 CPU_group_13c0d4dc4b448e67bac04705c4cf0956, 0.0/1.0 CPU_group_0_ce6012929ac9623150263bb845cff8e8, 0.0/1.0 CPU_group_1_ce6012929ac9623150263bb845cff8e8, 0.0/2.0 CPU_group_ce6012929ac9623150263bb845cff8e8, 0.0/2.0 CPU_group_359db8288e7c09c622134661ba803f8c, 0.0/1.0 CPU_group_0_359db8288e7c09c622134661ba803f8c, 0.0/1.0 CPU_group_1_359db8288e7c09c622134661ba803f8c, 0.0/1.0 CPU_group_1_13c0d4dc4b448e67bac04705c4cf0956)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_1cc7b_00000</td><td>RUNNING </td><td>192.168.1.21:97175</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">        115.873 </td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  159.45</td><td style=\"text-align: right;\">                 271</td><td style=\"text-align: right;\">                  38</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00001</td><td>RUNNING </td><td>192.168.1.21:97178</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         99.0342</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  144.7 </td><td style=\"text-align: right;\">                 252</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00002</td><td>RUNNING </td><td>192.168.1.21:97173</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         99.4359</td><td style=\"text-align: right;\">24000</td><td style=\"text-align: right;\">  135.31</td><td style=\"text-align: right;\">                 271</td><td style=\"text-align: right;\">                  20</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_1cc7b_00001:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-42-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 252.0\n",
      "  episode_reward_mean: 143.29\n",
      "  episode_reward_min: 27.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 280\n",
      "  experiment_id: 0c6578794e374e0ba692e935dfaa465e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.278125047683716\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5286913514137268\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015559675171971321\n",
      "          model: {}\n",
      "          policy_loss: -0.08302688598632812\n",
      "          total_loss: 2034.644287109375\n",
      "          vf_explained_var: -2.1726854981807264e-07\n",
      "          vf_loss: 2034.69189453125\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.66666666666667\n",
      "    ram_util_percent: 87.9875\n",
      "  pid: 97178\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08222641907512644\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08737513976030765\n",
      "    mean_inference_ms: 1.221256519241655\n",
      "    mean_raw_obs_processing_ms: 0.11781956966053507\n",
      "  time_since_restore: 116.00680637359619\n",
      "  time_this_iter_s: 16.972606658935547\n",
      "  time_total_s: 116.00680637359619\n",
      "  timers:\n",
      "    learn_throughput: 385.322\n",
      "    learn_time_ms: 10380.937\n",
      "    load_throughput: 207558.079\n",
      "    load_time_ms: 19.272\n",
      "    sample_throughput: 650.085\n",
      "    sample_time_ms: 6153.041\n",
      "    update_time_ms: 4.361\n",
      "  timestamp: 1627515726\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 1cc7b_00001\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00002:\n",
      "  agent_timesteps_total: 28000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-42-07\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.0\n",
      "  episode_reward_mean: 132.56\n",
      "  episode_reward_min: 22.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 280\n",
      "  experiment_id: f4f18905240646649c93157709d0a45e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5932918787002563\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010845103301107883\n",
      "          model: {}\n",
      "          policy_loss: -0.03289112076163292\n",
      "          total_loss: 1784.3184814453125\n",
      "          vf_explained_var: -2.005407804972492e-05\n",
      "          vf_loss: 1784.343994140625\n",
      "    num_agent_steps_sampled: 28000\n",
      "    num_agent_steps_trained: 28000\n",
      "    num_steps_sampled: 28000\n",
      "    num_steps_trained: 28000\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.91199999999999\n",
      "    ram_util_percent: 87.99199999999999\n",
      "  pid: 97173\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08401906951021378\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09046067981807283\n",
      "    mean_inference_ms: 1.2423456176929608\n",
      "    mean_raw_obs_processing_ms: 0.11733656502766239\n",
      "  time_since_restore: 116.43417310714722\n",
      "  time_this_iter_s: 16.99829077720642\n",
      "  time_total_s: 116.43417310714722\n",
      "  timers:\n",
      "    learn_throughput: 387.653\n",
      "    learn_time_ms: 10318.506\n",
      "    load_throughput: 193788.23\n",
      "    load_time_ms: 20.641\n",
      "    sample_throughput: 637.063\n",
      "    sample_time_ms: 6278.812\n",
      "    update_time_ms: 2.995\n",
      "  timestamp: 1627515727\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 28000\n",
      "  training_iteration: 7\n",
      "  trial_id: 1cc7b_00002\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00000:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-42-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 278.0\n",
      "  episode_reward_mean: 135.39\n",
      "  episode_reward_min: 37.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 320\n",
      "  experiment_id: 32588f95e5924ff18cc497c7b633b851\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5939368009567261\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012751136906445026\n",
      "          model: {}\n",
      "          policy_loss: -0.020568257197737694\n",
      "          total_loss: 52.360443115234375\n",
      "          vf_explained_var: 0.978903591632843\n",
      "          vf_loss: 52.36164093017578\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.46666666666667\n",
      "    ram_util_percent: 88.07083333333333\n",
      "  pid: 97175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.0846592550787631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08616315153626215\n",
      "    mean_inference_ms: 1.2224607349133334\n",
      "    mean_raw_obs_processing_ms: 0.11327592646575582\n",
      "  time_since_restore: 132.6270318031311\n",
      "  time_this_iter_s: 16.753949403762817\n",
      "  time_total_s: 132.6270318031311\n",
      "  timers:\n",
      "    learn_throughput: 384.654\n",
      "    learn_time_ms: 10398.95\n",
      "    load_throughput: 218528.737\n",
      "    load_time_ms: 18.304\n",
      "    sample_throughput: 651.813\n",
      "    sample_time_ms: 6136.725\n",
      "    update_time_ms: 5.968\n",
      "  timestamp: 1627515743\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 1cc7b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/6.62 GiB heap, 0.0/3.31 GiB objects (0.0/1.0 CPU_group_0_ce6012929ac9623150263bb845cff8e8, 0.0/2.0 CPU_group_13c0d4dc4b448e67bac04705c4cf0956, 0.0/1.0 CPU_group_0_359db8288e7c09c622134661ba803f8c, 0.0/1.0 CPU_group_0_13c0d4dc4b448e67bac04705c4cf0956, 0.0/1.0 CPU_group_1_ce6012929ac9623150263bb845cff8e8, 0.0/2.0 CPU_group_359db8288e7c09c622134661ba803f8c, 0.0/2.0 CPU_group_ce6012929ac9623150263bb845cff8e8, 0.0/1.0 CPU_group_1_13c0d4dc4b448e67bac04705c4cf0956, 0.0/1.0 CPU_group_1_359db8288e7c09c622134661ba803f8c)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_1cc7b_00000</td><td>RUNNING </td><td>192.168.1.21:97175</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         132.627</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  135.39</td><td style=\"text-align: right;\">                 278</td><td style=\"text-align: right;\">                  37</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00001</td><td>RUNNING </td><td>192.168.1.21:97178</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         116.007</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  143.29</td><td style=\"text-align: right;\">                 252</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00002</td><td>RUNNING </td><td>192.168.1.21:97173</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         116.434</td><td style=\"text-align: right;\">28000</td><td style=\"text-align: right;\">  132.56</td><td style=\"text-align: right;\">                 271</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_1cc7b_00001:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-42-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 252.0\n",
      "  episode_reward_mean: 135.02\n",
      "  episode_reward_min: 27.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 320\n",
      "  experiment_id: 0c6578794e374e0ba692e935dfaa465e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.278125047683716\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5110994577407837\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016471143811941147\n",
      "          model: {}\n",
      "          policy_loss: -0.08449587970972061\n",
      "          total_loss: 2091.235595703125\n",
      "          vf_explained_var: -1.0190471755322505e-07\n",
      "          vf_loss: 2091.282470703125\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.44166666666668\n",
      "    ram_util_percent: 88.07083333333333\n",
      "  pid: 97178\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08244897682315303\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08753380727191715\n",
      "    mean_inference_ms: 1.2256411019835722\n",
      "    mean_raw_obs_processing_ms: 0.11798282127900073\n",
      "  time_since_restore: 132.71760630607605\n",
      "  time_this_iter_s: 16.71079993247986\n",
      "  time_total_s: 132.71760630607605\n",
      "  timers:\n",
      "    learn_throughput: 385.398\n",
      "    learn_time_ms: 10378.886\n",
      "    load_throughput: 228417.605\n",
      "    load_time_ms: 17.512\n",
      "    sample_throughput: 648.249\n",
      "    sample_time_ms: 6170.468\n",
      "    update_time_ms: 4.299\n",
      "  timestamp: 1627515743\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 1cc7b_00001\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00002:\n",
      "  agent_timesteps_total: 32000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-42-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 271.0\n",
      "  episode_reward_mean: 135.95\n",
      "  episode_reward_min: 26.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 320\n",
      "  experiment_id: f4f18905240646649c93157709d0a45e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5898227095603943\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0131232850253582\n",
      "          model: {}\n",
      "          policy_loss: -0.03480513021349907\n",
      "          total_loss: 1763.1055908203125\n",
      "          vf_explained_var: -1.0600013411021791e-05\n",
      "          vf_loss: 1763.1317138671875\n",
      "    num_agent_steps_sampled: 32000\n",
      "    num_agent_steps_trained: 32000\n",
      "    num_steps_sampled: 32000\n",
      "    num_steps_trained: 32000\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 79.58749999999999\n",
      "    ram_util_percent: 88.06666666666666\n",
      "  pid: 97173\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08440089878316542\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09090781456121431\n",
      "    mean_inference_ms: 1.2504618892593369\n",
      "    mean_raw_obs_processing_ms: 0.11807921545529451\n",
      "  time_since_restore: 133.1472988128662\n",
      "  time_this_iter_s: 16.713125705718994\n",
      "  time_total_s: 133.1472988128662\n",
      "  timers:\n",
      "    learn_throughput: 387.95\n",
      "    learn_time_ms: 10310.611\n",
      "    load_throughput: 209199.067\n",
      "    load_time_ms: 19.121\n",
      "    sample_throughput: 635.05\n",
      "    sample_time_ms: 6298.72\n",
      "    update_time_ms: 2.973\n",
      "  timestamp: 1627515743\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 32000\n",
      "  training_iteration: 8\n",
      "  trial_id: 1cc7b_00002\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00000:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-42-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 278.0\n",
      "  episode_reward_mean: 140.22\n",
      "  episode_reward_min: 37.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 360\n",
      "  experiment_id: 32588f95e5924ff18cc497c7b633b851\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5763739347457886\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012534987181425095\n",
      "          model: {}\n",
      "          policy_loss: -0.015719471499323845\n",
      "          total_loss: 50.682315826416016\n",
      "          vf_explained_var: 0.9820374250411987\n",
      "          vf_loss: 50.67898941040039\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.7423076923077\n",
      "    ram_util_percent: 87.96538461538464\n",
      "  pid: 97175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08547777233843329\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08713841121121359\n",
      "    mean_inference_ms: 1.2354205633921926\n",
      "    mean_raw_obs_processing_ms: 0.11417304169863092\n",
      "  time_since_restore: 150.5659692287445\n",
      "  time_this_iter_s: 17.938937425613403\n",
      "  time_total_s: 150.5659692287445\n",
      "  timers:\n",
      "    learn_throughput: 383.912\n",
      "    learn_time_ms: 10419.048\n",
      "    load_throughput: 234779.182\n",
      "    load_time_ms: 17.037\n",
      "    sample_throughput: 637.999\n",
      "    sample_time_ms: 6269.605\n",
      "    update_time_ms: 6.138\n",
      "  timestamp: 1627515761\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 1cc7b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.5/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 6.0/8 CPUs, 0/0 GPUs, 0.0/6.62 GiB heap, 0.0/3.31 GiB objects (0.0/2.0 CPU_group_359db8288e7c09c622134661ba803f8c, 0.0/1.0 CPU_group_1_13c0d4dc4b448e67bac04705c4cf0956, 0.0/1.0 CPU_group_1_ce6012929ac9623150263bb845cff8e8, 0.0/2.0 CPU_group_13c0d4dc4b448e67bac04705c4cf0956, 0.0/1.0 CPU_group_0_13c0d4dc4b448e67bac04705c4cf0956, 0.0/1.0 CPU_group_0_ce6012929ac9623150263bb845cff8e8, 0.0/2.0 CPU_group_ce6012929ac9623150263bb845cff8e8, 0.0/1.0 CPU_group_1_359db8288e7c09c622134661ba803f8c, 0.0/1.0 CPU_group_0_359db8288e7c09c622134661ba803f8c)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 3/3 (3 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_1cc7b_00000</td><td>RUNNING </td><td>192.168.1.21:97175</td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         150.566</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  140.22</td><td style=\"text-align: right;\">                 278</td><td style=\"text-align: right;\">                  37</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00001</td><td>RUNNING </td><td>192.168.1.21:97178</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         132.718</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  135.02</td><td style=\"text-align: right;\">                 252</td><td style=\"text-align: right;\">                  27</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00002</td><td>RUNNING </td><td>192.168.1.21:97173</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         133.147</td><td style=\"text-align: right;\">32000</td><td style=\"text-align: right;\">  135.95</td><td style=\"text-align: right;\">                 271</td><td style=\"text-align: right;\">                  26</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_1cc7b_00001:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-42-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 255.0\n",
      "  episode_reward_mean: 129.7\n",
      "  episode_reward_min: 26.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 360\n",
      "  experiment_id: 0c6578794e374e0ba692e935dfaa465e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.278125047683716\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.5103259682655334\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015000885352492332\n",
      "          model: {}\n",
      "          policy_loss: -0.08032947033643723\n",
      "          total_loss: 2125.677734375\n",
      "          vf_explained_var: -7.690921677294682e-08\n",
      "          vf_loss: 2125.723876953125\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.74615384615385\n",
      "    ram_util_percent: 87.96923076923078\n",
      "  pid: 97178\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08331355428656925\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08821115432818746\n",
      "    mean_inference_ms: 1.2402091975454823\n",
      "    mean_raw_obs_processing_ms: 0.11872890202377243\n",
      "  time_since_restore: 150.7639274597168\n",
      "  time_this_iter_s: 18.046321153640747\n",
      "  time_total_s: 150.7639274597168\n",
      "  timers:\n",
      "    learn_throughput: 384.435\n",
      "    learn_time_ms: 10404.87\n",
      "    load_throughput: 243243.24\n",
      "    load_time_ms: 16.444\n",
      "    sample_throughput: 634.084\n",
      "    sample_time_ms: 6308.314\n",
      "    update_time_ms: 4.45\n",
      "  timestamp: 1627515761\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 1cc7b_00001\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00002:\n",
      "  agent_timesteps_total: 36000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-42-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 254.0\n",
      "  episode_reward_mean: 141.27\n",
      "  episode_reward_min: 21.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 360\n",
      "  experiment_id: f4f18905240646649c93157709d0a45e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.573863685131073\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013669835403561592\n",
      "          model: {}\n",
      "          policy_loss: -0.03282391279935837\n",
      "          total_loss: 1841.537841796875\n",
      "          vf_explained_var: -5.150995093572419e-06\n",
      "          vf_loss: 1841.561767578125\n",
      "    num_agent_steps_sampled: 36000\n",
      "    num_agent_steps_trained: 36000\n",
      "    num_steps_sampled: 36000\n",
      "    num_steps_trained: 36000\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 81.468\n",
      "    ram_util_percent: 87.96400000000001\n",
      "  pid: 97173\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08500747143858468\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09169748815659752\n",
      "    mean_inference_ms: 1.2668495071948855\n",
      "    mean_raw_obs_processing_ms: 0.11918850537400438\n",
      "  time_since_restore: 151.17100715637207\n",
      "  time_this_iter_s: 18.02370834350586\n",
      "  time_total_s: 151.17100715637207\n",
      "  timers:\n",
      "    learn_throughput: 387.568\n",
      "    learn_time_ms: 10320.781\n",
      "    load_throughput: 173907.821\n",
      "    load_time_ms: 23.001\n",
      "    sample_throughput: 621.353\n",
      "    sample_time_ms: 6437.57\n",
      "    update_time_ms: 3.221\n",
      "  timestamp: 1627515761\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 36000\n",
      "  training_iteration: 9\n",
      "  trial_id: 1cc7b_00002\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00000:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-42-58\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 277.0\n",
      "  episode_reward_mean: 147.97\n",
      "  episode_reward_min: 33.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 400\n",
      "  experiment_id: 32588f95e5924ff18cc497c7b633b851\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.5187499523162842\n",
      "          cur_lr: 0.009999999776482582\n",
      "          entropy: 0.5832011699676514\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012146069668233395\n",
      "          model: {}\n",
      "          policy_loss: -0.01367176417261362\n",
      "          total_loss: 51.35264205932617\n",
      "          vf_explained_var: 0.9809431433677673\n",
      "          vf_loss: 51.34786605834961\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.20416666666667\n",
      "    ram_util_percent: 88.05833333333332\n",
      "  pid: 97175\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08635188356952984\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.08816812709490635\n",
      "    mean_inference_ms: 1.2511488773787844\n",
      "    mean_raw_obs_processing_ms: 0.11520356330016018\n",
      "  time_since_restore: 167.7789556980133\n",
      "  time_this_iter_s: 17.2129864692688\n",
      "  time_total_s: 167.7789556980133\n",
      "  timers:\n",
      "    learn_throughput: 383.901\n",
      "    learn_time_ms: 10419.365\n",
      "    load_throughput: 249291.837\n",
      "    load_time_ms: 16.045\n",
      "    sample_throughput: 632.962\n",
      "    sample_time_ms: 6319.499\n",
      "    update_time_ms: 5.973\n",
      "  timestamp: 1627515778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 1cc7b_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/6.62 GiB heap, 0.0/3.31 GiB objects (0.0/1.0 CPU_group_0_ce6012929ac9623150263bb845cff8e8, 0.0/2.0 CPU_group_ce6012929ac9623150263bb845cff8e8, 0.0/1.0 CPU_group_1_359db8288e7c09c622134661ba803f8c, 0.0/2.0 CPU_group_13c0d4dc4b448e67bac04705c4cf0956, 0.0/1.0 CPU_group_0_359db8288e7c09c622134661ba803f8c, 0.0/1.0 CPU_group_1_ce6012929ac9623150263bb845cff8e8, 0.0/1.0 CPU_group_1_13c0d4dc4b448e67bac04705c4cf0956, 0.0/2.0 CPU_group_359db8288e7c09c622134661ba803f8c, 0.0/1.0 CPU_group_0_13c0d4dc4b448e67bac04705c4cf0956)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 3/3 (2 RUNNING, 1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc               </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_1cc7b_00001</td><td>RUNNING   </td><td>192.168.1.21:97178</td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         150.764</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  129.7 </td><td style=\"text-align: right;\">                 255</td><td style=\"text-align: right;\">                  26</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00002</td><td>RUNNING   </td><td>192.168.1.21:97173</td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         151.171</td><td style=\"text-align: right;\">36000</td><td style=\"text-align: right;\">  141.27</td><td style=\"text-align: right;\">                 254</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00000</td><td>TERMINATED</td><td>                  </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         167.779</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  147.97</td><td style=\"text-align: right;\">                 277</td><td style=\"text-align: right;\">                  33</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_MG_env_1cc7b_00001:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-42-58\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 255.0\n",
      "  episode_reward_mean: 138.33\n",
      "  episode_reward_min: 22.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 400\n",
      "  experiment_id: 0c6578794e374e0ba692e935dfaa465e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 2.278125047683716\n",
      "          cur_lr: 0.0010000000474974513\n",
      "          entropy: 0.4931609332561493\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015556617639958858\n",
      "          model: {}\n",
      "          policy_loss: -0.07537218183279037\n",
      "          total_loss: 2203.83251953125\n",
      "          vf_explained_var: -4.999099090241543e-08\n",
      "          vf_loss: 2203.87255859375\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.648\n",
      "    ram_util_percent: 87.97599999999998\n",
      "  pid: 97178\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08424607782695925\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0889826478346059\n",
      "    mean_inference_ms: 1.256298724845169\n",
      "    mean_raw_obs_processing_ms: 0.11967491284483177\n",
      "  time_since_restore: 168.02138447761536\n",
      "  time_this_iter_s: 17.25745701789856\n",
      "  time_total_s: 168.02138447761536\n",
      "  timers:\n",
      "    learn_throughput: 384.201\n",
      "    learn_time_ms: 10411.216\n",
      "    load_throughput: 259513.988\n",
      "    load_time_ms: 15.413\n",
      "    sample_throughput: 629.548\n",
      "    sample_time_ms: 6353.769\n",
      "    update_time_ms: 4.722\n",
      "  timestamp: 1627515778\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 1cc7b_00001\n",
      "  \n",
      "Result for PPO_MG_env_1cc7b_00002:\n",
      "  agent_timesteps_total: 40000\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-29_11-42-59\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 254.0\n",
      "  episode_reward_mean: 134.84\n",
      "  episode_reward_min: 21.0\n",
      "  episodes_this_iter: 40\n",
      "  episodes_total: 400\n",
      "  experiment_id: f4f18905240646649c93157709d0a45e\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      default_policy:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 9.999999747378752e-05\n",
      "          entropy: 0.5714896321296692\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01588440127670765\n",
      "          model: {}\n",
      "          policy_loss: -0.03913355991244316\n",
      "          total_loss: 1755.579345703125\n",
      "          vf_explained_var: -3.0340686407726025e-06\n",
      "          vf_loss: 1755.6080322265625\n",
      "    num_agent_steps_sampled: 40000\n",
      "    num_agent_steps_trained: 40000\n",
      "    num_steps_sampled: 40000\n",
      "    num_steps_trained: 40000\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 1\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.63600000000001\n",
      "    ram_util_percent: 87.99199999999999\n",
      "  pid: 97173\n",
      "  policy_reward_max: {}\n",
      "  policy_reward_mean: {}\n",
      "  policy_reward_min: {}\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.08571114566777994\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09249538925255925\n",
      "    mean_inference_ms: 1.2834583842964435\n",
      "    mean_raw_obs_processing_ms: 0.12015514663740064\n",
      "  time_since_restore: 168.34006595611572\n",
      "  time_this_iter_s: 17.169058799743652\n",
      "  time_total_s: 168.34006595611572\n",
      "  timers:\n",
      "    learn_throughput: 387.712\n",
      "    learn_time_ms: 10316.932\n",
      "    load_throughput: 185911.344\n",
      "    load_time_ms: 21.516\n",
      "    sample_throughput: 617.234\n",
      "    sample_time_ms: 6480.527\n",
      "    update_time_ms: 3.169\n",
      "  timestamp: 1627515779\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 40000\n",
      "  training_iteration: 10\n",
      "  trial_id: 1cc7b_00002\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.2/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/6.62 GiB heap, 0.0/3.31 GiB objects (0.0/1.0 CPU_group_0_ce6012929ac9623150263bb845cff8e8, 0.0/2.0 CPU_group_ce6012929ac9623150263bb845cff8e8, 0.0/2.0 CPU_group_13c0d4dc4b448e67bac04705c4cf0956, 0.0/1.0 CPU_group_0_359db8288e7c09c622134661ba803f8c, 0.0/1.0 CPU_group_1_359db8288e7c09c622134661ba803f8c, 0.0/2.0 CPU_group_359db8288e7c09c622134661ba803f8c, 0.0/1.0 CPU_group_0_13c0d4dc4b448e67bac04705c4cf0956, 0.0/1.0 CPU_group_1_ce6012929ac9623150263bb845cff8e8, 0.0/1.0 CPU_group_1_13c0d4dc4b448e67bac04705c4cf0956)<br>Result logdir: /home/peter/ray_results/PPO<br>Number of trials: 3/3 (3 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">    lr</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_MG_env_1cc7b_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.01  </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         167.779</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  147.97</td><td style=\"text-align: right;\">                 277</td><td style=\"text-align: right;\">                  33</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00001</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.001 </td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         168.021</td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  138.33</td><td style=\"text-align: right;\">                 255</td><td style=\"text-align: right;\">                  22</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "<tr><td>PPO_MG_env_1cc7b_00002</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">0.0001</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         168.34 </td><td style=\"text-align: right;\">40000</td><td style=\"text-align: right;\">  134.84</td><td style=\"text-align: right;\">                 254</td><td style=\"text-align: right;\">                  21</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-29 11:42:59,683\tINFO tune.py:549 -- Total run time: 184.33 seconds (183.78 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f6bc9b23d30>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune.run(\n",
    "    \"PPO\",\n",
    "    stop={\"training_iteration\":10},\n",
    "    config={\n",
    "        \"env\": \"MG_env\",\n",
    "        \"num_gpus\": 0,\n",
    "        \"num_workers\": 1,\n",
    "        \"lr\": tune.grid_search([0.01, 0.001, 0.0001]),\\\\\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_dqn = DEFAULT_CONFIG_DQN.copy()\n",
    "trainer_config_dqn['num_workers'] = 3\n",
    "trainer_config_dqn['n_step'] = 3\n",
    "trainer_config_dqn['noisy'] = True\n",
    "trainer_config_dqn['v_min'] = -10.\n",
    "trainer_config_dqn['v_max'] = 10.\n",
    "# trainer_config_dqn['num_atoms'] = 51\n",
    "\n",
    "# trainer_config_dqn['model']['fcnet_hiddens'] = [1024, 512,512, 256,256,32,8]\n",
    "trainer_config_dqn['model']['fcnet_hiddens'] = tune.grid_search([[256,256,32,8],[8]])\n",
    "# trainer_config_dqn['lr'] = tune.grid_search([0.1,0.01,0.001,0.0001,0.00001, 0.000001])\n",
    "# trainer_config_dqn['lr'] = tune.grid_search([0.1,0.0001, 0.000001])\n",
    "\n",
    "trainer_config_dqn['env'] = 'MG_env'\n",
    "# trainer_config_dqn['num_cpus_per_worker'] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 8.4/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/5.77 GiB heap, 0.0/2.88 GiB objects (0.0/1.0 CPU_group_2_772b2999f355f9847d5c651893ad1bab, 0.0/1.0 CPU_group_3_72ada1f747d30be8c2aad7e59fdce215, 0.0/1.0 CPU_group_3_772b2999f355f9847d5c651893ad1bab, 0.0/1.0 CPU_group_1_72ada1f747d30be8c2aad7e59fdce215, 0.0/1.0 CPU_group_0_772b2999f355f9847d5c651893ad1bab, 0.0/4.0 CPU_group_72ada1f747d30be8c2aad7e59fdce215, 0.0/1.0 CPU_group_0_72ada1f747d30be8c2aad7e59fdce215, 0.0/4.0 CPU_group_772b2999f355f9847d5c651893ad1bab, 0.0/1.0 CPU_group_1_772b2999f355f9847d5c651893ad1bab, 0.0/1.0 CPU_group_2_72ada1f747d30be8c2aad7e59fdce215)<br>Result logdir: /home/peter/ray_results/DQN<br>Number of trials: 2/2 (2 TERMINATED)<br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-30 20:52:04,928\tINFO tune.py:549 -- Total run time: 26.72 seconds (26.53 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "ray.init(ignore_reinit_error=True)\n",
    "results = tune.run(\n",
    "    \"DQN\",\n",
    "    stop={\"training_iteration\":2},\n",
    "    \n",
    "    config=trainer_config_dqn,\n",
    "#     config={\n",
    "# #         \"num_gpus\": 0,\n",
    "# #         \"num_workers\": 1,\n",
    "# #         \"lr\": tune.grid_search([0.01, 0.001, 0.0001]),\n",
    "#         \"config\":trainer_config_ppo\n",
    "#     },\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'5a081_00000': {'episode_reward_max': 245.0,\n",
       "  'episode_reward_min': 36.0,\n",
       "  'episode_reward_mean': 133.72222222222223,\n",
       "  'episode_len_mean': 100.0,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 9,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [41.0,\n",
       "    53.0,\n",
       "    47.0,\n",
       "    245.0,\n",
       "    230.0,\n",
       "    52.0,\n",
       "    219.0,\n",
       "    232.0,\n",
       "    36.0,\n",
       "    49.0,\n",
       "    55.0,\n",
       "    61.0,\n",
       "    58.0,\n",
       "    216.0,\n",
       "    201.0,\n",
       "    206.0,\n",
       "    208.0,\n",
       "    198.0],\n",
       "   'episode_lengths': [100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.23427919125765545,\n",
       "   'mean_inference_ms': 1.9808622579671733,\n",
       "   'mean_action_processing_ms': 0.09929705463455807,\n",
       "   'mean_env_wait_ms': 0.09874352527845993,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'off_policy_estimator': {},\n",
       "  'num_healthy_workers': 3,\n",
       "  'timesteps_total': 2016,\n",
       "  'agent_timesteps_total': 2016,\n",
       "  'timers': {'load_time_ms': 0.829,\n",
       "   'load_throughput': 38618.25,\n",
       "   'learn_time_ms': 3.668,\n",
       "   'learn_throughput': 8724.161,\n",
       "   'update_time_ms': 3.686},\n",
       "  'info': {'learner': {'default_policy': {'learner_stats': {'cur_lr': 0.0005000000237487257,\n",
       "      'mean_q': 2.5701275,\n",
       "      'min_q': 1.2809701,\n",
       "      'max_q': 3.6010046,\n",
       "      'mean_td_error': -1.4836521,\n",
       "      'model': {}},\n",
       "     'td_error': array([ 1.658066  ,  0.6730555 , -0.42961884,  1.5391994 , -3.483173  ,\n",
       "            -2.5030785 , -2.5522313 , -2.503019  ,  0.5963838 , -2.5522313 ,\n",
       "            -3.4813452 , -2.5554142 ,  0.580636  , -6.5973983 ,  1.5284762 ,\n",
       "             1.6202173 ,  1.4451659 , -8.542699  , -6.364059  ,  1.6174824 ,\n",
       "            -3.5376399 , -6.5130568 ,  1.5618777 , -2.516823  ,  0.6878356 ,\n",
       "             0.67943907, -5.5199947 ,  1.601353  ,  0.64856577,  1.5655358 ,\n",
       "            -2.384409  , -3.44397   ], dtype=float32)}},\n",
       "   'num_steps_sampled': 2016,\n",
       "   'num_agent_steps_sampled': 2016,\n",
       "   'num_steps_trained': 2720,\n",
       "   'num_agent_steps_trained': 2720,\n",
       "   'last_target_update_ts': 2016,\n",
       "   'num_target_updates': 3},\n",
       "  'done': True,\n",
       "  'episodes_total': 18,\n",
       "  'training_iteration': 2,\n",
       "  'experiment_id': 'd142ca7837a744c3bfdb8f7a0c86ec00',\n",
       "  'date': '2021-07-30_20-52-04',\n",
       "  'timestamp': 1627635124,\n",
       "  'time_this_iter_s': 2.3526883125305176,\n",
       "  'time_total_s': 4.747518062591553,\n",
       "  'pid': 79083,\n",
       "  'hostname': 'coolo-computer',\n",
       "  'node_ip': '192.168.1.21',\n",
       "  'config': {'num_workers': 3,\n",
       "   'num_envs_per_worker': 1,\n",
       "   'create_env_on_driver': False,\n",
       "   'rollout_fragment_length': 4,\n",
       "   'batch_mode': 'truncate_episodes',\n",
       "   'train_batch_size': 32,\n",
       "   'model': {'_use_default_native_models': False,\n",
       "    'fcnet_hiddens': [256, 256, 32, 8],\n",
       "    'fcnet_activation': 'tanh',\n",
       "    'conv_filters': None,\n",
       "    'conv_activation': 'relu',\n",
       "    'post_fcnet_hiddens': [],\n",
       "    'post_fcnet_activation': 'relu',\n",
       "    'free_log_std': False,\n",
       "    'no_final_linear': False,\n",
       "    'vf_share_layers': True,\n",
       "    'use_lstm': False,\n",
       "    'max_seq_len': 20,\n",
       "    'lstm_cell_size': 256,\n",
       "    'lstm_use_prev_action': False,\n",
       "    'lstm_use_prev_reward': False,\n",
       "    '_time_major': False,\n",
       "    'use_attention': False,\n",
       "    'attention_num_transformer_units': 1,\n",
       "    'attention_dim': 64,\n",
       "    'attention_num_heads': 1,\n",
       "    'attention_head_dim': 32,\n",
       "    'attention_memory_inference': 50,\n",
       "    'attention_memory_training': 50,\n",
       "    'attention_position_wise_mlp_dim': 32,\n",
       "    'attention_init_gru_gate_bias': 2.0,\n",
       "    'attention_use_n_prev_actions': 0,\n",
       "    'attention_use_n_prev_rewards': 0,\n",
       "    'num_framestacks': 'auto',\n",
       "    'dim': 84,\n",
       "    'grayscale': False,\n",
       "    'zero_mean': True,\n",
       "    'custom_model': None,\n",
       "    'custom_model_config': {},\n",
       "    'custom_action_dist': None,\n",
       "    'custom_preprocessor': None,\n",
       "    'lstm_use_prev_action_reward': -1,\n",
       "    'framestack': True},\n",
       "   'optimizer': {},\n",
       "   'gamma': 0.99,\n",
       "   'horizon': None,\n",
       "   'soft_horizon': False,\n",
       "   'no_done_at_end': False,\n",
       "   'env': 'MG_env',\n",
       "   'env_config': {},\n",
       "   'env_task_fn': None,\n",
       "   'render_env': False,\n",
       "   'record_env': False,\n",
       "   'normalize_actions': False,\n",
       "   'clip_rewards': None,\n",
       "   'clip_actions': True,\n",
       "   'preprocessor_pref': 'deepmind',\n",
       "   'lr': 0.0005,\n",
       "   'log_level': 'WARN',\n",
       "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "   'ignore_worker_failures': False,\n",
       "   'log_sys_usage': True,\n",
       "   'fake_sampler': False,\n",
       "   'framework': 'tf',\n",
       "   'eager_tracing': False,\n",
       "   'explore': True,\n",
       "   'exploration_config': {'type': 'EpsilonGreedy',\n",
       "    'initial_epsilon': 1.0,\n",
       "    'final_epsilon': 0.02,\n",
       "    'epsilon_timesteps': 10000},\n",
       "   'evaluation_interval': None,\n",
       "   'evaluation_num_episodes': 10,\n",
       "   'evaluation_parallel_to_training': False,\n",
       "   'in_evaluation': False,\n",
       "   'evaluation_config': {'explore': False},\n",
       "   'evaluation_num_workers': 0,\n",
       "   'custom_eval_function': None,\n",
       "   'sample_async': False,\n",
       "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "   'observation_filter': 'NoFilter',\n",
       "   'synchronize_filters': True,\n",
       "   'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "    'inter_op_parallelism_threads': 2,\n",
       "    'gpu_options': {'allow_growth': True},\n",
       "    'log_device_placement': False,\n",
       "    'device_count': {'CPU': 1},\n",
       "    'allow_soft_placement': True},\n",
       "   'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "    'inter_op_parallelism_threads': 8},\n",
       "   'compress_observations': False,\n",
       "   'collect_metrics_timeout': 180,\n",
       "   'metrics_smoothing_episodes': 100,\n",
       "   'remote_worker_envs': False,\n",
       "   'remote_env_batch_wait_ms': 0,\n",
       "   'min_iter_time_s': 1,\n",
       "   'timesteps_per_iteration': 1000,\n",
       "   'seed': None,\n",
       "   'extra_python_environs_for_driver': {},\n",
       "   'extra_python_environs_for_worker': {},\n",
       "   'num_gpus': 0,\n",
       "   '_fake_gpus': False,\n",
       "   'num_cpus_per_worker': 1,\n",
       "   'num_gpus_per_worker': 0,\n",
       "   'custom_resources_per_worker': {},\n",
       "   'num_cpus_for_driver': 1,\n",
       "   'placement_strategy': 'PACK',\n",
       "   'input': 'sampler',\n",
       "   'input_evaluation': ['is', 'wis'],\n",
       "   'postprocess_inputs': False,\n",
       "   'shuffle_buffer_size': 0,\n",
       "   'output': None,\n",
       "   'output_compress_columns': ['obs', 'new_obs'],\n",
       "   'output_max_file_size': 67108864,\n",
       "   'multiagent': {'policies': {},\n",
       "    'policy_mapping_fn': None,\n",
       "    'policies_to_train': None,\n",
       "    'observation_fn': None,\n",
       "    'replay_mode': 'independent',\n",
       "    'count_steps_by': 'env_steps'},\n",
       "   'logger_config': None,\n",
       "   'simple_optimizer': False,\n",
       "   'monitor': -1,\n",
       "   'num_atoms': 1,\n",
       "   'v_min': -10.0,\n",
       "   'v_max': 10.0,\n",
       "   'noisy': True,\n",
       "   'sigma0': 0.5,\n",
       "   'dueling': True,\n",
       "   'hiddens': [256],\n",
       "   'double_q': True,\n",
       "   'n_step': 3,\n",
       "   'target_network_update_freq': 500,\n",
       "   'buffer_size': 50000,\n",
       "   'replay_sequence_length': 1,\n",
       "   'prioritized_replay': True,\n",
       "   'prioritized_replay_alpha': 0.6,\n",
       "   'prioritized_replay_beta': 0.4,\n",
       "   'final_prioritized_replay_beta': 0.4,\n",
       "   'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       "   'prioritized_replay_eps': 1e-06,\n",
       "   'before_learn_on_batch': None,\n",
       "   'training_intensity': None,\n",
       "   'lr_schedule': None,\n",
       "   'adam_epsilon': 1e-08,\n",
       "   'grad_clip': 40,\n",
       "   'learning_starts': 1000,\n",
       "   'worker_side_prioritization': False},\n",
       "  'time_since_restore': 4.747518062591553,\n",
       "  'timesteps_since_restore': 0,\n",
       "  'iterations_since_restore': 2,\n",
       "  'perf': {'cpu_util_percent': 48.96666666666667,\n",
       "   'ram_util_percent': 55.93333333333334},\n",
       "  'trial_id': '5a081_00000',\n",
       "  'experiment_tag': '0_fcnet_hiddens=[256, 256, 32, 8]'},\n",
       " '5a081_00001': {'episode_reward_max': 238.0,\n",
       "  'episode_reward_min': 46.0,\n",
       "  'episode_reward_mean': 144.94444444444446,\n",
       "  'episode_len_mean': 100.0,\n",
       "  'episode_media': {},\n",
       "  'episodes_this_iter': 9,\n",
       "  'policy_reward_min': {},\n",
       "  'policy_reward_max': {},\n",
       "  'policy_reward_mean': {},\n",
       "  'custom_metrics': {},\n",
       "  'hist_stats': {'episode_reward': [238.0,\n",
       "    47.0,\n",
       "    207.0,\n",
       "    218.0,\n",
       "    230.0,\n",
       "    52.0,\n",
       "    48.0,\n",
       "    46.0,\n",
       "    232.0,\n",
       "    237.0,\n",
       "    222.0,\n",
       "    47.0,\n",
       "    227.0,\n",
       "    55.0,\n",
       "    54.0,\n",
       "    60.0,\n",
       "    196.0,\n",
       "    193.0],\n",
       "   'episode_lengths': [100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100,\n",
       "    100]},\n",
       "  'sampler_perf': {'mean_raw_obs_processing_ms': 0.21988745673255103,\n",
       "   'mean_inference_ms': 1.5196674610705283,\n",
       "   'mean_action_processing_ms': 0.0895798841706451,\n",
       "   'mean_env_wait_ms': 0.09128475626237703,\n",
       "   'mean_env_render_ms': 0.0},\n",
       "  'off_policy_estimator': {},\n",
       "  'num_healthy_workers': 3,\n",
       "  'timesteps_total': 2016,\n",
       "  'agent_timesteps_total': 2016,\n",
       "  'timers': {'load_time_ms': 0.866,\n",
       "   'load_throughput': 36933.882,\n",
       "   'learn_time_ms': 2.938,\n",
       "   'learn_throughput': 10892.439,\n",
       "   'update_time_ms': 3.219},\n",
       "  'info': {'learner': {'default_policy': {'learner_stats': {'cur_lr': 0.0005000000237487257,\n",
       "      'mean_q': 2.9332976,\n",
       "      'min_q': 0.7278897,\n",
       "      'max_q': 4.0850973,\n",
       "      'mean_td_error': -2.1697788,\n",
       "      'model': {}},\n",
       "     'td_error': array([ 1.9985614 ,  0.78112984, -1.935777  ,  1.2121544 , -4.8102565 ,\n",
       "            -1.5664899 , -4.799619  , -2.459191  , -2.9081137 , -6.4613466 ,\n",
       "             1.049478  , -6.678409  ,  1.1788324 , -8.777251  , -2.7366984 ,\n",
       "            -2.5034714 , -6.977104  , -4.6211557 ,  1.791975  ,  0.8552329 ,\n",
       "             1.1682297 ,  0.5704193 , -1.9016962 , -6.9770565 ,  1.7509837 ,\n",
       "            -6.8352604 , -4.3642907 ,  0.09310865,  1.4760082 , -2.0127828 ,\n",
       "             1.3086677 , -5.3417387 ], dtype=float32)}},\n",
       "   'num_steps_sampled': 2016,\n",
       "   'num_agent_steps_sampled': 2016,\n",
       "   'num_steps_trained': 2720,\n",
       "   'num_agent_steps_trained': 2720,\n",
       "   'last_target_update_ts': 2016,\n",
       "   'num_target_updates': 3},\n",
       "  'done': True,\n",
       "  'episodes_total': 18,\n",
       "  'training_iteration': 2,\n",
       "  'experiment_id': 'd6e0555473fb4b24a1080993c1c30215',\n",
       "  'date': '2021-07-30_20-52-01',\n",
       "  'timestamp': 1627635121,\n",
       "  'time_this_iter_s': 2.2616286277770996,\n",
       "  'time_total_s': 3.9536099433898926,\n",
       "  'pid': 79079,\n",
       "  'hostname': 'coolo-computer',\n",
       "  'node_ip': '192.168.1.21',\n",
       "  'config': {'num_workers': 3,\n",
       "   'num_envs_per_worker': 1,\n",
       "   'create_env_on_driver': False,\n",
       "   'rollout_fragment_length': 4,\n",
       "   'batch_mode': 'truncate_episodes',\n",
       "   'train_batch_size': 32,\n",
       "   'model': {'_use_default_native_models': False,\n",
       "    'fcnet_hiddens': [8],\n",
       "    'fcnet_activation': 'tanh',\n",
       "    'conv_filters': None,\n",
       "    'conv_activation': 'relu',\n",
       "    'post_fcnet_hiddens': [],\n",
       "    'post_fcnet_activation': 'relu',\n",
       "    'free_log_std': False,\n",
       "    'no_final_linear': False,\n",
       "    'vf_share_layers': True,\n",
       "    'use_lstm': False,\n",
       "    'max_seq_len': 20,\n",
       "    'lstm_cell_size': 256,\n",
       "    'lstm_use_prev_action': False,\n",
       "    'lstm_use_prev_reward': False,\n",
       "    '_time_major': False,\n",
       "    'use_attention': False,\n",
       "    'attention_num_transformer_units': 1,\n",
       "    'attention_dim': 64,\n",
       "    'attention_num_heads': 1,\n",
       "    'attention_head_dim': 32,\n",
       "    'attention_memory_inference': 50,\n",
       "    'attention_memory_training': 50,\n",
       "    'attention_position_wise_mlp_dim': 32,\n",
       "    'attention_init_gru_gate_bias': 2.0,\n",
       "    'attention_use_n_prev_actions': 0,\n",
       "    'attention_use_n_prev_rewards': 0,\n",
       "    'num_framestacks': 'auto',\n",
       "    'dim': 84,\n",
       "    'grayscale': False,\n",
       "    'zero_mean': True,\n",
       "    'custom_model': None,\n",
       "    'custom_model_config': {},\n",
       "    'custom_action_dist': None,\n",
       "    'custom_preprocessor': None,\n",
       "    'lstm_use_prev_action_reward': -1,\n",
       "    'framestack': True},\n",
       "   'optimizer': {},\n",
       "   'gamma': 0.99,\n",
       "   'horizon': None,\n",
       "   'soft_horizon': False,\n",
       "   'no_done_at_end': False,\n",
       "   'env': 'MG_env',\n",
       "   'env_config': {},\n",
       "   'env_task_fn': None,\n",
       "   'render_env': False,\n",
       "   'record_env': False,\n",
       "   'normalize_actions': False,\n",
       "   'clip_rewards': None,\n",
       "   'clip_actions': True,\n",
       "   'preprocessor_pref': 'deepmind',\n",
       "   'lr': 0.0005,\n",
       "   'log_level': 'WARN',\n",
       "   'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "   'ignore_worker_failures': False,\n",
       "   'log_sys_usage': True,\n",
       "   'fake_sampler': False,\n",
       "   'framework': 'tf',\n",
       "   'eager_tracing': False,\n",
       "   'explore': True,\n",
       "   'exploration_config': {'type': 'EpsilonGreedy',\n",
       "    'initial_epsilon': 1.0,\n",
       "    'final_epsilon': 0.02,\n",
       "    'epsilon_timesteps': 10000},\n",
       "   'evaluation_interval': None,\n",
       "   'evaluation_num_episodes': 10,\n",
       "   'evaluation_parallel_to_training': False,\n",
       "   'in_evaluation': False,\n",
       "   'evaluation_config': {'explore': False},\n",
       "   'evaluation_num_workers': 0,\n",
       "   'custom_eval_function': None,\n",
       "   'sample_async': False,\n",
       "   'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "   'observation_filter': 'NoFilter',\n",
       "   'synchronize_filters': True,\n",
       "   'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "    'inter_op_parallelism_threads': 2,\n",
       "    'gpu_options': {'allow_growth': True},\n",
       "    'log_device_placement': False,\n",
       "    'device_count': {'CPU': 1},\n",
       "    'allow_soft_placement': True},\n",
       "   'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "    'inter_op_parallelism_threads': 8},\n",
       "   'compress_observations': False,\n",
       "   'collect_metrics_timeout': 180,\n",
       "   'metrics_smoothing_episodes': 100,\n",
       "   'remote_worker_envs': False,\n",
       "   'remote_env_batch_wait_ms': 0,\n",
       "   'min_iter_time_s': 1,\n",
       "   'timesteps_per_iteration': 1000,\n",
       "   'seed': None,\n",
       "   'extra_python_environs_for_driver': {},\n",
       "   'extra_python_environs_for_worker': {},\n",
       "   'num_gpus': 0,\n",
       "   '_fake_gpus': False,\n",
       "   'num_cpus_per_worker': 1,\n",
       "   'num_gpus_per_worker': 0,\n",
       "   'custom_resources_per_worker': {},\n",
       "   'num_cpus_for_driver': 1,\n",
       "   'placement_strategy': 'PACK',\n",
       "   'input': 'sampler',\n",
       "   'input_evaluation': ['is', 'wis'],\n",
       "   'postprocess_inputs': False,\n",
       "   'shuffle_buffer_size': 0,\n",
       "   'output': None,\n",
       "   'output_compress_columns': ['obs', 'new_obs'],\n",
       "   'output_max_file_size': 67108864,\n",
       "   'multiagent': {'policies': {},\n",
       "    'policy_mapping_fn': None,\n",
       "    'policies_to_train': None,\n",
       "    'observation_fn': None,\n",
       "    'replay_mode': 'independent',\n",
       "    'count_steps_by': 'env_steps'},\n",
       "   'logger_config': None,\n",
       "   'simple_optimizer': False,\n",
       "   'monitor': -1,\n",
       "   'num_atoms': 1,\n",
       "   'v_min': -10.0,\n",
       "   'v_max': 10.0,\n",
       "   'noisy': True,\n",
       "   'sigma0': 0.5,\n",
       "   'dueling': True,\n",
       "   'hiddens': [256],\n",
       "   'double_q': True,\n",
       "   'n_step': 3,\n",
       "   'target_network_update_freq': 500,\n",
       "   'buffer_size': 50000,\n",
       "   'replay_sequence_length': 1,\n",
       "   'prioritized_replay': True,\n",
       "   'prioritized_replay_alpha': 0.6,\n",
       "   'prioritized_replay_beta': 0.4,\n",
       "   'final_prioritized_replay_beta': 0.4,\n",
       "   'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       "   'prioritized_replay_eps': 1e-06,\n",
       "   'before_learn_on_batch': None,\n",
       "   'training_intensity': None,\n",
       "   'lr_schedule': None,\n",
       "   'adam_epsilon': 1e-08,\n",
       "   'grad_clip': 40,\n",
       "   'learning_starts': 1000,\n",
       "   'worker_side_prioritization': False},\n",
       "  'time_since_restore': 3.9536099433898926,\n",
       "  'timesteps_since_restore': 0,\n",
       "  'iterations_since_restore': 2,\n",
       "  'perf': {'cpu_util_percent': 62.93333333333334, 'ram_util_percent': 62.4},\n",
       "  'trial_id': '5a081_00001',\n",
       "  'experiment_tag': '1_fcnet_hiddens=[8]'}}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.results.con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
