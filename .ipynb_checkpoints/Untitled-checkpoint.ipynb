{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "from gym import spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-26 14:06:57,438\tINFO worker.py:745 -- Calling ray.init() again after it has already been called.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.examples.env.multi_agent import MultiAgentCartPole\n",
    "from ray.rllib.examples.models.shared_weights_model import \\\n",
    "    SharedWeightsModel1, SharedWeightsModel2, TF2SharedWeightsModel, \\\n",
    "    TorchSharedWeightsModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "# from ray.rllib.policy import PolicySpec\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer\n",
    "from ray.rllib.agents.ppo import DEFAULT_CONFIG as DEFAULT_CONFIG_PPO\n",
    "\n",
    "from ray.rllib.agents.dqn import DQNTrainer, DEFAULT_CONFIG \n",
    "from ray.rllib.agents.dqn import  DEFAULT_CONFIG as DEFAULT_CONFIG_DQN\n",
    "\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixGame():\n",
    "    \n",
    "    def __init__(self, RPST=(3,1,0,5)):\n",
    "        \n",
    "        self.RPST = RPST\n",
    "        \n",
    "        self.payoff_mat = np.empty((2,2), dtype=np.object)\n",
    "        \n",
    "        self.payoff_mat[0, 0] = (RPST[0], RPST[0])\n",
    "        self.payoff_mat[1, 1] = (RPST[1], RPST[1])\n",
    "        self.payoff_mat[0, 1] = (RPST[2], RPST[3])\n",
    "        self.payoff_mat[1, 0] = (RPST[3], RPST[2])\n",
    "        \n",
    "    def play(self, a_row, a_col):\n",
    "        # for ease of things 0 is coooperate\n",
    "#                            1 is defect\n",
    "        \n",
    "        \n",
    "#         if a_row == 'c':\n",
    "#             row = 0\n",
    "#         else:\n",
    "#             row = 1\n",
    "            \n",
    "#         if a_col == 'c':\n",
    "#             col = 0\n",
    "#         else:\n",
    "#             col = 1\n",
    "            \n",
    "        return self.payoff_mat[a_row, a_col]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoAgentMatrixGameEnv(MultiAgentEnv):\n",
    "    \n",
    "    def __init__(self, RPST=(3,1,0,5), history_n=100):\n",
    "        \n",
    "        self.num_agents = 2\n",
    "        \n",
    "        self.RPST = RPST\n",
    "        self.history_n = history_n\n",
    "        self.history = np.zeros((2,2,self.history_n))\n",
    "        \n",
    "        self._counter = 0\n",
    "        self._setup_spaces()\n",
    "        self.game = MatrixGame(RPST=self.RPST)\n",
    "    \n",
    "    \n",
    "    def _setup_spaces(self):\n",
    "        \n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        self.observation_space = spaces.Box(0, 1,\n",
    "                                           shape=(self.history_n * 4,))\n",
    "        \n",
    "        \n",
    "    def history_to_states(self, history=None):\n",
    "        \n",
    "        if history is None:\n",
    "            history = self.history\n",
    "            \n",
    "        state1 = history.flatten()\n",
    "        state2 = history[::-1,:,:].flatten()\n",
    "        \n",
    "        states = {0: state1, 1:state2}\n",
    "        \n",
    "        return states\n",
    "            \n",
    "        \n",
    "        \n",
    "    def step(self, action_dict):\n",
    "        \n",
    "        print((action_dict[0], action_dict[1]))\n",
    "        rewards = self.game.play(action_dict[0], action_dict[1])\n",
    "        rew = {i: rewards[i] for i in [0, 1]}\n",
    "        \n",
    "        self.history[0, action_dict[0], self._counter] = 1\n",
    "        self.history[1, action_dict[1], self._counter] = 1\n",
    "        \n",
    "        obs = self.history_to_states(self.history)\n",
    "        \n",
    "        self._counter += 1\n",
    "        \n",
    "        is_done = self._counter >= self.history_n\n",
    "        done = {i: is_done for i in [0, 1, \"__all__\"]}\n",
    "        \n",
    "        info = {0:{}, 1:{}}\n",
    "        \n",
    "        return obs, rew, done, info\n",
    "        \n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.history = np.zeros((2,2,self.history_n))\n",
    "        obs = self.history_to_states(self.history)\n",
    "\n",
    "        self._counter = 0\n",
    "        \n",
    "        return obs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TwoAgentMatrixGameEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4, 1: 3}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = (4,3)\n",
    "{i:aa[i] for i in [0,1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({0: array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  1: array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.])},\n",
       " {0: 3, 1: 3},\n",
       " {0: False, 1: False, '__all__': False},\n",
       " {0: {}, 1: {}})"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step({0:0,1:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "info ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[0] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[1] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {}, 1: {}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env('twoagent_PD', lambda c: TwoAgentMatrixGameEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_ppo = DEFAULT_CONFIG_PPO.copy()\n",
    "trainer_config_ppo['num_workers'] = 1\n",
    "trainer_config_ppo['num_sgd_iter'] = 20\n",
    "trainer_config_ppo['sgd_minibatch_size'] = 32\n",
    "# trainer_config_ppo['model']['fcnet_hiddens'] = [1024, 512,512, 256,256,32,8]\n",
    "trainer_config_ppo['model']['fcnet_hiddens'] = [256,256,32,8]\n",
    "\n",
    "trainer_config_ppo['num_cpus_per_worker'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-26 15:52:54,124\tINFO trainable.py:101 -- Trainable.setup took 13.018 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2021-07-26 15:52:54,125\tWARNING util.py:53 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "trainer = PPOTrainer(trainer_config_ppo, env=\"twoagent_PD\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iteration 0...\n",
      "agent_timesteps_total: 4000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-26_15-53-24\n",
      "done: false\n",
      "episode_len_mean: 100.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 478.0\n",
      "episode_reward_mean: 455.8\n",
      "episode_reward_min: 428.0\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 20\n",
      "experiment_id: 39b34ab8c778431db4c7d7363eca7f2b\n",
      "hostname: coolo-computer\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.20000000298023224\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6630105972290039\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.031176701188087463\n",
      "        model: {}\n",
      "        policy_loss: -0.036966897547245026\n",
      "        total_loss: 8868.671875\n",
      "        vf_explained_var: -7.157325967455108e-07\n",
      "        vf_loss: 8868.7041015625\n",
      "  num_agent_steps_sampled: 4000\n",
      "  num_agent_steps_trained: 4000\n",
      "  num_steps_sampled: 4000\n",
      "  num_steps_trained: 4000\n",
      "iterations_since_restore: 1\n",
      "node_ip: 192.168.1.21\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 44.47272727272727\n",
      "  ram_util_percent: 90.11363636363636\n",
      "pid: 31107\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.18910751647796706\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.20377746765045215\n",
      "  mean_inference_ms: 2.210391872468917\n",
      "  mean_raw_obs_processing_ms: 0.2931082981458489\n",
      "time_since_restore: 30.39619755744934\n",
      "time_this_iter_s: 30.39619755744934\n",
      "time_total_s: 30.39619755744934\n",
      "timers:\n",
      "  learn_throughput: 164.283\n",
      "  learn_time_ms: 24348.216\n",
      "  load_throughput: 50540.329\n",
      "  load_time_ms: 79.145\n",
      "  sample_throughput: 679.394\n",
      "  sample_time_ms: 5887.602\n",
      "  update_time_ms: 6.434\n",
      "timestamp: 1627271604\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 4000\n",
      "training_iteration: 1\n",
      "\n",
      "Training iteration 1...\n",
      "agent_timesteps_total: 8000\n",
      "custom_metrics: {}\n",
      "date: 2021-07-26_15-53-58\n",
      "done: false\n",
      "episode_len_mean: 100.0\n",
      "episode_media: {}\n",
      "episode_reward_max: 478.0\n",
      "episode_reward_mean: 435.475\n",
      "episode_reward_min: 387.0\n",
      "episodes_this_iter: 20\n",
      "episodes_total: 40\n",
      "experiment_id: 39b34ab8c778431db4c7d7363eca7f2b\n",
      "hostname: coolo-computer\n",
      "info:\n",
      "  learner:\n",
      "    default_policy:\n",
      "      learner_stats:\n",
      "        cur_kl_coeff: 0.30000001192092896\n",
      "        cur_lr: 4.999999873689376e-05\n",
      "        entropy: 0.6342822313308716\n",
      "        entropy_coeff: 0.0\n",
      "        kl: 0.011929323896765709\n",
      "        model: {}\n",
      "        policy_loss: -0.026292547583580017\n",
      "        total_loss: 7051.43798828125\n",
      "        vf_explained_var: -9.441375681262798e-08\n",
      "        vf_loss: 7051.4609375\n",
      "  num_agent_steps_sampled: 8000\n",
      "  num_agent_steps_trained: 8000\n",
      "  num_steps_sampled: 8000\n",
      "  num_steps_trained: 8000\n",
      "iterations_since_restore: 2\n",
      "node_ip: 192.168.1.21\n",
      "num_healthy_workers: 1\n",
      "off_policy_estimator: {}\n",
      "perf:\n",
      "  cpu_util_percent: 49.70208333333333\n",
      "  ram_util_percent: 90.23958333333333\n",
      "pid: 31107\n",
      "policy_reward_max: {}\n",
      "policy_reward_mean: {}\n",
      "policy_reward_min: {}\n",
      "sampler_perf:\n",
      "  mean_action_processing_ms: 0.19724825582773636\n",
      "  mean_env_render_ms: 0.0\n",
      "  mean_env_wait_ms: 0.21430118699969797\n",
      "  mean_inference_ms: 2.310685709354304\n",
      "  mean_raw_obs_processing_ms: 0.30467423266134136\n",
      "time_since_restore: 64.54147911071777\n",
      "time_this_iter_s: 34.14528155326843\n",
      "time_total_s: 64.54147911071777\n",
      "timers:\n",
      "  learn_throughput: 155.268\n",
      "  learn_time_ms: 25761.966\n",
      "  load_throughput: 88602.619\n",
      "  load_time_ms: 45.145\n",
      "  sample_throughput: 623.774\n",
      "  sample_time_ms: 6412.578\n",
      "  update_time_ms: 6.202\n",
      "timestamp: 1627271638\n",
      "timesteps_since_restore: 0\n",
      "timesteps_total: 8000\n",
      "training_iteration: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(\"Training iteration {}...\".format(i))\n",
    "    result=trainer.train()\n",
    "    print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TwoAgentMatrixGameEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 1, 1: 1}\n",
      "(1, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 1}\n",
      "(0, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 1, 1: 1}\n",
      "(1, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 1, 1: 1}\n",
      "(1, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 1}\n",
      "(0, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 1, 1: 1}\n",
      "(1, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 1, 1: 0}\n",
      "(1, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 1}\n",
      "(0, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 1}\n",
      "(0, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 1, 1: 1}\n",
      "(1, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 1}\n",
      "(0, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 1, 1: 0}\n",
      "(1, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 1}\n",
      "(0, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 1, 1: 1}\n",
      "(1, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 1}\n",
      "(0, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 1, 1: 0}\n",
      "(1, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 1}\n",
      "(0, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 1}\n",
      "(0, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 1, 1: 0}\n",
      "(1, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 1}\n",
      "(0, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 1, 1: 1}\n",
      "(1, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 1}\n",
      "(0, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 1, 1: 0}\n",
      "(1, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 1}\n",
      "(0, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 1, 1: 0}\n",
      "(1, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 1, 1: 0}\n",
      "(1, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 0}\n",
      "(0, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 1, 1: 0}\n",
      "(1, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 1}\n",
      "(0, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 0, 1: 1}\n",
      "(0, 1)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 1, 1: 0}\n",
      "(1, 0)\n",
      "===========\n",
      "0\n",
      "===========\n",
      "{0: 1, 1: 0}\n",
      "(1, 0)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_samples = 50\n",
    "\n",
    "\n",
    "defects = []\n",
    "rewards = []\n",
    "for i in range(n_samples):\n",
    "    state = env.reset()\n",
    "    \n",
    "    total_defect = 0\n",
    "    cum_reward = 0\n",
    "    done = False\n",
    "    i = 0\n",
    "    while not done:\n",
    "        print('===========')\n",
    "        print(i)\n",
    "        print('===========')\n",
    "        \n",
    "        i+=1\n",
    "        action = trainer.compute_actions(state)\n",
    "        print(action)\n",
    "#         total_defect += action\n",
    "        state, reward, done, results = env.step(action)\n",
    "#         cum_reward += reward\n",
    "    defects.append(total_defect)\n",
    "    rewards.append(cum_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def env_creator(_):\n",
    "    return TwoAgentMatrixGameEnv()\n",
    "single_env = TwoAgentMatrixGameEnv()\n",
    "env_name = \"TwoAgent_PD\"\n",
    "register_env(env_name, env_creator)\n",
    "\n",
    "\n",
    "obs_space = single_env.observation_space\n",
    "act_space = single_env.action_space\n",
    "num_agents = single_env.num_agents\n",
    "\n",
    "def gen_policy():\n",
    "    return (None, obs_space, act_space, {})\n",
    "policy_graphs = {}\n",
    "for i in range(num_agents):\n",
    "    policy_graphs['agent-' + str(i)] = gen_policy()\n",
    "def policy_mapping_fn(agent_id):\n",
    "        return 'agent-' + str(agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"log_level\": \"WARN\",\n",
    "    \"num_workers\": 3,\n",
    "    \"num_cpus_for_driver\": 1,\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "    \"lr\": 5e-3,\n",
    "    \"model\":{\"fcnet_hiddens\": [8, 8]},\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policy_graphs,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "    },\n",
    "    \"env\": \"TwoAgent_PD\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.7/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 15996\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-18-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 496.0\n",
      "  episode_reward_mean: 451.0897435897436\n",
      "  episode_reward_min: 406.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 78\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.664276659488678\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.029636424034833908\n",
      "          model: {}\n",
      "          policy_loss: -0.060856517404317856\n",
      "          total_loss: 7095.986328125\n",
      "          vf_explained_var: -2.1957581338938326e-05\n",
      "          vf_loss: 7096.04150390625\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.20000000298023224\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.6590754389762878\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.03510422259569168\n",
      "          model: {}\n",
      "          policy_loss: -0.055008288472890854\n",
      "          total_loss: 6811.607421875\n",
      "          vf_explained_var: -1.3433156709652394e-05\n",
      "          vf_loss: 6811.6552734375\n",
      "    num_agent_steps_sampled: 15996\n",
      "    num_agent_steps_trained: 15996\n",
      "    num_steps_sampled: 7998\n",
      "    num_steps_trained: 7998\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 49.3\n",
      "    ram_util_percent: 97.44499999999998\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 273.0\n",
      "    agent-1: 270.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 228.14102564102564\n",
      "    agent-1: 222.94871794871796\n",
      "  policy_reward_min:\n",
      "    agent-0: 177.0\n",
      "    agent-1: 183.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10901596408801563\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09700969194951703\n",
      "    mean_inference_ms: 1.7989399045456231\n",
      "    mean_raw_obs_processing_ms: 0.15861567013443387\n",
      "  time_since_restore: 13.665232419967651\n",
      "  time_this_iter_s: 13.665232419967651\n",
      "  time_total_s: 13.665232419967651\n",
      "  timers:\n",
      "    learn_throughput: 1065.319\n",
      "    learn_time_ms: 7507.612\n",
      "    load_throughput: 57758.936\n",
      "    load_time_ms: 138.472\n",
      "    sample_throughput: 1350.753\n",
      "    sample_time_ms: 5921.141\n",
      "    update_time_ms: 2.614\n",
      "  timestamp: 1627273137\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7998\n",
      "  training_iteration: 1\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         13.6652</td><td style=\"text-align: right;\">7998</td><td style=\"text-align: right;\">  451.09</td><td style=\"text-align: right;\">                 496</td><td style=\"text-align: right;\">                 406</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 31992\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-19-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 480.0\n",
      "  episode_reward_mean: 443.6\n",
      "  episode_reward_min: 382.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 159\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.6360675096511841\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.028073500841856003\n",
      "          model: {}\n",
      "          policy_loss: -0.05279109999537468\n",
      "          total_loss: 4550.61962890625\n",
      "          vf_explained_var: 0.072012759745121\n",
      "          vf_loss: 4550.66455078125\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.30000001192092896\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.6255916953086853\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.028973288834095\n",
      "          model: {}\n",
      "          policy_loss: -0.05241768807172775\n",
      "          total_loss: 6438.40283203125\n",
      "          vf_explained_var: -6.3911561483109836e-06\n",
      "          vf_loss: 6438.44677734375\n",
      "    num_agent_steps_sampled: 31992\n",
      "    num_agent_steps_trained: 31992\n",
      "    num_steps_sampled: 15996\n",
      "    num_steps_trained: 15996\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.63333333333333\n",
      "    ram_util_percent: 97.58333333333331\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 277.0\n",
      "    agent-1: 282.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 214.15\n",
      "    agent-1: 229.45\n",
      "  policy_reward_min:\n",
      "    agent-0: 150.0\n",
      "    agent-1: 154.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11120137333890863\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09834379744598597\n",
      "    mean_inference_ms: 1.8014407998370945\n",
      "    mean_raw_obs_processing_ms: 0.1598135023358829\n",
      "  time_since_restore: 26.554832935333252\n",
      "  time_this_iter_s: 12.8896005153656\n",
      "  time_total_s: 26.554832935333252\n",
      "  timers:\n",
      "    learn_throughput: 1115.908\n",
      "    learn_time_ms: 7167.258\n",
      "    load_throughput: 88062.729\n",
      "    load_time_ms: 90.822\n",
      "    sample_throughput: 1340.629\n",
      "    sample_time_ms: 5965.856\n",
      "    update_time_ms: 2.414\n",
      "  timestamp: 1627273150\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15996\n",
      "  training_iteration: 2\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         26.5548</td><td style=\"text-align: right;\">15996</td><td style=\"text-align: right;\">   443.6</td><td style=\"text-align: right;\">                 480</td><td style=\"text-align: right;\">                 382</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 47988\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-19-23\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 477.0\n",
      "  episode_reward_mean: 435.44\n",
      "  episode_reward_min: 360.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 237\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.6167100667953491\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02464049868285656\n",
      "          model: {}\n",
      "          policy_loss: -0.052305497229099274\n",
      "          total_loss: 3251.7626953125\n",
      "          vf_explained_var: 0.17814792692661285\n",
      "          vf_loss: 3251.803955078125\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.44999998807907104\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5898683667182922\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02540029026567936\n",
      "          model: {}\n",
      "          policy_loss: -0.04735615849494934\n",
      "          total_loss: 5114.13525390625\n",
      "          vf_explained_var: -2.4226403638749616e-06\n",
      "          vf_loss: 5114.1708984375\n",
      "    num_agent_steps_sampled: 47988\n",
      "    num_agent_steps_trained: 47988\n",
      "    num_steps_sampled: 23994\n",
      "    num_steps_trained: 23994\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 50.736842105263165\n",
      "    ram_util_percent: 97.5894736842105\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 276.0\n",
      "    agent-1: 280.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 206.22\n",
      "    agent-1: 229.22\n",
      "  policy_reward_min:\n",
      "    agent-0: 150.0\n",
      "    agent-1: 154.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11195924247744368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09900429379768245\n",
      "    mean_inference_ms: 1.7889322872280093\n",
      "    mean_raw_obs_processing_ms: 0.1606520248655018\n",
      "  time_since_restore: 39.72160053253174\n",
      "  time_this_iter_s: 13.166767597198486\n",
      "  time_total_s: 39.72160053253174\n",
      "  timers:\n",
      "    learn_throughput: 1105.964\n",
      "    learn_time_ms: 7231.699\n",
      "    load_throughput: 123622.98\n",
      "    load_time_ms: 64.697\n",
      "    sample_throughput: 1354.274\n",
      "    sample_time_ms: 5905.746\n",
      "    update_time_ms: 2.263\n",
      "  timestamp: 1627273163\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 23994\n",
      "  training_iteration: 3\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         39.7216</td><td style=\"text-align: right;\">23994</td><td style=\"text-align: right;\">  435.44</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">                 360</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 63984\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-19-37\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 478.0\n",
      "  episode_reward_mean: 437.54\n",
      "  episode_reward_min: 379.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 318\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.6062700152397156\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.02044774405658245\n",
      "          model: {}\n",
      "          policy_loss: -0.040002740919589996\n",
      "          total_loss: 2311.958740234375\n",
      "          vf_explained_var: 0.2912120521068573\n",
      "          vf_loss: 2311.985107421875\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5761978030204773\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.017642317339777946\n",
      "          model: {}\n",
      "          policy_loss: -0.03806866705417633\n",
      "          total_loss: 4354.07958984375\n",
      "          vf_explained_var: -5.873941404388461e-07\n",
      "          vf_loss: 4354.1064453125\n",
      "    num_agent_steps_sampled: 63984\n",
      "    num_agent_steps_trained: 63984\n",
      "    num_steps_sampled: 31992\n",
      "    num_steps_trained: 31992\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.970000000000006\n",
      "    ram_util_percent: 97.565\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 279.0\n",
      "    agent-1: 299.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 206.52\n",
      "    agent-1: 231.02\n",
      "  policy_reward_min:\n",
      "    agent-0: 124.0\n",
      "    agent-1: 168.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11041979116034191\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09777892367444614\n",
      "    mean_inference_ms: 1.7534341432487732\n",
      "    mean_raw_obs_processing_ms: 0.15906670244147467\n",
      "  time_since_restore: 53.877995014190674\n",
      "  time_this_iter_s: 14.156394481658936\n",
      "  time_total_s: 53.877995014190674\n",
      "  timers:\n",
      "    learn_throughput: 1050.328\n",
      "    learn_time_ms: 7614.768\n",
      "    load_throughput: 143829.217\n",
      "    load_time_ms: 55.608\n",
      "    sample_throughput: 1386.658\n",
      "    sample_time_ms: 5767.824\n",
      "    update_time_ms: 2.453\n",
      "  timestamp: 1627273177\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 31992\n",
      "  training_iteration: 4\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">          53.878</td><td style=\"text-align: right;\">31992</td><td style=\"text-align: right;\">  437.54</td><td style=\"text-align: right;\">                 478</td><td style=\"text-align: right;\">                 379</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 79980\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-19-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 488.0\n",
      "  episode_reward_mean: 429.11\n",
      "  episode_reward_min: 364.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 399\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.592710554599762\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014430426061153412\n",
      "          model: {}\n",
      "          policy_loss: -0.03324376791715622\n",
      "          total_loss: 1766.1417236328125\n",
      "          vf_explained_var: 0.4310234487056732\n",
      "          vf_loss: 1766.16015625\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5369526743888855\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01950351521372795\n",
      "          model: {}\n",
      "          policy_loss: -0.03523269295692444\n",
      "          total_loss: 3090.308837890625\n",
      "          vf_explained_var: -1.2882294697647012e-07\n",
      "          vf_loss: 3090.33154296875\n",
      "    num_agent_steps_sampled: 79980\n",
      "    num_agent_steps_trained: 79980\n",
      "    num_steps_sampled: 39990\n",
      "    num_steps_trained: 39990\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.37619047619047\n",
      "    ram_util_percent: 97.70476190476191\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 294.0\n",
      "    agent-1: 274.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 206.48\n",
      "    agent-1: 222.63\n",
      "  policy_reward_min:\n",
      "    agent-0: 150.0\n",
      "    agent-1: 163.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.10979131186183956\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09720451006795344\n",
      "    mean_inference_ms: 1.7338832965729432\n",
      "    mean_raw_obs_processing_ms: 0.15804662665562183\n",
      "  time_since_restore: 67.94851398468018\n",
      "  time_this_iter_s: 14.070518970489502\n",
      "  time_total_s: 67.94851398468018\n",
      "  timers:\n",
      "    learn_throughput: 1025.136\n",
      "    learn_time_ms: 7801.89\n",
      "    load_throughput: 169222.932\n",
      "    load_time_ms: 47.263\n",
      "    sample_throughput: 1399.865\n",
      "    sample_time_ms: 5713.409\n",
      "    update_time_ms: 2.563\n",
      "  timestamp: 1627273192\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 39990\n",
      "  training_iteration: 5\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         67.9485</td><td style=\"text-align: right;\">39990</td><td style=\"text-align: right;\">  429.11</td><td style=\"text-align: right;\">                 488</td><td style=\"text-align: right;\">                 364</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 95976\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-20-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 486.0\n",
      "  episode_reward_mean: 419.56\n",
      "  episode_reward_min: 365.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 477\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5845174193382263\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01274198293685913\n",
      "          model: {}\n",
      "          policy_loss: -0.03116431273519993\n",
      "          total_loss: 1294.3248291015625\n",
      "          vf_explained_var: 0.5572563409805298\n",
      "          vf_loss: 1294.3433837890625\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5038086175918579\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.019823916256427765\n",
      "          model: {}\n",
      "          policy_loss: -0.0337551049888134\n",
      "          total_loss: 2319.753662109375\n",
      "          vf_explained_var: -1.826593987175329e-08\n",
      "          vf_loss: 2319.7744140625\n",
      "    num_agent_steps_sampled: 95976\n",
      "    num_agent_steps_trained: 95976\n",
      "    num_steps_sampled: 47988\n",
      "    num_steps_trained: 47988\n",
      "  iterations_since_restore: 6\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.019047619047626\n",
      "    ram_util_percent: 97.72857142857141\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 298.0\n",
      "    agent-1: 289.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 202.48\n",
      "    agent-1: 217.08\n",
      "  policy_reward_min:\n",
      "    agent-0: 149.0\n",
      "    agent-1: 155.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.109599776251567\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09704999032831595\n",
      "    mean_inference_ms: 1.7290791234716887\n",
      "    mean_raw_obs_processing_ms: 0.15746742249373236\n",
      "  time_since_restore: 82.73729038238525\n",
      "  time_this_iter_s: 14.788776397705078\n",
      "  time_total_s: 82.73729038238525\n",
      "  timers:\n",
      "    learn_throughput: 997.25\n",
      "    learn_time_ms: 8020.053\n",
      "    load_throughput: 191295.058\n",
      "    load_time_ms: 41.81\n",
      "    sample_throughput: 1402.321\n",
      "    sample_time_ms: 5703.403\n",
      "    update_time_ms: 2.563\n",
      "  timestamp: 1627273206\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 47988\n",
      "  training_iteration: 6\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.0/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">     6</td><td style=\"text-align: right;\">         82.7373</td><td style=\"text-align: right;\">47988</td><td style=\"text-align: right;\">  419.56</td><td style=\"text-align: right;\">                 486</td><td style=\"text-align: right;\">                 365</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 111972\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-20-22\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 508.0\n",
      "  episode_reward_mean: 421.31\n",
      "  episode_reward_min: 365.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 558\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.572909414768219\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012790652923285961\n",
      "          model: {}\n",
      "          policy_loss: -0.030833085998892784\n",
      "          total_loss: 842.5501708984375\n",
      "          vf_explained_var: 0.6742656826972961\n",
      "          vf_loss: 842.5680541992188\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.49706533551216125\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.018653424456715584\n",
      "          model: {}\n",
      "          policy_loss: -0.03153543174266815\n",
      "          total_loss: 2066.850830078125\n",
      "          vf_explained_var: -1.826593987175329e-08\n",
      "          vf_loss: 2066.869384765625\n",
      "    num_agent_steps_sampled: 111972\n",
      "    num_agent_steps_trained: 111972\n",
      "    num_steps_sampled: 55986\n",
      "    num_steps_trained: 55986\n",
      "  iterations_since_restore: 7\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.350000000000016\n",
      "    ram_util_percent: 97.7090909090909\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 306.0\n",
      "    agent-1: 288.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 202.88\n",
      "    agent-1: 218.43\n",
      "  policy_reward_min:\n",
      "    agent-0: 149.0\n",
      "    agent-1: 156.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11113259680610668\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0985686315555936\n",
      "    mean_inference_ms: 1.762048779703069\n",
      "    mean_raw_obs_processing_ms: 0.15911841568967136\n",
      "  time_since_restore: 98.36692190170288\n",
      "  time_this_iter_s: 15.629631519317627\n",
      "  time_total_s: 98.36692190170288\n",
      "  timers:\n",
      "    learn_throughput: 979.734\n",
      "    learn_time_ms: 8163.44\n",
      "    load_throughput: 196537.407\n",
      "    load_time_ms: 40.695\n",
      "    sample_throughput: 1372.913\n",
      "    sample_time_ms: 5825.571\n",
      "    update_time_ms: 2.81\n",
      "  timestamp: 1627273222\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 55986\n",
      "  training_iteration: 7\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">     7</td><td style=\"text-align: right;\">         98.3669</td><td style=\"text-align: right;\">55986</td><td style=\"text-align: right;\">  421.31</td><td style=\"text-align: right;\">                 508</td><td style=\"text-align: right;\">                 365</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 127968\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-20-38\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 486.0\n",
      "  episode_reward_mean: 418.86\n",
      "  episode_reward_min: 359.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 639\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5610260367393494\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012288955971598625\n",
      "          model: {}\n",
      "          policy_loss: -0.02785569615662098\n",
      "          total_loss: 828.1436157226562\n",
      "          vf_explained_var: 0.7296446561813354\n",
      "          vf_loss: 828.1589965820312\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.675000011920929\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5015184283256531\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.022236373275518417\n",
      "          model: {}\n",
      "          policy_loss: -0.037004828453063965\n",
      "          total_loss: 1889.3631591796875\n",
      "          vf_explained_var: -5.7681912579710115e-09\n",
      "          vf_loss: 1889.3851318359375\n",
      "    num_agent_steps_sampled: 127968\n",
      "    num_agent_steps_trained: 127968\n",
      "    num_steps_sampled: 63984\n",
      "    num_steps_trained: 63984\n",
      "  iterations_since_restore: 8\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.77391304347826\n",
      "    ram_util_percent: 98.01304347826085\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 306.0\n",
      "    agent-1: 288.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 208.78\n",
      "    agent-1: 210.08\n",
      "  policy_reward_min:\n",
      "    agent-0: 142.0\n",
      "    agent-1: 144.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11143217880651413\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.09904753556178754\n",
      "    mean_inference_ms: 1.770003639233364\n",
      "    mean_raw_obs_processing_ms: 0.15965547941446737\n",
      "  time_since_restore: 114.52575302124023\n",
      "  time_this_iter_s: 16.158831119537354\n",
      "  time_total_s: 114.52575302124023\n",
      "  timers:\n",
      "    learn_throughput: 948.379\n",
      "    learn_time_ms: 8433.338\n",
      "    load_throughput: 212285.156\n",
      "    load_time_ms: 37.676\n",
      "    sample_throughput: 1373.98\n",
      "    sample_time_ms: 5821.044\n",
      "    update_time_ms: 4.375\n",
      "  timestamp: 1627273238\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 63984\n",
      "  training_iteration: 8\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">     8</td><td style=\"text-align: right;\">         114.526</td><td style=\"text-align: right;\">63984</td><td style=\"text-align: right;\">  418.86</td><td style=\"text-align: right;\">                 486</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 143964\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-20-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 484.0\n",
      "  episode_reward_mean: 419.74\n",
      "  episode_reward_min: 359.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 717\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5469088554382324\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013633936643600464\n",
      "          model: {}\n",
      "          policy_loss: -0.030268583446741104\n",
      "          total_loss: 545.896240234375\n",
      "          vf_explained_var: 0.8160889148712158\n",
      "          vf_loss: 545.9127197265625\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.48665159940719604\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014359887689352036\n",
      "          model: {}\n",
      "          policy_loss: -0.029139328747987747\n",
      "          total_loss: 1949.5472412109375\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 1949.5616455078125\n",
      "    num_agent_steps_sampled: 143964\n",
      "    num_agent_steps_trained: 143964\n",
      "    num_steps_sampled: 71982\n",
      "    num_steps_trained: 71982\n",
      "  iterations_since_restore: 9\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 67.27307692307693\n",
      "    ram_util_percent: 98.64230769230768\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 290.0\n",
      "    agent-1: 277.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 208.32\n",
      "    agent-1: 211.42\n",
      "  policy_reward_min:\n",
      "    agent-0: 142.0\n",
      "    agent-1: 137.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11439146626894832\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10192261640413842\n",
      "    mean_inference_ms: 1.833904834706775\n",
      "    mean_raw_obs_processing_ms: 0.16438015627775018\n",
      "  time_since_restore: 132.57339525222778\n",
      "  time_this_iter_s: 18.04764223098755\n",
      "  time_total_s: 132.57339525222778\n",
      "  timers:\n",
      "    learn_throughput: 931.362\n",
      "    learn_time_ms: 8587.421\n",
      "    load_throughput: 219229.553\n",
      "    load_time_ms: 36.482\n",
      "    sample_throughput: 1314.621\n",
      "    sample_time_ms: 6083.881\n",
      "    update_time_ms: 4.232\n",
      "  timestamp: 1627273256\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 71982\n",
      "  training_iteration: 9\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">     9</td><td style=\"text-align: right;\">         132.573</td><td style=\"text-align: right;\">71982</td><td style=\"text-align: right;\">  419.74</td><td style=\"text-align: right;\">                 484</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 159960\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-21-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 490.0\n",
      "  episode_reward_mean: 421.86\n",
      "  episode_reward_min: 360.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 798\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5442790389060974\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011967494152486324\n",
      "          model: {}\n",
      "          policy_loss: -0.0268519576638937\n",
      "          total_loss: 479.240478515625\n",
      "          vf_explained_var: 0.8568693995475769\n",
      "          vf_loss: 479.2552795410156\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5050936937332153\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013154036365449429\n",
      "          model: {}\n",
      "          policy_loss: -0.027125440537929535\n",
      "          total_loss: 1753.9063720703125\n",
      "          vf_explained_var: -2.8840956289855058e-09\n",
      "          vf_loss: 1753.9200439453125\n",
      "    num_agent_steps_sampled: 159960\n",
      "    num_agent_steps_trained: 159960\n",
      "    num_steps_sampled: 79980\n",
      "    num_steps_trained: 79980\n",
      "  iterations_since_restore: 10\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.46956521739129\n",
      "    ram_util_percent: 98.79130434782608\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 288.0\n",
      "    agent-1: 266.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 218.63\n",
      "    agent-1: 203.23\n",
      "  policy_reward_min:\n",
      "    agent-0: 146.0\n",
      "    agent-1: 137.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11622938828139957\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10383288589323403\n",
      "    mean_inference_ms: 1.8758393242246711\n",
      "    mean_raw_obs_processing_ms: 0.1677794603562182\n",
      "  time_since_restore: 148.95412731170654\n",
      "  time_this_iter_s: 16.38073205947876\n",
      "  time_total_s: 148.95412731170654\n",
      "  timers:\n",
      "    learn_throughput: 923.392\n",
      "    learn_time_ms: 8661.545\n",
      "    load_throughput: 228540.969\n",
      "    load_time_ms: 34.996\n",
      "    sample_throughput: 1295.044\n",
      "    sample_time_ms: 6175.85\n",
      "    update_time_ms: 5.02\n",
      "  timestamp: 1627273273\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 79980\n",
      "  training_iteration: 10\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         148.954</td><td style=\"text-align: right;\">79980</td><td style=\"text-align: right;\">  421.86</td><td style=\"text-align: right;\">                 490</td><td style=\"text-align: right;\">                 360</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 175956\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-21-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 490.0\n",
      "  episode_reward_mean: 423.97\n",
      "  episode_reward_min: 352.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 879\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5351572632789612\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012634098529815674\n",
      "          model: {}\n",
      "          policy_loss: -0.030532993376255035\n",
      "          total_loss: 463.7942199707031\n",
      "          vf_explained_var: 0.8818807005882263\n",
      "          vf_loss: 463.81195068359375\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4981108605861664\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016147328540682793\n",
      "          model: {}\n",
      "          policy_loss: -0.029036905616521835\n",
      "          total_loss: 1932.1578369140625\n",
      "          vf_explained_var: -9.613652096618353e-10\n",
      "          vf_loss: 1932.170654296875\n",
      "    num_agent_steps_sampled: 175956\n",
      "    num_agent_steps_trained: 175956\n",
      "    num_steps_sampled: 87978\n",
      "    num_steps_trained: 87978\n",
      "  iterations_since_restore: 11\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.02692307692307\n",
      "    ram_util_percent: 98.78076923076924\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 319.0\n",
      "    agent-1: 273.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 216.61\n",
      "    agent-1: 207.36\n",
      "  policy_reward_min:\n",
      "    agent-0: 146.0\n",
      "    agent-1: 133.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11740717567036388\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10528384565428088\n",
      "    mean_inference_ms: 1.9040294484341502\n",
      "    mean_raw_obs_processing_ms: 0.169756339623203\n",
      "  time_since_restore: 166.56990003585815\n",
      "  time_this_iter_s: 17.61577272415161\n",
      "  time_total_s: 166.56990003585815\n",
      "  timers:\n",
      "    learn_throughput: 894.251\n",
      "    learn_time_ms: 8943.798\n",
      "    load_throughput: 275575.642\n",
      "    load_time_ms: 29.023\n",
      "    sample_throughput: 1269.281\n",
      "    sample_time_ms: 6301.204\n",
      "    update_time_ms: 6.202\n",
      "  timestamp: 1627273290\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 87978\n",
      "  training_iteration: 11\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    11</td><td style=\"text-align: right;\">          166.57</td><td style=\"text-align: right;\">87978</td><td style=\"text-align: right;\">  423.97</td><td style=\"text-align: right;\">                 490</td><td style=\"text-align: right;\">                 352</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 191952\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-21-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 512.0\n",
      "  episode_reward_mean: 425.44\n",
      "  episode_reward_min: 352.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 957\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5376333594322205\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013263986445963383\n",
      "          model: {}\n",
      "          policy_loss: -0.027589788660407066\n",
      "          total_loss: 318.56915283203125\n",
      "          vf_explained_var: 0.9269373416900635\n",
      "          vf_loss: 318.58331298828125\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4882891774177551\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012914363294839859\n",
      "          model: {}\n",
      "          policy_loss: -0.025481242686510086\n",
      "          total_loss: 1904.7034912109375\n",
      "          vf_explained_var: -6.729556467632847e-09\n",
      "          vf_loss: 1904.715576171875\n",
      "    num_agent_steps_sampled: 191952\n",
      "    num_agent_steps_trained: 191952\n",
      "    num_steps_sampled: 95976\n",
      "    num_steps_trained: 95976\n",
      "  iterations_since_restore: 12\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 64.02173913043478\n",
      "    ram_util_percent: 98.78695652173913\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 319.0\n",
      "    agent-1: 263.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 216.92\n",
      "    agent-1: 208.52\n",
      "  policy_reward_min:\n",
      "    agent-0: 152.0\n",
      "    agent-1: 144.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11870596309164531\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1065357528801416\n",
      "    mean_inference_ms: 1.932057875323544\n",
      "    mean_raw_obs_processing_ms: 0.17142330193225783\n",
      "  time_since_restore: 183.21843194961548\n",
      "  time_this_iter_s: 16.648531913757324\n",
      "  time_total_s: 183.21843194961548\n",
      "  timers:\n",
      "    learn_throughput: 870.79\n",
      "    learn_time_ms: 9184.758\n",
      "    load_throughput: 299347.99\n",
      "    load_time_ms: 26.718\n",
      "    sample_throughput: 1242.313\n",
      "    sample_time_ms: 6437.992\n",
      "    update_time_ms: 6.275\n",
      "  timestamp: 1627273307\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 95976\n",
      "  training_iteration: 12\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    12</td><td style=\"text-align: right;\">         183.218</td><td style=\"text-align: right;\">95976</td><td style=\"text-align: right;\">  425.44</td><td style=\"text-align: right;\">                 512</td><td style=\"text-align: right;\">                 352</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 207948\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-22-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 479.0\n",
      "  episode_reward_mean: 418.02\n",
      "  episode_reward_min: 349.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 1038\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5311990976333618\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012791587971150875\n",
      "          model: {}\n",
      "          policy_loss: -0.02650475688278675\n",
      "          total_loss: 344.9759216308594\n",
      "          vf_explained_var: 0.9068655967712402\n",
      "          vf_loss: 344.9894104003906\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4692382216453552\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015098492614924908\n",
      "          model: {}\n",
      "          policy_loss: -0.02885802835226059\n",
      "          total_loss: 1950.7236328125\n",
      "          vf_explained_var: -4.806826048309176e-09\n",
      "          vf_loss: 1950.7373046875\n",
      "    num_agent_steps_sampled: 207948\n",
      "    num_agent_steps_trained: 207948\n",
      "    num_steps_sampled: 103974\n",
      "    num_steps_trained: 103974\n",
      "  iterations_since_restore: 13\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.232\n",
      "    ram_util_percent: 98.304\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 293.0\n",
      "    agent-1: 268.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 210.26\n",
      "    agent-1: 207.76\n",
      "  policy_reward_min:\n",
      "    agent-0: 153.0\n",
      "    agent-1: 146.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1192274383658103\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10698076115973358\n",
      "    mean_inference_ms: 1.943447261822435\n",
      "    mean_raw_obs_processing_ms: 0.1720553989831604\n",
      "  time_since_restore: 200.30792617797852\n",
      "  time_this_iter_s: 17.089494228363037\n",
      "  time_total_s: 200.30792617797852\n",
      "  timers:\n",
      "    learn_throughput: 842.152\n",
      "    learn_time_ms: 9497.096\n",
      "    load_throughput: 285469.573\n",
      "    load_time_ms: 28.017\n",
      "    sample_throughput: 1227.723\n",
      "    sample_time_ms: 6514.501\n",
      "    update_time_ms: 7.429\n",
      "  timestamp: 1627273324\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 103974\n",
      "  training_iteration: 13\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    13</td><td style=\"text-align: right;\">         200.308</td><td style=\"text-align: right;\">103974</td><td style=\"text-align: right;\">  418.02</td><td style=\"text-align: right;\">                 479</td><td style=\"text-align: right;\">                 349</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 223944\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-22-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 499.0\n",
      "  episode_reward_mean: 416.49\n",
      "  episode_reward_min: 360.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 1119\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5157775282859802\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012146435678005219\n",
      "          model: {}\n",
      "          policy_loss: -0.02660209685564041\n",
      "          total_loss: 429.18585205078125\n",
      "          vf_explained_var: 0.919399082660675\n",
      "          vf_loss: 429.2001953125\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4756297171115875\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013177896849811077\n",
      "          model: {}\n",
      "          policy_loss: -0.027760758996009827\n",
      "          total_loss: 1846.097412109375\n",
      "          vf_explained_var: -1.3459112935265694e-08\n",
      "          vf_loss: 1846.1119384765625\n",
      "    num_agent_steps_sampled: 223944\n",
      "    num_agent_steps_trained: 223944\n",
      "    num_steps_sampled: 111972\n",
      "    num_steps_trained: 111972\n",
      "  iterations_since_restore: 14\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.286363636363646\n",
      "    ram_util_percent: 98.45454545454545\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 323.0\n",
      "    agent-1: 261.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 216.62\n",
      "    agent-1: 199.87\n",
      "  policy_reward_min:\n",
      "    agent-0: 153.0\n",
      "    agent-1: 150.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11939919282699318\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10712408545998339\n",
      "    mean_inference_ms: 1.945974680986618\n",
      "    mean_raw_obs_processing_ms: 0.1723025901795866\n",
      "  time_since_restore: 215.99094247817993\n",
      "  time_this_iter_s: 15.683016300201416\n",
      "  time_total_s: 215.99094247817993\n",
      "  timers:\n",
      "    learn_throughput: 838.636\n",
      "    learn_time_ms: 9536.916\n",
      "    load_throughput: 294397.825\n",
      "    load_time_ms: 27.167\n",
      "    sample_throughput: 1206.857\n",
      "    sample_time_ms: 6627.134\n",
      "    update_time_ms: 7.687\n",
      "  timestamp: 1627273340\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 111972\n",
      "  training_iteration: 14\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    14</td><td style=\"text-align: right;\">         215.991</td><td style=\"text-align: right;\">111972</td><td style=\"text-align: right;\">  416.49</td><td style=\"text-align: right;\">                 499</td><td style=\"text-align: right;\">                 360</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 239940\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-22-36\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 499.0\n",
      "  episode_reward_mean: 417.26\n",
      "  episode_reward_min: 350.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 1197\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5083068609237671\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010312821716070175\n",
      "          model: {}\n",
      "          policy_loss: -0.02481580525636673\n",
      "          total_loss: 326.3589172363281\n",
      "          vf_explained_var: 0.9183055758476257\n",
      "          vf_loss: 326.373291015625\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4445822536945343\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014657674357295036\n",
      "          model: {}\n",
      "          policy_loss: -0.03073952905833721\n",
      "          total_loss: 1897.7227783203125\n",
      "          vf_explained_var: 9.613652096618353e-10\n",
      "          vf_loss: 1897.7386474609375\n",
      "    num_agent_steps_sampled: 239940\n",
      "    num_agent_steps_trained: 239940\n",
      "    num_steps_sampled: 119970\n",
      "    num_steps_trained: 119970\n",
      "  iterations_since_restore: 15\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.27826086956522\n",
      "    ram_util_percent: 98.32608695652173\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 310.0\n",
      "    agent-1: 270.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 219.83\n",
      "    agent-1: 197.43\n",
      "  policy_reward_min:\n",
      "    agent-0: 161.0\n",
      "    agent-1: 142.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11951304913273539\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10710626488509\n",
      "    mean_inference_ms: 1.9454302907855174\n",
      "    mean_raw_obs_processing_ms: 0.1722590589113625\n",
      "  time_since_restore: 231.91695284843445\n",
      "  time_this_iter_s: 15.926010370254517\n",
      "  time_total_s: 231.91695284843445\n",
      "  timers:\n",
      "    learn_throughput: 829.875\n",
      "    learn_time_ms: 9637.6\n",
      "    load_throughput: 295815.271\n",
      "    load_time_ms: 27.037\n",
      "    sample_throughput: 1191.632\n",
      "    sample_time_ms: 6711.806\n",
      "    update_time_ms: 7.677\n",
      "  timestamp: 1627273356\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 119970\n",
      "  training_iteration: 15\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    15</td><td style=\"text-align: right;\">         231.917</td><td style=\"text-align: right;\">119970</td><td style=\"text-align: right;\">  417.26</td><td style=\"text-align: right;\">                 499</td><td style=\"text-align: right;\">                 350</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 255936\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-22-51\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 495.0\n",
      "  episode_reward_mean: 422.22\n",
      "  episode_reward_min: 350.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 1278\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5146774649620056\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012435809709131718\n",
      "          model: {}\n",
      "          policy_loss: -0.020723560824990273\n",
      "          total_loss: 406.0738830566406\n",
      "          vf_explained_var: 0.9189573526382446\n",
      "          vf_loss: 406.08203125\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.44831782579421997\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011757285334169865\n",
      "          model: {}\n",
      "          policy_loss: -0.02275053970515728\n",
      "          total_loss: 1866.192626953125\n",
      "          vf_explained_var: -1.634320945242962e-08\n",
      "          vf_loss: 1866.20361328125\n",
      "    num_agent_steps_sampled: 255936\n",
      "    num_agent_steps_trained: 255936\n",
      "    num_steps_sampled: 127968\n",
      "    num_steps_trained: 127968\n",
      "  iterations_since_restore: 16\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.02380952380951\n",
      "    ram_util_percent: 98.10000000000002\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 316.0\n",
      "    agent-1: 263.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 222.16\n",
      "    agent-1: 200.06\n",
      "  policy_reward_min:\n",
      "    agent-0: 165.0\n",
      "    agent-1: 131.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11908913256796343\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10672336566096391\n",
      "    mean_inference_ms: 1.9356369160009281\n",
      "    mean_raw_obs_processing_ms: 0.17148565783015324\n",
      "  time_since_restore: 246.555766582489\n",
      "  time_this_iter_s: 14.638813734054565\n",
      "  time_total_s: 246.555766582489\n",
      "  timers:\n",
      "    learn_throughput: 832.042\n",
      "    learn_time_ms: 9612.496\n",
      "    load_throughput: 274718.521\n",
      "    load_time_ms: 29.113\n",
      "    sample_throughput: 1190.215\n",
      "    sample_time_ms: 6719.793\n",
      "    update_time_ms: 7.726\n",
      "  timestamp: 1627273371\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 127968\n",
      "  training_iteration: 16\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    16</td><td style=\"text-align: right;\">         246.556</td><td style=\"text-align: right;\">127968</td><td style=\"text-align: right;\">  422.22</td><td style=\"text-align: right;\">                 495</td><td style=\"text-align: right;\">                 350</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 271932\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-23-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 496.0\n",
      "  episode_reward_mean: 419.54\n",
      "  episode_reward_min: 363.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 1359\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5043562650680542\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011548547074198723\n",
      "          model: {}\n",
      "          policy_loss: -0.024400994181632996\n",
      "          total_loss: 473.79034423828125\n",
      "          vf_explained_var: 0.9180144667625427\n",
      "          vf_loss: 473.80303955078125\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4373171329498291\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012964409776031971\n",
      "          model: {}\n",
      "          policy_loss: -0.02707073464989662\n",
      "          total_loss: 1983.27734375\n",
      "          vf_explained_var: -3.845460838647341e-09\n",
      "          vf_loss: 1983.2913818359375\n",
      "    num_agent_steps_sampled: 271932\n",
      "    num_agent_steps_trained: 271932\n",
      "    num_steps_sampled: 135966\n",
      "    num_steps_trained: 135966\n",
      "  iterations_since_restore: 17\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.11363636363635\n",
      "    ram_util_percent: 98.23181818181818\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 332.0\n",
      "    agent-1: 289.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 220.37\n",
      "    agent-1: 199.17\n",
      "  policy_reward_min:\n",
      "    agent-0: 147.0\n",
      "    agent-1: 127.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11880135333709571\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1063889569529212\n",
      "    mean_inference_ms: 1.9274918086148416\n",
      "    mean_raw_obs_processing_ms: 0.17099795664367576\n",
      "  time_since_restore: 261.77578997612\n",
      "  time_this_iter_s: 15.220023393630981\n",
      "  time_total_s: 261.77578997612\n",
      "  timers:\n",
      "    learn_throughput: 830.459\n",
      "    learn_time_ms: 9630.824\n",
      "    load_throughput: 274056.443\n",
      "    load_time_ms: 29.184\n",
      "    sample_throughput: 1200.754\n",
      "    sample_time_ms: 6660.813\n",
      "    update_time_ms: 7.553\n",
      "  timestamp: 1627273386\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 135966\n",
      "  training_iteration: 17\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    17</td><td style=\"text-align: right;\">         261.776</td><td style=\"text-align: right;\">135966</td><td style=\"text-align: right;\">  419.54</td><td style=\"text-align: right;\">                 496</td><td style=\"text-align: right;\">                 363</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 287928\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-23-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 496.0\n",
      "  episode_reward_mean: 417.65\n",
      "  episode_reward_min: 343.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 1437\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5002204179763794\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012781798839569092\n",
      "          model: {}\n",
      "          policy_loss: -0.03071506880223751\n",
      "          total_loss: 379.0797424316406\n",
      "          vf_explained_var: 0.9072388410568237\n",
      "          vf_loss: 379.0975341796875\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4248342216014862\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012705943547189236\n",
      "          model: {}\n",
      "          policy_loss: -0.02495438978075981\n",
      "          total_loss: 2086.112060546875\n",
      "          vf_explained_var: -1.4420478144927529e-08\n",
      "          vf_loss: 2086.123779296875\n",
      "    num_agent_steps_sampled: 287928\n",
      "    num_agent_steps_trained: 287928\n",
      "    num_steps_sampled: 143964\n",
      "    num_steps_trained: 143964\n",
      "  iterations_since_restore: 18\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.92999999999999\n",
      "    ram_util_percent: 98.235\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 309.0\n",
      "    agent-1: 289.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 214.7\n",
      "    agent-1: 202.95\n",
      "  policy_reward_min:\n",
      "    agent-0: 147.0\n",
      "    agent-1: 114.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11835355195546217\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10590737484294811\n",
      "    mean_inference_ms: 1.915091538148419\n",
      "    mean_raw_obs_processing_ms: 0.17034331295317975\n",
      "  time_since_restore: 275.9898855686188\n",
      "  time_this_iter_s: 14.21409559249878\n",
      "  time_total_s: 275.9898855686188\n",
      "  timers:\n",
      "    learn_throughput: 845.241\n",
      "    learn_time_ms: 9462.387\n",
      "    load_throughput: 277006.976\n",
      "    load_time_ms: 28.873\n",
      "    sample_throughput: 1205.05\n",
      "    sample_time_ms: 6637.067\n",
      "    update_time_ms: 6.29\n",
      "  timestamp: 1627273400\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 143964\n",
      "  training_iteration: 18\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    18</td><td style=\"text-align: right;\">          275.99</td><td style=\"text-align: right;\">143964</td><td style=\"text-align: right;\">  417.65</td><td style=\"text-align: right;\">                 496</td><td style=\"text-align: right;\">                 343</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 303924\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-23-34\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 492.0\n",
      "  episode_reward_mean: 420.48\n",
      "  episode_reward_min: 359.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 1518\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.5033784508705139\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010813777334988117\n",
      "          model: {}\n",
      "          policy_loss: -0.02617633156478405\n",
      "          total_loss: 393.9627990722656\n",
      "          vf_explained_var: 0.9057968258857727\n",
      "          vf_loss: 393.9781188964844\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.42221349477767944\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01242129411548376\n",
      "          model: {}\n",
      "          policy_loss: -0.023730076849460602\n",
      "          total_loss: 2181.250244140625\n",
      "          vf_explained_var: -3.845460838647341e-09\n",
      "          vf_loss: 2181.26123046875\n",
      "    num_agent_steps_sampled: 303924\n",
      "    num_agent_steps_trained: 303924\n",
      "    num_steps_sampled: 151962\n",
      "    num_steps_trained: 151962\n",
      "  iterations_since_restore: 19\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.515\n",
      "    ram_util_percent: 98.04499999999999\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 331.0\n",
      "    agent-1: 284.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 208.04\n",
      "    agent-1: 212.44\n",
      "  policy_reward_min:\n",
      "    agent-0: 151.0\n",
      "    agent-1: 137.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11790114558917625\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10553483983250374\n",
      "    mean_inference_ms: 1.905767799739319\n",
      "    mean_raw_obs_processing_ms: 0.16964876844387003\n",
      "  time_since_restore: 290.122620344162\n",
      "  time_this_iter_s: 14.132734775543213\n",
      "  time_total_s: 290.122620344162\n",
      "  timers:\n",
      "    learn_throughput: 858.48\n",
      "    learn_time_ms: 9316.467\n",
      "    load_throughput: 287675.764\n",
      "    load_time_ms: 27.802\n",
      "    sample_throughput: 1251.082\n",
      "    sample_time_ms: 6392.865\n",
      "    update_time_ms: 6.231\n",
      "  timestamp: 1627273414\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 151962\n",
      "  training_iteration: 19\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    19</td><td style=\"text-align: right;\">         290.123</td><td style=\"text-align: right;\">151962</td><td style=\"text-align: right;\">  420.48</td><td style=\"text-align: right;\">                 492</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 319920\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-23-50\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 475.0\n",
      "  episode_reward_mean: 415.58\n",
      "  episode_reward_min: 342.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 1599\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.50180983543396\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012369298376142979\n",
      "          model: {}\n",
      "          policy_loss: -0.02220752462744713\n",
      "          total_loss: 330.0269775390625\n",
      "          vf_explained_var: 0.8993403911590576\n",
      "          vf_loss: 330.0366516113281\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.40930771827697754\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011794565245509148\n",
      "          model: {}\n",
      "          policy_loss: -0.017842885106801987\n",
      "          total_loss: 2030.086669921875\n",
      "          vf_explained_var: -1.2497747725603858e-08\n",
      "          vf_loss: 2030.0926513671875\n",
      "    num_agent_steps_sampled: 319920\n",
      "    num_agent_steps_trained: 319920\n",
      "    num_steps_sampled: 159960\n",
      "    num_steps_trained: 159960\n",
      "  iterations_since_restore: 20\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.33636363636363\n",
      "    ram_util_percent: 98.25454545454546\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 311.0\n",
      "    agent-1: 284.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 208.94\n",
      "    agent-1: 206.64\n",
      "  policy_reward_min:\n",
      "    agent-0: 140.0\n",
      "    agent-1: 140.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11793410867763959\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10560968633177346\n",
      "    mean_inference_ms: 1.9059984136470143\n",
      "    mean_raw_obs_processing_ms: 0.1695318862248221\n",
      "  time_since_restore: 305.31813502311707\n",
      "  time_this_iter_s: 15.195514678955078\n",
      "  time_total_s: 305.31813502311707\n",
      "  timers:\n",
      "    learn_throughput: 863.604\n",
      "    learn_time_ms: 9261.192\n",
      "    load_throughput: 262281.322\n",
      "    load_time_ms: 30.494\n",
      "    sample_throughput: 1263.782\n",
      "    sample_time_ms: 6328.625\n",
      "    update_time_ms: 5.293\n",
      "  timestamp: 1627273430\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 159960\n",
      "  training_iteration: 20\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         305.318</td><td style=\"text-align: right;\">159960</td><td style=\"text-align: right;\">  415.58</td><td style=\"text-align: right;\">                 475</td><td style=\"text-align: right;\">                 342</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 335916\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-24-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 487.0\n",
      "  episode_reward_mean: 422.61\n",
      "  episode_reward_min: 349.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 1677\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.49518388509750366\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010999547317624092\n",
      "          model: {}\n",
      "          policy_loss: -0.023102272301912308\n",
      "          total_loss: 423.2574157714844\n",
      "          vf_explained_var: 0.9109407663345337\n",
      "          vf_loss: 423.2694091796875\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.41886720061302185\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01299589779227972\n",
      "          model: {}\n",
      "          policy_loss: -0.020916124805808067\n",
      "          total_loss: 2187.648193359375\n",
      "          vf_explained_var: -1.1536382515942023e-08\n",
      "          vf_loss: 2187.656005859375\n",
      "    num_agent_steps_sampled: 335916\n",
      "    num_agent_steps_trained: 335916\n",
      "    num_steps_sampled: 167958\n",
      "    num_steps_trained: 167958\n",
      "  iterations_since_restore: 21\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.63333333333333\n",
      "    ram_util_percent: 98.43333333333335\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 311.0\n",
      "    agent-1: 299.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 212.08\n",
      "    agent-1: 210.53\n",
      "  policy_reward_min:\n",
      "    agent-0: 153.0\n",
      "    agent-1: 140.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11796436593631096\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10556860202157708\n",
      "    mean_inference_ms: 1.9051057798755382\n",
      "    mean_raw_obs_processing_ms: 0.16950005573956792\n",
      "  time_since_restore: 319.81788182258606\n",
      "  time_this_iter_s: 14.499746799468994\n",
      "  time_total_s: 319.81788182258606\n",
      "  timers:\n",
      "    learn_throughput: 882.885\n",
      "    learn_time_ms: 9058.941\n",
      "    load_throughput: 321942.704\n",
      "    load_time_ms: 24.843\n",
      "    sample_throughput: 1284.896\n",
      "    sample_time_ms: 6224.63\n",
      "    update_time_ms: 5.949\n",
      "  timestamp: 1627273444\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 167958\n",
      "  training_iteration: 21\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    21</td><td style=\"text-align: right;\">         319.818</td><td style=\"text-align: right;\">167958</td><td style=\"text-align: right;\">  422.61</td><td style=\"text-align: right;\">                 487</td><td style=\"text-align: right;\">                 349</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 351912\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-24-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 487.0\n",
      "  episode_reward_mean: 418.69\n",
      "  episode_reward_min: 360.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 1758\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4879505932331085\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010824897326529026\n",
      "          model: {}\n",
      "          policy_loss: -0.02461123652756214\n",
      "          total_loss: 334.1821594238281\n",
      "          vf_explained_var: 0.9001011848449707\n",
      "          vf_loss: 334.1958312988281\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.41551920771598816\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011266941204667091\n",
      "          model: {}\n",
      "          policy_loss: -0.022408582270145416\n",
      "          total_loss: 1947.8973388671875\n",
      "          vf_explained_var: -1.1536382515942023e-08\n",
      "          vf_loss: 1947.9080810546875\n",
      "    num_agent_steps_sampled: 351912\n",
      "    num_agent_steps_trained: 351912\n",
      "    num_steps_sampled: 175956\n",
      "    num_steps_trained: 175956\n",
      "  iterations_since_restore: 22\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.14000000000001\n",
      "    ram_util_percent: 98.39000000000001\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 340.0\n",
      "    agent-1: 272.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 210.77\n",
      "    agent-1: 207.92\n",
      "  policy_reward_min:\n",
      "    agent-0: 155.0\n",
      "    agent-1: 135.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11753924575358222\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1052351504311935\n",
      "    mean_inference_ms: 1.8970033278620042\n",
      "    mean_raw_obs_processing_ms: 0.16884929381384445\n",
      "  time_since_restore: 334.04767870903015\n",
      "  time_this_iter_s: 14.229796886444092\n",
      "  time_total_s: 334.04767870903015\n",
      "  timers:\n",
      "    learn_throughput: 888.554\n",
      "    learn_time_ms: 9001.143\n",
      "    load_throughput: 330291.374\n",
      "    load_time_ms: 24.215\n",
      "    sample_throughput: 1324.018\n",
      "    sample_time_ms: 6040.704\n",
      "    update_time_ms: 6.297\n",
      "  timestamp: 1627273459\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 175956\n",
      "  training_iteration: 22\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    22</td><td style=\"text-align: right;\">         334.048</td><td style=\"text-align: right;\">175956</td><td style=\"text-align: right;\">  418.69</td><td style=\"text-align: right;\">                 487</td><td style=\"text-align: right;\">                 360</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 367908\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-24-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 476.0\n",
      "  episode_reward_mean: 416.07\n",
      "  episode_reward_min: 362.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 1839\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.48041394352912903\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011771474964916706\n",
      "          model: {}\n",
      "          policy_loss: -0.025291211903095245\n",
      "          total_loss: 394.431396484375\n",
      "          vf_explained_var: 0.8900811076164246\n",
      "          vf_loss: 394.44476318359375\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.41243183612823486\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01206279918551445\n",
      "          model: {}\n",
      "          policy_loss: -0.0197091493755579\n",
      "          total_loss: 2066.10107421875\n",
      "          vf_explained_var: -8.652286886956517e-09\n",
      "          vf_loss: 2066.108642578125\n",
      "    num_agent_steps_sampled: 367908\n",
      "    num_agent_steps_trained: 367908\n",
      "    num_steps_sampled: 183954\n",
      "    num_steps_trained: 183954\n",
      "  iterations_since_restore: 23\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.2\n",
      "    ram_util_percent: 98.21428571428571\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 340.0\n",
      "    agent-1: 260.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 212.46\n",
      "    agent-1: 203.61\n",
      "  policy_reward_min:\n",
      "    agent-0: 155.0\n",
      "    agent-1: 135.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1171692154310005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10490177470199337\n",
      "    mean_inference_ms: 1.888414768695065\n",
      "    mean_raw_obs_processing_ms: 0.1683604355186273\n",
      "  time_since_restore: 348.40726470947266\n",
      "  time_this_iter_s: 14.359586000442505\n",
      "  time_total_s: 348.40726470947266\n",
      "  timers:\n",
      "    learn_throughput: 906.732\n",
      "    learn_time_ms: 8820.691\n",
      "    load_throughput: 334452.394\n",
      "    load_time_ms: 23.914\n",
      "    sample_throughput: 1344.45\n",
      "    sample_time_ms: 5948.901\n",
      "    update_time_ms: 6.242\n",
      "  timestamp: 1627273473\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 183954\n",
      "  training_iteration: 23\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    23</td><td style=\"text-align: right;\">         348.407</td><td style=\"text-align: right;\">183954</td><td style=\"text-align: right;\">  416.07</td><td style=\"text-align: right;\">                 476</td><td style=\"text-align: right;\">                 362</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 383904\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-24-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 475.0\n",
      "  episode_reward_mean: 419.75\n",
      "  episode_reward_min: 361.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 1917\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.49678874015808105\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011160513386130333\n",
      "          model: {}\n",
      "          policy_loss: -0.026624765247106552\n",
      "          total_loss: 364.4513854980469\n",
      "          vf_explained_var: 0.9277566075325012\n",
      "          vf_loss: 364.46673583984375\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4138137400150299\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01248707715421915\n",
      "          model: {}\n",
      "          policy_loss: -0.022394727915525436\n",
      "          total_loss: 1999.63671875\n",
      "          vf_explained_var: -3.845460838647341e-09\n",
      "          vf_loss: 1999.6466064453125\n",
      "    num_agent_steps_sampled: 383904\n",
      "    num_agent_steps_trained: 383904\n",
      "    num_steps_sampled: 191952\n",
      "    num_steps_trained: 191952\n",
      "  iterations_since_restore: 24\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.419047619047625\n",
      "    ram_util_percent: 98.25714285714287\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 342.0\n",
      "    agent-1: 274.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 214.75\n",
      "    agent-1: 205.0\n",
      "  policy_reward_min:\n",
      "    agent-0: 152.0\n",
      "    agent-1: 117.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11724885646109101\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10491813328547417\n",
      "    mean_inference_ms: 1.8889258875138488\n",
      "    mean_raw_obs_processing_ms: 0.16850676507750065\n",
      "  time_since_restore: 363.5930347442627\n",
      "  time_this_iter_s: 15.185770034790039\n",
      "  time_total_s: 363.5930347442627\n",
      "  timers:\n",
      "    learn_throughput: 911.231\n",
      "    learn_time_ms: 8777.137\n",
      "    load_throughput: 299365.622\n",
      "    load_time_ms: 26.716\n",
      "    sample_throughput: 1346.39\n",
      "    sample_time_ms: 5940.329\n",
      "    update_time_ms: 5.926\n",
      "  timestamp: 1627273488\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 191952\n",
      "  training_iteration: 24\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    24</td><td style=\"text-align: right;\">         363.593</td><td style=\"text-align: right;\">191952</td><td style=\"text-align: right;\">  419.75</td><td style=\"text-align: right;\">                 475</td><td style=\"text-align: right;\">                 361</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 399900\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-25-03\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 498.0\n",
      "  episode_reward_mean: 418.24\n",
      "  episode_reward_min: 350.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 1998\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.47878965735435486\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00977051630616188\n",
      "          model: {}\n",
      "          policy_loss: -0.021930230781435966\n",
      "          total_loss: 366.6459655761719\n",
      "          vf_explained_var: 0.9023867249488831\n",
      "          vf_loss: 366.65802001953125\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4259341359138489\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010957092978060246\n",
      "          model: {}\n",
      "          policy_loss: -0.01927340403199196\n",
      "          total_loss: 1994.0009765625\n",
      "          vf_explained_var: -1.5381843354589364e-08\n",
      "          vf_loss: 1994.0093994140625\n",
      "    num_agent_steps_sampled: 399900\n",
      "    num_agent_steps_trained: 399900\n",
      "    num_steps_sampled: 199950\n",
      "    num_steps_trained: 199950\n",
      "  iterations_since_restore: 25\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.41904761904762\n",
      "    ram_util_percent: 98.31428571428573\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 311.0\n",
      "    agent-1: 287.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 218.47\n",
      "    agent-1: 199.77\n",
      "  policy_reward_min:\n",
      "    agent-0: 147.0\n",
      "    agent-1: 127.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11696544096359478\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10467966921260215\n",
      "    mean_inference_ms: 1.883203107736626\n",
      "    mean_raw_obs_processing_ms: 0.1680781123533302\n",
      "  time_since_restore: 377.89818120002747\n",
      "  time_this_iter_s: 14.30514645576477\n",
      "  time_total_s: 377.89818120002747\n",
      "  timers:\n",
      "    learn_throughput: 919.834\n",
      "    learn_time_ms: 8695.048\n",
      "    load_throughput: 295051.637\n",
      "    load_time_ms: 27.107\n",
      "    sample_throughput: 1364.858\n",
      "    sample_time_ms: 5859.95\n",
      "    update_time_ms: 5.946\n",
      "  timestamp: 1627273503\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 199950\n",
      "  training_iteration: 25\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    25</td><td style=\"text-align: right;\">         377.898</td><td style=\"text-align: right;\">199950</td><td style=\"text-align: right;\">  418.24</td><td style=\"text-align: right;\">                 498</td><td style=\"text-align: right;\">                 350</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 415896\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-25-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 498.0\n",
      "  episode_reward_mean: 418.64\n",
      "  episode_reward_min: 350.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 2079\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4692426323890686\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012027472257614136\n",
      "          model: {}\n",
      "          policy_loss: -0.027322715148329735\n",
      "          total_loss: 373.313232421875\n",
      "          vf_explained_var: 0.8980517983436584\n",
      "          vf_loss: 373.3283386230469\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.43850985169410706\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010453908704221249\n",
      "          model: {}\n",
      "          policy_loss: -0.016568822786211967\n",
      "          total_loss: 1917.339599609375\n",
      "          vf_explained_var: -2.8840956289855058e-09\n",
      "          vf_loss: 1917.3455810546875\n",
      "    num_agent_steps_sampled: 415896\n",
      "    num_agent_steps_trained: 415896\n",
      "    num_steps_sampled: 207948\n",
      "    num_steps_trained: 207948\n",
      "  iterations_since_restore: 26\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.22380952380953\n",
      "    ram_util_percent: 98.34761904761906\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 311.0\n",
      "    agent-1: 273.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 223.37\n",
      "    agent-1: 195.27\n",
      "  policy_reward_min:\n",
      "    agent-0: 139.0\n",
      "    agent-1: 127.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11672607713906885\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1044421161790738\n",
      "    mean_inference_ms: 1.8781077334498397\n",
      "    mean_raw_obs_processing_ms: 0.16778774881288197\n",
      "  time_since_restore: 392.50031185150146\n",
      "  time_this_iter_s: 14.602130651473999\n",
      "  time_total_s: 392.50031185150146\n",
      "  timers:\n",
      "    learn_throughput: 920.617\n",
      "    learn_time_ms: 8687.654\n",
      "    load_throughput: 320360.865\n",
      "    load_time_ms: 24.966\n",
      "    sample_throughput: 1363.5\n",
      "    sample_time_ms: 5865.788\n",
      "    update_time_ms: 5.938\n",
      "  timestamp: 1627273517\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 207948\n",
      "  training_iteration: 26\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    26</td><td style=\"text-align: right;\">           392.5</td><td style=\"text-align: right;\">207948</td><td style=\"text-align: right;\">  418.64</td><td style=\"text-align: right;\">                 498</td><td style=\"text-align: right;\">                 350</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 431892\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-25-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 475.0\n",
      "  episode_reward_mean: 417.67\n",
      "  episode_reward_min: 369.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 2157\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4671894311904907\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011732826009392738\n",
      "          model: {}\n",
      "          policy_loss: -0.02497178502380848\n",
      "          total_loss: 273.74920654296875\n",
      "          vf_explained_var: 0.9229036569595337\n",
      "          vf_loss: 273.7623291015625\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.42537668347358704\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011145790107548237\n",
      "          model: {}\n",
      "          policy_loss: -0.020728684961795807\n",
      "          total_loss: 1725.383544921875\n",
      "          vf_explained_var: -4.806826048309176e-09\n",
      "          vf_loss: 1725.3931884765625\n",
      "    num_agent_steps_sampled: 431892\n",
      "    num_agent_steps_trained: 431892\n",
      "    num_steps_sampled: 215946\n",
      "    num_steps_trained: 215946\n",
      "  iterations_since_restore: 27\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.69\n",
      "    ram_util_percent: 98.17\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 318.0\n",
      "    agent-1: 273.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 228.61\n",
      "    agent-1: 189.06\n",
      "  policy_reward_min:\n",
      "    agent-0: 167.0\n",
      "    agent-1: 133.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11663257299462224\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10427579163708176\n",
      "    mean_inference_ms: 1.8741288561982297\n",
      "    mean_raw_obs_processing_ms: 0.1676796329794425\n",
      "  time_since_restore: 406.53540086746216\n",
      "  time_this_iter_s: 14.035089015960693\n",
      "  time_total_s: 406.53540086746216\n",
      "  timers:\n",
      "    learn_throughput: 931.991\n",
      "    learn_time_ms: 8581.628\n",
      "    load_throughput: 333259.588\n",
      "    load_time_ms: 23.999\n",
      "    sample_throughput: 1366.198\n",
      "    sample_time_ms: 5854.201\n",
      "    update_time_ms: 5.978\n",
      "  timestamp: 1627273531\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 215946\n",
      "  training_iteration: 27\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    27</td><td style=\"text-align: right;\">         406.535</td><td style=\"text-align: right;\">215946</td><td style=\"text-align: right;\">  417.67</td><td style=\"text-align: right;\">                 475</td><td style=\"text-align: right;\">                 369</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 447888\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-25-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 475.0\n",
      "  episode_reward_mean: 416.58\n",
      "  episode_reward_min: 366.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 2238\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.44279366731643677\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01171612087637186\n",
      "          model: {}\n",
      "          policy_loss: -0.025281231850385666\n",
      "          total_loss: 337.9268493652344\n",
      "          vf_explained_var: 0.9053969979286194\n",
      "          vf_loss: 337.9402770996094\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4150516986846924\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010015402920544147\n",
      "          model: {}\n",
      "          policy_loss: -0.017311712726950645\n",
      "          total_loss: 1810.4095458984375\n",
      "          vf_explained_var: -8.652286886956517e-09\n",
      "          vf_loss: 1810.4166259765625\n",
      "    num_agent_steps_sampled: 447888\n",
      "    num_agent_steps_trained: 447888\n",
      "    num_steps_sampled: 223944\n",
      "    num_steps_trained: 223944\n",
      "  iterations_since_restore: 28\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.166666666666664\n",
      "    ram_util_percent: 98.33333333333336\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 349.0\n",
      "    agent-1: 259.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 230.94\n",
      "    agent-1: 185.64\n",
      "  policy_reward_min:\n",
      "    agent-0: 154.0\n",
      "    agent-1: 101.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1164383317556943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10415950618732137\n",
      "    mean_inference_ms: 1.870923686382684\n",
      "    mean_raw_obs_processing_ms: 0.16735878749974384\n",
      "  time_since_restore: 421.28805780410767\n",
      "  time_this_iter_s: 14.752656936645508\n",
      "  time_total_s: 421.28805780410767\n",
      "  timers:\n",
      "    learn_throughput: 929.372\n",
      "    learn_time_ms: 8605.808\n",
      "    load_throughput: 327629.115\n",
      "    load_time_ms: 24.412\n",
      "    sample_throughput: 1359.666\n",
      "    sample_time_ms: 5882.326\n",
      "    update_time_ms: 6.338\n",
      "  timestamp: 1627273546\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 223944\n",
      "  training_iteration: 28\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    28</td><td style=\"text-align: right;\">         421.288</td><td style=\"text-align: right;\">223944</td><td style=\"text-align: right;\">  416.58</td><td style=\"text-align: right;\">                 475</td><td style=\"text-align: right;\">                 366</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 463884\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-26-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 498.0\n",
      "  episode_reward_mean: 419.03\n",
      "  episode_reward_min: 358.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 2319\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4306054711341858\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010086486116051674\n",
      "          model: {}\n",
      "          policy_loss: -0.024644965305924416\n",
      "          total_loss: 436.92205810546875\n",
      "          vf_explained_var: 0.9093042612075806\n",
      "          vf_loss: 436.93646240234375\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.41942790150642395\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012206478975713253\n",
      "          model: {}\n",
      "          policy_loss: -0.0156849417835474\n",
      "          total_loss: 1829.59326171875\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 1829.5968017578125\n",
      "    num_agent_steps_sampled: 463884\n",
      "    num_agent_steps_trained: 463884\n",
      "    num_steps_sampled: 231942\n",
      "    num_steps_trained: 231942\n",
      "  iterations_since_restore: 29\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.81818181818183\n",
      "    ram_util_percent: 98.38181818181819\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 356.0\n",
      "    agent-1: 265.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 239.64\n",
      "    agent-1: 179.39\n",
      "  policy_reward_min:\n",
      "    agent-0: 163.0\n",
      "    agent-1: 112.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11648833412216093\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10418933514512979\n",
      "    mean_inference_ms: 1.870770587065396\n",
      "    mean_raw_obs_processing_ms: 0.16735060887719386\n",
      "  time_since_restore: 436.491348028183\n",
      "  time_this_iter_s: 15.203290224075317\n",
      "  time_total_s: 436.491348028183\n",
      "  timers:\n",
      "    learn_throughput: 922.423\n",
      "    learn_time_ms: 8670.643\n",
      "    load_throughput: 322842.77\n",
      "    load_time_ms: 24.774\n",
      "    sample_throughput: 1350.294\n",
      "    sample_time_ms: 5923.154\n",
      "    update_time_ms: 6.743\n",
      "  timestamp: 1627273561\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 231942\n",
      "  training_iteration: 29\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    29</td><td style=\"text-align: right;\">         436.491</td><td style=\"text-align: right;\">231942</td><td style=\"text-align: right;\">  419.03</td><td style=\"text-align: right;\">                 498</td><td style=\"text-align: right;\">                 358</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 479880\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-26-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 485.0\n",
      "  episode_reward_mean: 415.74\n",
      "  episode_reward_min: 371.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 2397\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4373614192008972\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009538330137729645\n",
      "          model: {}\n",
      "          policy_loss: -0.021356092765927315\n",
      "          total_loss: 398.82208251953125\n",
      "          vf_explained_var: 0.9067662358283997\n",
      "          vf_loss: 398.833740234375\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.41027626395225525\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010543686337769032\n",
      "          model: {}\n",
      "          policy_loss: -0.01629951223731041\n",
      "          total_loss: 1699.9580078125\n",
      "          vf_explained_var: -4.806826048309176e-09\n",
      "          vf_loss: 1699.963623046875\n",
      "    num_agent_steps_sampled: 479880\n",
      "    num_agent_steps_trained: 479880\n",
      "    num_steps_sampled: 239940\n",
      "    num_steps_trained: 239940\n",
      "  iterations_since_restore: 30\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.29\n",
      "    ram_util_percent: 98.39000000000001\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 356.0\n",
      "    agent-1: 246.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 238.22\n",
      "    agent-1: 177.52\n",
      "  policy_reward_min:\n",
      "    agent-0: 155.0\n",
      "    agent-1: 100.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11635037316438432\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10404546040724359\n",
      "    mean_inference_ms: 1.8666395994105531\n",
      "    mean_raw_obs_processing_ms: 0.16717221193842968\n",
      "  time_since_restore: 450.974534034729\n",
      "  time_this_iter_s: 14.48318600654602\n",
      "  time_total_s: 450.974534034729\n",
      "  timers:\n",
      "    learn_throughput: 921.874\n",
      "    learn_time_ms: 8675.805\n",
      "    load_throughput: 378058.134\n",
      "    load_time_ms: 21.155\n",
      "    sample_throughput: 1367.315\n",
      "    sample_time_ms: 5849.421\n",
      "    update_time_ms: 7.064\n",
      "  timestamp: 1627273576\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 239940\n",
      "  training_iteration: 30\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    30</td><td style=\"text-align: right;\">         450.975</td><td style=\"text-align: right;\">239940</td><td style=\"text-align: right;\">  415.74</td><td style=\"text-align: right;\">                 485</td><td style=\"text-align: right;\">                 371</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 495876\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-26-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 475.0\n",
      "  episode_reward_mean: 412.54\n",
      "  episode_reward_min: 364.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 2478\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.446271151304245\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011439556255936623\n",
      "          model: {}\n",
      "          policy_loss: -0.023191988468170166\n",
      "          total_loss: 390.84185791015625\n",
      "          vf_explained_var: 0.915631890296936\n",
      "          vf_loss: 390.8534851074219\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4057779014110565\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011251180432736874\n",
      "          model: {}\n",
      "          policy_loss: -0.01654532365500927\n",
      "          total_loss: 1629.1158447265625\n",
      "          vf_explained_var: -2.8840956289855058e-09\n",
      "          vf_loss: 1629.12060546875\n",
      "    num_agent_steps_sampled: 495876\n",
      "    num_agent_steps_trained: 495876\n",
      "    num_steps_sampled: 247938\n",
      "    num_steps_trained: 247938\n",
      "  iterations_since_restore: 31\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.89\n",
      "    ram_util_percent: 98.36000000000001\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 346.0\n",
      "    agent-1: 246.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 236.57\n",
      "    agent-1: 175.97\n",
      "  policy_reward_min:\n",
      "    agent-0: 174.0\n",
      "    agent-1: 96.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11613661959938254\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10390519666732433\n",
      "    mean_inference_ms: 1.862594711736836\n",
      "    mean_raw_obs_processing_ms: 0.1667861245809558\n",
      "  time_since_restore: 464.8472406864166\n",
      "  time_this_iter_s: 13.872706651687622\n",
      "  time_total_s: 464.8472406864166\n",
      "  timers:\n",
      "    learn_throughput: 923.433\n",
      "    learn_time_ms: 8661.158\n",
      "    load_throughput: 390082.972\n",
      "    load_time_ms: 20.503\n",
      "    sample_throughput: 1377.913\n",
      "    sample_time_ms: 5804.43\n",
      "    update_time_ms: 5.251\n",
      "  timestamp: 1627273590\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 247938\n",
      "  training_iteration: 31\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    31</td><td style=\"text-align: right;\">         464.847</td><td style=\"text-align: right;\">247938</td><td style=\"text-align: right;\">  412.54</td><td style=\"text-align: right;\">                 475</td><td style=\"text-align: right;\">                 364</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 511872\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-26-44\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 492.0\n",
      "  episode_reward_mean: 414.36\n",
      "  episode_reward_min: 359.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 2559\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4214867651462555\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012173079885542393\n",
      "          model: {}\n",
      "          policy_loss: -0.02358979918062687\n",
      "          total_loss: 296.13385009765625\n",
      "          vf_explained_var: 0.9316717386245728\n",
      "          vf_loss: 296.1451416015625\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.41488760709762573\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009304298087954521\n",
      "          model: {}\n",
      "          policy_loss: -0.014616894535720348\n",
      "          total_loss: 1642.26171875\n",
      "          vf_explained_var: -9.613652096618353e-09\n",
      "          vf_loss: 1642.2669677734375\n",
      "    num_agent_steps_sampled: 511872\n",
      "    num_agent_steps_trained: 511872\n",
      "    num_steps_sampled: 255936\n",
      "    num_steps_trained: 255936\n",
      "  iterations_since_restore: 32\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.46\n",
      "    ram_util_percent: 98.35499999999999\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 337.0\n",
      "    agent-1: 248.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 245.83\n",
      "    agent-1: 168.53\n",
      "  policy_reward_min:\n",
      "    agent-0: 174.0\n",
      "    agent-1: 111.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11590387408398005\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10367454622438746\n",
      "    mean_inference_ms: 1.857421079315265\n",
      "    mean_raw_obs_processing_ms: 0.1664087455482078\n",
      "  time_since_restore: 479.04243564605713\n",
      "  time_this_iter_s: 14.195194959640503\n",
      "  time_total_s: 479.04243564605713\n",
      "  timers:\n",
      "    learn_throughput: 923.819\n",
      "    learn_time_ms: 8657.543\n",
      "    load_throughput: 388032.201\n",
      "    load_time_ms: 20.612\n",
      "    sample_throughput: 1377.771\n",
      "    sample_time_ms: 5805.028\n",
      "    update_time_ms: 4.85\n",
      "  timestamp: 1627273604\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 255936\n",
      "  training_iteration: 32\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    32</td><td style=\"text-align: right;\">         479.042</td><td style=\"text-align: right;\">255936</td><td style=\"text-align: right;\">  414.36</td><td style=\"text-align: right;\">                 492</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 527868\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-26-59\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 492.0\n",
      "  episode_reward_mean: 416.22\n",
      "  episode_reward_min: 359.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 2637\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4104391038417816\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010475213639438152\n",
      "          model: {}\n",
      "          policy_loss: -0.02290097065269947\n",
      "          total_loss: 414.4047546386719\n",
      "          vf_explained_var: 0.8982279300689697\n",
      "          vf_loss: 414.4170227050781\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.409149169921875\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009981212206184864\n",
      "          model: {}\n",
      "          policy_loss: -0.011877741664648056\n",
      "          total_loss: 1699.728271484375\n",
      "          vf_explained_var: -1.1536382515942023e-08\n",
      "          vf_loss: 1699.7301025390625\n",
      "    num_agent_steps_sampled: 527868\n",
      "    num_agent_steps_trained: 527868\n",
      "    num_steps_sampled: 263934\n",
      "    num_steps_trained: 263934\n",
      "  iterations_since_restore: 33\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.98095238095239\n",
      "    ram_util_percent: 98.24761904761905\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 331.0\n",
      "    agent-1: 259.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 247.36\n",
      "    agent-1: 168.86\n",
      "  policy_reward_min:\n",
      "    agent-0: 180.0\n",
      "    agent-1: 116.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11576750751260181\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10349614414926979\n",
      "    mean_inference_ms: 1.8535158647837355\n",
      "    mean_raw_obs_processing_ms: 0.16626949330751026\n",
      "  time_since_restore: 493.56796002388\n",
      "  time_this_iter_s: 14.525524377822876\n",
      "  time_total_s: 493.56796002388\n",
      "  timers:\n",
      "    learn_throughput: 922.928\n",
      "    learn_time_ms: 8665.899\n",
      "    load_throughput: 397637.404\n",
      "    load_time_ms: 20.114\n",
      "    sample_throughput: 1375.365\n",
      "    sample_time_ms: 5815.183\n",
      "    update_time_ms: 3.887\n",
      "  timestamp: 1627273619\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 263934\n",
      "  training_iteration: 33\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    33</td><td style=\"text-align: right;\">         493.568</td><td style=\"text-align: right;\">263934</td><td style=\"text-align: right;\">  416.22</td><td style=\"text-align: right;\">                 492</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 543864\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-27-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 481.0\n",
      "  episode_reward_mean: 407.37\n",
      "  episode_reward_min: 357.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 2718\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3904101550579071\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01346401497721672\n",
      "          model: {}\n",
      "          policy_loss: -0.02374507673084736\n",
      "          total_loss: 265.3645935058594\n",
      "          vf_explained_var: 0.9472312331199646\n",
      "          vf_loss: 265.3747253417969\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3994404375553131\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009894931688904762\n",
      "          model: {}\n",
      "          policy_loss: -0.018834557384252548\n",
      "          total_loss: 1480.8189697265625\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 1480.828125\n",
      "    num_agent_steps_sampled: 543864\n",
      "    num_agent_steps_trained: 543864\n",
      "    num_steps_sampled: 271932\n",
      "    num_steps_trained: 271932\n",
      "  iterations_since_restore: 34\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.660000000000004\n",
      "    ram_util_percent: 98.16000000000001\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 332.0\n",
      "    agent-1: 278.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 245.56\n",
      "    agent-1: 161.81\n",
      "  policy_reward_min:\n",
      "    agent-0: 181.0\n",
      "    agent-1: 113.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11549450225136614\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10329491703633678\n",
      "    mean_inference_ms: 1.8489145882082385\n",
      "    mean_raw_obs_processing_ms: 0.1658552383670212\n",
      "  time_since_restore: 507.46353578567505\n",
      "  time_this_iter_s: 13.895575761795044\n",
      "  time_total_s: 507.46353578567505\n",
      "  timers:\n",
      "    learn_throughput: 927.167\n",
      "    learn_time_ms: 8626.279\n",
      "    load_throughput: 471716.102\n",
      "    load_time_ms: 16.955\n",
      "    sample_throughput: 1395.965\n",
      "    sample_time_ms: 5729.371\n",
      "    update_time_ms: 3.931\n",
      "  timestamp: 1627273632\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 271932\n",
      "  training_iteration: 34\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    34</td><td style=\"text-align: right;\">         507.464</td><td style=\"text-align: right;\">271932</td><td style=\"text-align: right;\">  407.37</td><td style=\"text-align: right;\">                 481</td><td style=\"text-align: right;\">                 357</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 559860\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-27-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 481.0\n",
      "  episode_reward_mean: 409.56\n",
      "  episode_reward_min: 360.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 2799\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3926931917667389\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009313439950346947\n",
      "          model: {}\n",
      "          policy_loss: -0.01963900588452816\n",
      "          total_loss: 394.74554443359375\n",
      "          vf_explained_var: 0.901046872138977\n",
      "          vf_loss: 394.7557678222656\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.39903539419174194\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010356412269175053\n",
      "          model: {}\n",
      "          policy_loss: -0.015449187718331814\n",
      "          total_loss: 1535.31103515625\n",
      "          vf_explained_var: -1.1536382515942023e-08\n",
      "          vf_loss: 1535.3157958984375\n",
      "    num_agent_steps_sampled: 559860\n",
      "    num_agent_steps_trained: 559860\n",
      "    num_steps_sampled: 279930\n",
      "    num_steps_trained: 279930\n",
      "  iterations_since_restore: 35\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.26666666666667\n",
      "    ram_util_percent: 98.19047619047619\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 349.0\n",
      "    agent-1: 246.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 252.58\n",
      "    agent-1: 156.98\n",
      "  policy_reward_min:\n",
      "    agent-0: 190.0\n",
      "    agent-1: 81.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11547207034505778\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10326488515042678\n",
      "    mean_inference_ms: 1.84798049602993\n",
      "    mean_raw_obs_processing_ms: 0.16580320979019011\n",
      "  time_since_restore: 522.2673003673553\n",
      "  time_this_iter_s: 14.803764581680298\n",
      "  time_total_s: 522.2673003673553\n",
      "  timers:\n",
      "    learn_throughput: 927.377\n",
      "    learn_time_ms: 8624.325\n",
      "    load_throughput: 474319.98\n",
      "    load_time_ms: 16.862\n",
      "    sample_throughput: 1383.385\n",
      "    sample_time_ms: 5781.472\n",
      "    update_time_ms: 3.899\n",
      "  timestamp: 1627273647\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 279930\n",
      "  training_iteration: 35\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    35</td><td style=\"text-align: right;\">         522.267</td><td style=\"text-align: right;\">279930</td><td style=\"text-align: right;\">  409.56</td><td style=\"text-align: right;\">                 481</td><td style=\"text-align: right;\">                 360</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 575856\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-27-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 481.0\n",
      "  episode_reward_mean: 407.73\n",
      "  episode_reward_min: 353.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 2877\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.36743873357772827\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011000602506101131\n",
      "          model: {}\n",
      "          policy_loss: -0.017738547176122665\n",
      "          total_loss: 478.0013427734375\n",
      "          vf_explained_var: 0.900433361530304\n",
      "          vf_loss: 478.0079345703125\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3966531455516815\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010578335262835026\n",
      "          model: {}\n",
      "          policy_loss: -0.015214414335787296\n",
      "          total_loss: 1512.1903076171875\n",
      "          vf_explained_var: 5.7681912579710115e-09\n",
      "          vf_loss: 1512.1947021484375\n",
      "    num_agent_steps_sampled: 575856\n",
      "    num_agent_steps_trained: 575856\n",
      "    num_steps_sampled: 287928\n",
      "    num_steps_trained: 287928\n",
      "  iterations_since_restore: 36\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.85714285714287\n",
      "    ram_util_percent: 98.37142857142858\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 345.0\n",
      "    agent-1: 238.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 251.69\n",
      "    agent-1: 156.04\n",
      "  policy_reward_min:\n",
      "    agent-0: 193.0\n",
      "    agent-1: 100.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11537235592861002\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10312389933679604\n",
      "    mean_inference_ms: 1.8448177661413028\n",
      "    mean_raw_obs_processing_ms: 0.16567955677672308\n",
      "  time_since_restore: 536.7298221588135\n",
      "  time_this_iter_s: 14.46252179145813\n",
      "  time_total_s: 536.7298221588135\n",
      "  timers:\n",
      "    learn_throughput: 927.428\n",
      "    learn_time_ms: 8623.851\n",
      "    load_throughput: 404812.508\n",
      "    load_time_ms: 19.757\n",
      "    sample_throughput: 1387.325\n",
      "    sample_time_ms: 5765.053\n",
      "    update_time_ms: 3.904\n",
      "  timestamp: 1627273662\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 287928\n",
      "  training_iteration: 36\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    36</td><td style=\"text-align: right;\">          536.73</td><td style=\"text-align: right;\">287928</td><td style=\"text-align: right;\">  407.73</td><td style=\"text-align: right;\">                 481</td><td style=\"text-align: right;\">                 353</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 591852\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-27-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 482.0\n",
      "  episode_reward_mean: 414.65\n",
      "  episode_reward_min: 355.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 2958\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3565111756324768\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009375002235174179\n",
      "          model: {}\n",
      "          policy_loss: -0.017311180010437965\n",
      "          total_loss: 321.8431701660156\n",
      "          vf_explained_var: 0.9309085607528687\n",
      "          vf_loss: 321.8509216308594\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.39709779620170593\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008891678415238857\n",
      "          model: {}\n",
      "          policy_loss: -0.017298992723226547\n",
      "          total_loss: 1366.310302734375\n",
      "          vf_explained_var: 9.613652096618353e-10\n",
      "          vf_loss: 1366.3184814453125\n",
      "    num_agent_steps_sampled: 591852\n",
      "    num_agent_steps_trained: 591852\n",
      "    num_steps_sampled: 295926\n",
      "    num_steps_trained: 295926\n",
      "  iterations_since_restore: 37\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.43333333333334\n",
      "    ram_util_percent: 98.44761904761906\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 345.0\n",
      "    agent-1: 238.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 264.6\n",
      "    agent-1: 150.05\n",
      "  policy_reward_min:\n",
      "    agent-0: 193.0\n",
      "    agent-1: 99.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11524107078555415\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10305264712628269\n",
      "    mean_inference_ms: 1.8430172718643607\n",
      "    mean_raw_obs_processing_ms: 0.16549603898588153\n",
      "  time_since_restore: 551.3124339580536\n",
      "  time_this_iter_s: 14.582611799240112\n",
      "  time_total_s: 551.3124339580536\n",
      "  timers:\n",
      "    learn_throughput: 921.347\n",
      "    learn_time_ms: 8680.772\n",
      "    load_throughput: 431419.859\n",
      "    load_time_ms: 18.539\n",
      "    sample_throughput: 1387.573\n",
      "    sample_time_ms: 5764.021\n",
      "    update_time_ms: 3.863\n",
      "  timestamp: 1627273676\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 295926\n",
      "  training_iteration: 37\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    37</td><td style=\"text-align: right;\">         551.312</td><td style=\"text-align: right;\">295926</td><td style=\"text-align: right;\">  414.65</td><td style=\"text-align: right;\">                 482</td><td style=\"text-align: right;\">                 355</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 607848\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-28-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 484.0\n",
      "  episode_reward_mean: 421.87\n",
      "  episode_reward_min: 365.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 3039\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3496035039424896\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010366411879658699\n",
      "          model: {}\n",
      "          policy_loss: -0.02243584208190441\n",
      "          total_loss: 371.18487548828125\n",
      "          vf_explained_var: 0.9297465682029724\n",
      "          vf_loss: 371.1968078613281\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4001935124397278\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011350423097610474\n",
      "          model: {}\n",
      "          policy_loss: -0.015326712280511856\n",
      "          total_loss: 1617.9779052734375\n",
      "          vf_explained_var: 1.9227304193236705e-09\n",
      "          vf_loss: 1617.9818115234375\n",
      "    num_agent_steps_sampled: 607848\n",
      "    num_agent_steps_trained: 607848\n",
      "    num_steps_sampled: 303924\n",
      "    num_steps_trained: 303924\n",
      "  iterations_since_restore: 38\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.14210526315789\n",
      "    ram_util_percent: 98.29473684210527\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 359.0\n",
      "    agent-1: 248.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 266.11\n",
      "    agent-1: 155.76\n",
      "  policy_reward_min:\n",
      "    agent-0: 191.0\n",
      "    agent-1: 99.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11509226622056229\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10291415438219623\n",
      "    mean_inference_ms: 1.839805314339005\n",
      "    mean_raw_obs_processing_ms: 0.16531985740915384\n",
      "  time_since_restore: 565.1311123371124\n",
      "  time_this_iter_s: 13.818678379058838\n",
      "  time_total_s: 565.1311123371124\n",
      "  timers:\n",
      "    learn_throughput: 928.835\n",
      "    learn_time_ms: 8610.789\n",
      "    load_throughput: 433333.033\n",
      "    load_time_ms: 18.457\n",
      "    sample_throughput: 1392.951\n",
      "    sample_time_ms: 5741.765\n",
      "    update_time_ms: 3.479\n",
      "  timestamp: 1627273690\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 303924\n",
      "  training_iteration: 38\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    38</td><td style=\"text-align: right;\">         565.131</td><td style=\"text-align: right;\">303924</td><td style=\"text-align: right;\">  421.87</td><td style=\"text-align: right;\">                 484</td><td style=\"text-align: right;\">                 365</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 623844\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-28-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 497.0\n",
      "  episode_reward_mean: 420.07\n",
      "  episode_reward_min: 366.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 3117\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.330331414937973\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008779103867709637\n",
      "          model: {}\n",
      "          policy_loss: -0.01809767447412014\n",
      "          total_loss: 393.6966247558594\n",
      "          vf_explained_var: 0.9157066941261292\n",
      "          vf_loss: 393.7057800292969\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3967960774898529\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01017904281616211\n",
      "          model: {}\n",
      "          policy_loss: -0.013102187775075436\n",
      "          total_loss: 1379.41796875\n",
      "          vf_explained_var: -4.806826048309176e-09\n",
      "          vf_loss: 1379.4207763671875\n",
      "    num_agent_steps_sampled: 623844\n",
      "    num_agent_steps_trained: 623844\n",
      "    num_steps_sampled: 311922\n",
      "    num_steps_trained: 311922\n",
      "  iterations_since_restore: 39\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.44285714285714\n",
      "    ram_util_percent: 98.27142857142856\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 380.0\n",
      "    agent-1: 248.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 272.16\n",
      "    agent-1: 147.91\n",
      "  policy_reward_min:\n",
      "    agent-0: 213.0\n",
      "    agent-1: 80.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11494492155881003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10273664482624449\n",
      "    mean_inference_ms: 1.8354547459373873\n",
      "    mean_raw_obs_processing_ms: 0.16514317285393937\n",
      "  time_since_restore: 579.4249379634857\n",
      "  time_this_iter_s: 14.293825626373291\n",
      "  time_total_s: 579.4249379634857\n",
      "  timers:\n",
      "    learn_throughput: 931.35\n",
      "    learn_time_ms: 8587.538\n",
      "    load_throughput: 438006.52\n",
      "    load_time_ms: 18.26\n",
      "    sample_throughput: 1409.255\n",
      "    sample_time_ms: 5675.34\n",
      "    update_time_ms: 3.097\n",
      "  timestamp: 1627273705\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 311922\n",
      "  training_iteration: 39\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    39</td><td style=\"text-align: right;\">         579.425</td><td style=\"text-align: right;\">311922</td><td style=\"text-align: right;\">  420.07</td><td style=\"text-align: right;\">                 497</td><td style=\"text-align: right;\">                 366</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 639840\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-28-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 477.0\n",
      "  episode_reward_mean: 417.73\n",
      "  episode_reward_min: 375.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 3198\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.341340035200119\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01011302787810564\n",
      "          model: {}\n",
      "          policy_loss: -0.019485045224428177\n",
      "          total_loss: 188.4803924560547\n",
      "          vf_explained_var: 0.9506176114082336\n",
      "          vf_loss: 188.48963928222656\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3932532072067261\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008812504820525646\n",
      "          model: {}\n",
      "          policy_loss: -0.012805239297449589\n",
      "          total_loss: 1346.1319580078125\n",
      "          vf_explained_var: -1.1536382515942023e-08\n",
      "          vf_loss: 1346.1361083984375\n",
      "    num_agent_steps_sampled: 639840\n",
      "    num_agent_steps_trained: 639840\n",
      "    num_steps_sampled: 319920\n",
      "    num_steps_trained: 319920\n",
      "  iterations_since_restore: 40\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.81904761904762\n",
      "    ram_util_percent: 98.37619047619049\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 357.0\n",
      "    agent-1: 221.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 267.69\n",
      "    agent-1: 150.04\n",
      "  policy_reward_min:\n",
      "    agent-0: 222.0\n",
      "    agent-1: 107.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11484678453714114\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10269337040165283\n",
      "    mean_inference_ms: 1.833969476656763\n",
      "    mean_raw_obs_processing_ms: 0.1649788316576626\n",
      "  time_since_restore: 594.092767238617\n",
      "  time_this_iter_s: 14.667829275131226\n",
      "  time_total_s: 594.092767238617\n",
      "  timers:\n",
      "    learn_throughput: 932.178\n",
      "    learn_time_ms: 8579.904\n",
      "    load_throughput: 437971.637\n",
      "    load_time_ms: 18.261\n",
      "    sample_throughput: 1402.783\n",
      "    sample_time_ms: 5701.525\n",
      "    update_time_ms: 2.812\n",
      "  timestamp: 1627273719\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 319920\n",
      "  training_iteration: 40\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    40</td><td style=\"text-align: right;\">         594.093</td><td style=\"text-align: right;\">319920</td><td style=\"text-align: right;\">  417.73</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">                 375</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 655836\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-28-55\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 487.0\n",
      "  episode_reward_mean: 420.77\n",
      "  episode_reward_min: 366.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 3279\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3346293866634369\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008375790901482105\n",
      "          model: {}\n",
      "          policy_loss: -0.01599036529660225\n",
      "          total_loss: 315.8465881347656\n",
      "          vf_explained_var: 0.9263412952423096\n",
      "          vf_loss: 315.8541259765625\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.40252336859703064\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00794644933193922\n",
      "          model: {}\n",
      "          policy_loss: -0.011439547874033451\n",
      "          total_loss: 1444.6614990234375\n",
      "          vf_explained_var: -7.690921677294682e-09\n",
      "          vf_loss: 1444.6649169921875\n",
      "    num_agent_steps_sampled: 655836\n",
      "    num_agent_steps_trained: 655836\n",
      "    num_steps_sampled: 327918\n",
      "    num_steps_trained: 327918\n",
      "  iterations_since_restore: 41\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.02272727272728\n",
      "    ram_util_percent: 98.52727272727275\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 363.0\n",
      "    agent-1: 221.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 271.16\n",
      "    agent-1: 149.61\n",
      "  policy_reward_min:\n",
      "    agent-0: 191.0\n",
      "    agent-1: 112.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11493837345544432\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10276508058130941\n",
      "    mean_inference_ms: 1.835117961247214\n",
      "    mean_raw_obs_processing_ms: 0.16507636219799365\n",
      "  time_since_restore: 609.6790177822113\n",
      "  time_this_iter_s: 15.58625054359436\n",
      "  time_total_s: 609.6790177822113\n",
      "  timers:\n",
      "    learn_throughput: 919.782\n",
      "    learn_time_ms: 8695.541\n",
      "    load_throughput: 423479.88\n",
      "    load_time_ms: 18.886\n",
      "    sample_throughput: 1389.811\n",
      "    sample_time_ms: 5754.741\n",
      "    update_time_ms: 3.744\n",
      "  timestamp: 1627273735\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 327918\n",
      "  training_iteration: 41\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    41</td><td style=\"text-align: right;\">         609.679</td><td style=\"text-align: right;\">327918</td><td style=\"text-align: right;\">  420.77</td><td style=\"text-align: right;\">                 487</td><td style=\"text-align: right;\">                 366</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 671832\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-29-09\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 487.0\n",
      "  episode_reward_mean: 418.62\n",
      "  episode_reward_min: 355.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 3357\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.32570719718933105\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009847952052950859\n",
      "          model: {}\n",
      "          policy_loss: -0.016284430399537086\n",
      "          total_loss: 270.1335144042969\n",
      "          vf_explained_var: 0.9318884611129761\n",
      "          vf_loss: 270.13983154296875\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4049447774887085\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009714130312204361\n",
      "          model: {}\n",
      "          policy_loss: -0.011839180253446102\n",
      "          total_loss: 1377.7562255859375\n",
      "          vf_explained_var: 3.845460838647341e-09\n",
      "          vf_loss: 1377.7579345703125\n",
      "    num_agent_steps_sampled: 671832\n",
      "    num_agent_steps_trained: 671832\n",
      "    num_steps_sampled: 335916\n",
      "    num_steps_trained: 335916\n",
      "  iterations_since_restore: 42\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.523809523809526\n",
      "    ram_util_percent: 98.51428571428572\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 346.0\n",
      "    agent-1: 239.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 274.06\n",
      "    agent-1: 144.56\n",
      "  policy_reward_min:\n",
      "    agent-0: 204.0\n",
      "    agent-1: 91.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1148285645480572\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10261983055816654\n",
      "    mean_inference_ms: 1.8316925639391406\n",
      "    mean_raw_obs_processing_ms: 0.1649790348582644\n",
      "  time_since_restore: 624.1318998336792\n",
      "  time_this_iter_s: 14.452882051467896\n",
      "  time_total_s: 624.1318998336792\n",
      "  timers:\n",
      "    learn_throughput: 916.154\n",
      "    learn_time_ms: 8729.971\n",
      "    load_throughput: 421542.328\n",
      "    load_time_ms: 18.973\n",
      "    sample_throughput: 1392.26\n",
      "    sample_time_ms: 5744.616\n",
      "    update_time_ms: 4.728\n",
      "  timestamp: 1627273749\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 335916\n",
      "  training_iteration: 42\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    42</td><td style=\"text-align: right;\">         624.132</td><td style=\"text-align: right;\">335916</td><td style=\"text-align: right;\">  418.62</td><td style=\"text-align: right;\">                 487</td><td style=\"text-align: right;\">                 355</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 687828\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-29-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 470.0\n",
      "  episode_reward_mean: 421.04\n",
      "  episode_reward_min: 355.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 3438\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.2989307641983032\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010250059887766838\n",
      "          model: {}\n",
      "          policy_loss: -0.02106541581451893\n",
      "          total_loss: 253.9081268310547\n",
      "          vf_explained_var: 0.9499887824058533\n",
      "          vf_loss: 253.91879272460938\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.39548975229263306\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009077596478164196\n",
      "          model: {}\n",
      "          policy_loss: -0.012937183491885662\n",
      "          total_loss: 1332.719482421875\n",
      "          vf_explained_var: 9.613652096618353e-09\n",
      "          vf_loss: 1332.72314453125\n",
      "    num_agent_steps_sampled: 687828\n",
      "    num_agent_steps_trained: 687828\n",
      "    num_steps_sampled: 343914\n",
      "    num_steps_trained: 343914\n",
      "  iterations_since_restore: 43\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.78571428571428\n",
      "    ram_util_percent: 98.39047619047618\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 345.0\n",
      "    agent-1: 239.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 281.97\n",
      "    agent-1: 139.07\n",
      "  policy_reward_min:\n",
      "    agent-0: 204.0\n",
      "    agent-1: 90.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11468065266493908\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10253573467353767\n",
      "    mean_inference_ms: 1.8296766453433957\n",
      "    mean_raw_obs_processing_ms: 0.16477359640279257\n",
      "  time_since_restore: 638.8002734184265\n",
      "  time_this_iter_s: 14.668373584747314\n",
      "  time_total_s: 638.8002734184265\n",
      "  timers:\n",
      "    learn_throughput: 915.299\n",
      "    learn_time_ms: 8738.125\n",
      "    load_throughput: 394194.185\n",
      "    load_time_ms: 20.289\n",
      "    sample_throughput: 1391.517\n",
      "    sample_time_ms: 5747.682\n",
      "    update_time_ms: 5.705\n",
      "  timestamp: 1627273764\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 343914\n",
      "  training_iteration: 43\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    43</td><td style=\"text-align: right;\">           638.8</td><td style=\"text-align: right;\">343914</td><td style=\"text-align: right;\">  421.04</td><td style=\"text-align: right;\">                 470</td><td style=\"text-align: right;\">                 355</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 703824\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-29-40\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 472.0\n",
      "  episode_reward_mean: 416.99\n",
      "  episode_reward_min: 351.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 3519\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.29989901185035706\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008046877570450306\n",
      "          model: {}\n",
      "          policy_loss: -0.0159334484487772\n",
      "          total_loss: 266.8089904785156\n",
      "          vf_explained_var: 0.9465422630310059\n",
      "          vf_loss: 266.8167724609375\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.4087730646133423\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009482516907155514\n",
      "          model: {}\n",
      "          policy_loss: -0.011610058136284351\n",
      "          total_loss: 1291.95458984375\n",
      "          vf_explained_var: -1.1536382515942023e-08\n",
      "          vf_loss: 1291.9566650390625\n",
      "    num_agent_steps_sampled: 703824\n",
      "    num_agent_steps_trained: 703824\n",
      "    num_steps_sampled: 351912\n",
      "    num_steps_trained: 351912\n",
      "  iterations_since_restore: 44\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.59545454545455\n",
      "    ram_util_percent: 98.54090909090907\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 343.0\n",
      "    agent-1: 215.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 274.97\n",
      "    agent-1: 142.02\n",
      "  policy_reward_min:\n",
      "    agent-0: 203.0\n",
      "    agent-1: 105.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11474843103325987\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1025835142818778\n",
      "    mean_inference_ms: 1.8304389035731434\n",
      "    mean_raw_obs_processing_ms: 0.16481504373750025\n",
      "  time_since_restore: 654.2650034427643\n",
      "  time_this_iter_s: 15.464730024337769\n",
      "  time_total_s: 654.2650034427643\n",
      "  timers:\n",
      "    learn_throughput: 905.817\n",
      "    learn_time_ms: 8829.601\n",
      "    load_throughput: 385918.507\n",
      "    load_time_ms: 20.725\n",
      "    sample_throughput: 1376.345\n",
      "    sample_time_ms: 5811.041\n",
      "    update_time_ms: 6.855\n",
      "  timestamp: 1627273780\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 351912\n",
      "  training_iteration: 44\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    44</td><td style=\"text-align: right;\">         654.265</td><td style=\"text-align: right;\">351912</td><td style=\"text-align: right;\">  416.99</td><td style=\"text-align: right;\">                 472</td><td style=\"text-align: right;\">                 351</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 719820\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-29-54\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 472.0\n",
      "  episode_reward_mean: 414.5\n",
      "  episode_reward_min: 368.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 3597\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3137498199939728\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009873571805655956\n",
      "          model: {}\n",
      "          policy_loss: -0.018065692856907845\n",
      "          total_loss: 336.83697509765625\n",
      "          vf_explained_var: 0.9381076097488403\n",
      "          vf_loss: 336.84503173828125\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.39795440435409546\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00826975516974926\n",
      "          model: {}\n",
      "          policy_loss: -0.013833940029144287\n",
      "          total_loss: 1388.8460693359375\n",
      "          vf_explained_var: -4.806826048309176e-09\n",
      "          vf_loss: 1388.8515625\n",
      "    num_agent_steps_sampled: 719820\n",
      "    num_agent_steps_trained: 719820\n",
      "    num_steps_sampled: 359910\n",
      "    num_steps_trained: 359910\n",
      "  iterations_since_restore: 45\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.185\n",
      "    ram_util_percent: 98.66000000000001\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 335.0\n",
      "    agent-1: 246.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 267.45\n",
      "    agent-1: 147.05\n",
      "  policy_reward_min:\n",
      "    agent-0: 197.0\n",
      "    agent-1: 78.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11466648910167943\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1024604528938013\n",
      "    mean_inference_ms: 1.8274645097343905\n",
      "    mean_raw_obs_processing_ms: 0.16474823136769395\n",
      "  time_since_restore: 668.3892889022827\n",
      "  time_this_iter_s: 14.124285459518433\n",
      "  time_total_s: 668.3892889022827\n",
      "  timers:\n",
      "    learn_throughput: 907.028\n",
      "    learn_time_ms: 8817.808\n",
      "    load_throughput: 390953.099\n",
      "    load_time_ms: 20.458\n",
      "    sample_throughput: 1389.973\n",
      "    sample_time_ms: 5754.067\n",
      "    update_time_ms: 7.636\n",
      "  timestamp: 1627273794\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 359910\n",
      "  training_iteration: 45\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    45</td><td style=\"text-align: right;\">         668.389</td><td style=\"text-align: right;\">359910</td><td style=\"text-align: right;\">   414.5</td><td style=\"text-align: right;\">                 472</td><td style=\"text-align: right;\">                 368</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 735816\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-30-10\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 458.0\n",
      "  episode_reward_mean: 408.04\n",
      "  episode_reward_min: 346.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 3678\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.2990724742412567\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007989493198692799\n",
      "          model: {}\n",
      "          policy_loss: -0.015127877704799175\n",
      "          total_loss: 452.8586730957031\n",
      "          vf_explained_var: 0.9096102118492126\n",
      "          vf_loss: 452.86578369140625\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.406154066324234\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008561591617763042\n",
      "          model: {}\n",
      "          policy_loss: -0.015000482089817524\n",
      "          total_loss: 1185.3541259765625\n",
      "          vf_explained_var: -3.845460838647341e-09\n",
      "          vf_loss: 1185.3604736328125\n",
      "    num_agent_steps_sampled: 735816\n",
      "    num_agent_steps_trained: 735816\n",
      "    num_steps_sampled: 367908\n",
      "    num_steps_trained: 367908\n",
      "  iterations_since_restore: 46\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.64545454545455\n",
      "    ram_util_percent: 98.53181818181817\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 337.0\n",
      "    agent-1: 209.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 268.77\n",
      "    agent-1: 139.27\n",
      "  policy_reward_min:\n",
      "    agent-0: 195.0\n",
      "    agent-1: 88.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11462111651931213\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10246357980694862\n",
      "    mean_inference_ms: 1.8276541046542893\n",
      "    mean_raw_obs_processing_ms: 0.16464138275465878\n",
      "  time_since_restore: 684.0778887271881\n",
      "  time_this_iter_s: 15.688599824905396\n",
      "  time_total_s: 684.0778887271881\n",
      "  timers:\n",
      "    learn_throughput: 899.156\n",
      "    learn_time_ms: 8895.012\n",
      "    load_throughput: 453124.067\n",
      "    load_time_ms: 17.651\n",
      "    sample_throughput: 1378.855\n",
      "    sample_time_ms: 5800.465\n",
      "    update_time_ms: 8.661\n",
      "  timestamp: 1627273810\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 367908\n",
      "  training_iteration: 46\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    46</td><td style=\"text-align: right;\">         684.078</td><td style=\"text-align: right;\">367908</td><td style=\"text-align: right;\">  408.04</td><td style=\"text-align: right;\">                 458</td><td style=\"text-align: right;\">                 346</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 751812\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-30-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 468.0\n",
      "  episode_reward_mean: 406.94\n",
      "  episode_reward_min: 339.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 3759\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.312396764755249\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010748588480055332\n",
      "          model: {}\n",
      "          policy_loss: -0.019549570977687836\n",
      "          total_loss: 323.150390625\n",
      "          vf_explained_var: 0.9373660683631897\n",
      "          vf_loss: 323.1590270996094\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.39887091517448425\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0089175496250391\n",
      "          model: {}\n",
      "          policy_loss: -0.013539605773985386\n",
      "          total_loss: 1313.2723388671875\n",
      "          vf_explained_var: 1.9227304193236705e-09\n",
      "          vf_loss: 1313.2767333984375\n",
      "    num_agent_steps_sampled: 751812\n",
      "    num_agent_steps_trained: 751812\n",
      "    num_steps_sampled: 375906\n",
      "    num_steps_trained: 375906\n",
      "  iterations_since_restore: 47\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.23636363636364\n",
      "    ram_util_percent: 98.48181818181818\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 333.0\n",
      "    agent-1: 216.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 261.32\n",
      "    agent-1: 145.62\n",
      "  policy_reward_min:\n",
      "    agent-0: 177.0\n",
      "    agent-1: 97.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1146216651200486\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1024617980802585\n",
      "    mean_inference_ms: 1.8272817404925312\n",
      "    mean_raw_obs_processing_ms: 0.16463789269123288\n",
      "  time_since_restore: 699.452169418335\n",
      "  time_this_iter_s: 15.37428069114685\n",
      "  time_total_s: 699.452169418335\n",
      "  timers:\n",
      "    learn_throughput: 892.192\n",
      "    learn_time_ms: 8964.441\n",
      "    load_throughput: 427467.189\n",
      "    load_time_ms: 18.71\n",
      "    sample_throughput: 1377.289\n",
      "    sample_time_ms: 5807.059\n",
      "    update_time_ms: 9.856\n",
      "  timestamp: 1627273825\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 375906\n",
      "  training_iteration: 47\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    47</td><td style=\"text-align: right;\">         699.452</td><td style=\"text-align: right;\">375906</td><td style=\"text-align: right;\">  406.94</td><td style=\"text-align: right;\">                 468</td><td style=\"text-align: right;\">                 339</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 767808\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-30-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 455.0\n",
      "  episode_reward_mean: 402.99\n",
      "  episode_reward_min: 350.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 3837\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3011467754840851\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009151036851108074\n",
      "          model: {}\n",
      "          policy_loss: -0.016776341944932938\n",
      "          total_loss: 279.3138732910156\n",
      "          vf_explained_var: 0.9430414438247681\n",
      "          vf_loss: 279.3213806152344\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3889623284339905\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0071129752323031425\n",
      "          model: {}\n",
      "          policy_loss: -0.01092197559773922\n",
      "          total_loss: 1193.192138671875\n",
      "          vf_explained_var: 4.806826048309176e-09\n",
      "          vf_loss: 1193.196044921875\n",
      "    num_agent_steps_sampled: 767808\n",
      "    num_agent_steps_trained: 767808\n",
      "    num_steps_sampled: 383904\n",
      "    num_steps_trained: 383904\n",
      "  iterations_since_restore: 48\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.30434782608697\n",
      "    ram_util_percent: 98.69130434782608\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 330.0\n",
      "    agent-1: 203.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 259.27\n",
      "    agent-1: 143.72\n",
      "  policy_reward_min:\n",
      "    agent-0: 177.0\n",
      "    agent-1: 101.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11470409748498396\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10252618723972393\n",
      "    mean_inference_ms: 1.8289423113344234\n",
      "    mean_raw_obs_processing_ms: 0.16482377345101484\n",
      "  time_since_restore: 715.5086019039154\n",
      "  time_this_iter_s: 16.056432485580444\n",
      "  time_total_s: 715.5086019039154\n",
      "  timers:\n",
      "    learn_throughput: 878.539\n",
      "    learn_time_ms: 9103.749\n",
      "    load_throughput: 421175.027\n",
      "    load_time_ms: 18.99\n",
      "    sample_throughput: 1357.997\n",
      "    sample_time_ms: 5889.555\n",
      "    update_time_ms: 10.811\n",
      "  timestamp: 1627273841\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 383904\n",
      "  training_iteration: 48\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    48</td><td style=\"text-align: right;\">         715.509</td><td style=\"text-align: right;\">383904</td><td style=\"text-align: right;\">  402.99</td><td style=\"text-align: right;\">                 455</td><td style=\"text-align: right;\">                 350</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 783804\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-30-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 455.0\n",
      "  episode_reward_mean: 400.07\n",
      "  episode_reward_min: 339.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 3918\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.299337238073349\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008621602319180965\n",
      "          model: {}\n",
      "          policy_loss: -0.016346028074622154\n",
      "          total_loss: 463.72894287109375\n",
      "          vf_explained_var: 0.8967483639717102\n",
      "          vf_loss: 463.7366027832031\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3878239691257477\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007730793207883835\n",
      "          model: {}\n",
      "          policy_loss: -0.010305074043571949\n",
      "          total_loss: 1233.5185546875\n",
      "          vf_explained_var: -3.845460838647341e-09\n",
      "          vf_loss: 1233.521240234375\n",
      "    num_agent_steps_sampled: 783804\n",
      "    num_agent_steps_trained: 783804\n",
      "    num_steps_sampled: 391902\n",
      "    num_steps_trained: 391902\n",
      "  iterations_since_restore: 49\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.100000000000016\n",
      "    ram_util_percent: 98.64999999999999\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 330.0\n",
      "    agent-1: 208.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 255.61\n",
      "    agent-1: 144.46\n",
      "  policy_reward_min:\n",
      "    agent-0: 185.0\n",
      "    agent-1: 88.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11473075118816027\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10259978675625561\n",
      "    mean_inference_ms: 1.8302787016738113\n",
      "    mean_raw_obs_processing_ms: 0.16482822534669203\n",
      "  time_since_restore: 730.6107437610626\n",
      "  time_this_iter_s: 15.102141857147217\n",
      "  time_total_s: 730.6107437610626\n",
      "  timers:\n",
      "    learn_throughput: 876.563\n",
      "    learn_time_ms: 9124.275\n",
      "    load_throughput: 424906.154\n",
      "    load_time_ms: 18.823\n",
      "    sample_throughput: 1344.38\n",
      "    sample_time_ms: 5949.21\n",
      "    update_time_ms: 11.102\n",
      "  timestamp: 1627273856\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 391902\n",
      "  training_iteration: 49\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    49</td><td style=\"text-align: right;\">         730.611</td><td style=\"text-align: right;\">391902</td><td style=\"text-align: right;\">  400.07</td><td style=\"text-align: right;\">                 455</td><td style=\"text-align: right;\">                 339</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 799800\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-31-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 462.0\n",
      "  episode_reward_mean: 402.46\n",
      "  episode_reward_min: 345.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 3999\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.32766851782798767\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01038407813757658\n",
      "          model: {}\n",
      "          policy_loss: -0.02032681368291378\n",
      "          total_loss: 254.59320068359375\n",
      "          vf_explained_var: 0.9377588033676147\n",
      "          vf_loss: 254.60299682617188\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.38677361607551575\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007398444227874279\n",
      "          model: {}\n",
      "          policy_loss: -0.011143951676785946\n",
      "          total_loss: 1493.8046875\n",
      "          vf_explained_var: -2.8840956289855058e-09\n",
      "          vf_loss: 1493.8084716796875\n",
      "    num_agent_steps_sampled: 799800\n",
      "    num_agent_steps_trained: 799800\n",
      "    num_steps_sampled: 399900\n",
      "    num_steps_trained: 399900\n",
      "  iterations_since_restore: 50\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.56190476190477\n",
      "    ram_util_percent: 98.67142857142856\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 312.0\n",
      "    agent-1: 235.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 244.33\n",
      "    agent-1: 158.13\n",
      "  policy_reward_min:\n",
      "    agent-0: 171.0\n",
      "    agent-1: 99.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11466669451928473\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10256762412041383\n",
      "    mean_inference_ms: 1.8291896631641618\n",
      "    mean_raw_obs_processing_ms: 0.16476361243708137\n",
      "  time_since_restore: 745.5962555408478\n",
      "  time_this_iter_s: 14.985511779785156\n",
      "  time_total_s: 745.5962555408478\n",
      "  timers:\n",
      "    learn_throughput: 872.612\n",
      "    learn_time_ms: 9165.583\n",
      "    load_throughput: 409954.996\n",
      "    load_time_ms: 19.509\n",
      "    sample_throughput: 1346.746\n",
      "    sample_time_ms: 5938.758\n",
      "    update_time_ms: 11.463\n",
      "  timestamp: 1627273871\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 399900\n",
      "  training_iteration: 50\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    50</td><td style=\"text-align: right;\">         745.596</td><td style=\"text-align: right;\">399900</td><td style=\"text-align: right;\">  402.46</td><td style=\"text-align: right;\">                 462</td><td style=\"text-align: right;\">                 345</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 815796\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-31-25\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 462.0\n",
      "  episode_reward_mean: 399.77\n",
      "  episode_reward_min: 345.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 4077\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3049357235431671\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009027469903230667\n",
      "          model: {}\n",
      "          policy_loss: -0.01716648042201996\n",
      "          total_loss: 278.42376708984375\n",
      "          vf_explained_var: 0.9303034543991089\n",
      "          vf_loss: 278.4317932128906\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3829578757286072\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008595275692641735\n",
      "          model: {}\n",
      "          policy_loss: -0.011776640079915524\n",
      "          total_loss: 1424.795654296875\n",
      "          vf_explained_var: -3.845460838647341e-09\n",
      "          vf_loss: 1424.798583984375\n",
      "    num_agent_steps_sampled: 815796\n",
      "    num_agent_steps_trained: 815796\n",
      "    num_steps_sampled: 407898\n",
      "    num_steps_trained: 407898\n",
      "  iterations_since_restore: 51\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.98499999999999\n",
      "    ram_util_percent: 98.67\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 319.0\n",
      "    agent-1: 222.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 245.81\n",
      "    agent-1: 153.96\n",
      "  policy_reward_min:\n",
      "    agent-0: 172.0\n",
      "    agent-1: 109.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1145666506255575\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10243783182141378\n",
      "    mean_inference_ms: 1.8264620715712652\n",
      "    mean_raw_obs_processing_ms: 0.1646875468150612\n",
      "  time_since_restore: 759.6471858024597\n",
      "  time_this_iter_s: 14.050930261611938\n",
      "  time_total_s: 759.6471858024597\n",
      "  timers:\n",
      "    learn_throughput: 880.898\n",
      "    learn_time_ms: 9079.371\n",
      "    load_throughput: 419144.786\n",
      "    load_time_ms: 19.082\n",
      "    sample_throughput: 1361.851\n",
      "    sample_time_ms: 5872.887\n",
      "    update_time_ms: 10.897\n",
      "  timestamp: 1627273885\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 407898\n",
      "  training_iteration: 51\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    51</td><td style=\"text-align: right;\">         759.647</td><td style=\"text-align: right;\">407898</td><td style=\"text-align: right;\">  399.77</td><td style=\"text-align: right;\">                 462</td><td style=\"text-align: right;\">                 345</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 831792\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-31-41\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 446.0\n",
      "  episode_reward_mean: 400.55\n",
      "  episode_reward_min: 350.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 4158\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.327518492937088\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01096846628934145\n",
      "          model: {}\n",
      "          policy_loss: -0.022159064188599586\n",
      "          total_loss: 318.7226257324219\n",
      "          vf_explained_var: 0.9332990050315857\n",
      "          vf_loss: 318.733642578125\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.38038894534111023\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008381608873605728\n",
      "          model: {}\n",
      "          policy_loss: -0.009154959581792355\n",
      "          total_loss: 1527.3779296875\n",
      "          vf_explained_var: -9.613652096618353e-09\n",
      "          vf_loss: 1527.3785400390625\n",
      "    num_agent_steps_sampled: 831792\n",
      "    num_agent_steps_trained: 831792\n",
      "    num_steps_sampled: 415896\n",
      "    num_steps_trained: 415896\n",
      "  iterations_since_restore: 52\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.95454545454545\n",
      "    ram_util_percent: 98.7318181818182\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 320.0\n",
      "    agent-1: 241.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 241.35\n",
      "    agent-1: 159.2\n",
      "  policy_reward_min:\n",
      "    agent-0: 176.0\n",
      "    agent-1: 99.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11447093957772225\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10241171361066667\n",
      "    mean_inference_ms: 1.825730167110772\n",
      "    mean_raw_obs_processing_ms: 0.1644945293667471\n",
      "  time_since_restore: 774.9283316135406\n",
      "  time_this_iter_s: 15.281145811080933\n",
      "  time_total_s: 774.9283316135406\n",
      "  timers:\n",
      "    learn_throughput: 877.255\n",
      "    learn_time_ms: 9117.074\n",
      "    load_throughput: 393416.6\n",
      "    load_time_ms: 20.33\n",
      "    sample_throughput: 1351.681\n",
      "    sample_time_ms: 5917.075\n",
      "    update_time_ms: 10.794\n",
      "  timestamp: 1627273901\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 415896\n",
      "  training_iteration: 52\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    52</td><td style=\"text-align: right;\">         774.928</td><td style=\"text-align: right;\">415896</td><td style=\"text-align: right;\">  400.55</td><td style=\"text-align: right;\">                 446</td><td style=\"text-align: right;\">                 350</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 847788\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-31-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 447.0\n",
      "  episode_reward_mean: 402.33\n",
      "  episode_reward_min: 329.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 4236\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.29000550508499146\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009304358623921871\n",
      "          model: {}\n",
      "          policy_loss: -0.018292805179953575\n",
      "          total_loss: 374.8450927734375\n",
      "          vf_explained_var: 0.9332983493804932\n",
      "          vf_loss: 374.8539733886719\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3940551280975342\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007099108770489693\n",
      "          model: {}\n",
      "          policy_loss: -0.011474455706775188\n",
      "          total_loss: 1355.9466552734375\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 1355.9508056640625\n",
      "    num_agent_steps_sampled: 847788\n",
      "    num_agent_steps_trained: 847788\n",
      "    num_steps_sampled: 423894\n",
      "    num_steps_trained: 423894\n",
      "  iterations_since_restore: 53\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.71363636363637\n",
      "    ram_util_percent: 98.81818181818181\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 322.0\n",
      "    agent-1: 228.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 248.69\n",
      "    agent-1: 153.64\n",
      "  policy_reward_min:\n",
      "    agent-0: 176.0\n",
      "    agent-1: 98.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1145291018457504\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10242861163047833\n",
      "    mean_inference_ms: 1.82589510901226\n",
      "    mean_raw_obs_processing_ms: 0.16463931311589972\n",
      "  time_since_restore: 789.9340491294861\n",
      "  time_this_iter_s: 15.005717515945435\n",
      "  time_total_s: 789.9340491294861\n",
      "  timers:\n",
      "    learn_throughput: 877.179\n",
      "    learn_time_ms: 9117.862\n",
      "    load_throughput: 420222.718\n",
      "    load_time_ms: 19.033\n",
      "    sample_throughput: 1343.778\n",
      "    sample_time_ms: 5951.874\n",
      "    update_time_ms: 10.615\n",
      "  timestamp: 1627273916\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 423894\n",
      "  training_iteration: 53\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    53</td><td style=\"text-align: right;\">         789.934</td><td style=\"text-align: right;\">423894</td><td style=\"text-align: right;\">  402.33</td><td style=\"text-align: right;\">                 447</td><td style=\"text-align: right;\">                 329</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 863784\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-32-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 444.0\n",
      "  episode_reward_mean: 406.59\n",
      "  episode_reward_min: 355.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 4317\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.2732475996017456\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0092241782695055\n",
      "          model: {}\n",
      "          policy_loss: -0.016067108139395714\n",
      "          total_loss: 241.9945068359375\n",
      "          vf_explained_var: 0.9417319297790527\n",
      "          vf_loss: 242.00120544433594\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3954707682132721\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007231445051729679\n",
      "          model: {}\n",
      "          policy_loss: -0.010554694570600986\n",
      "          total_loss: 1439.0306396484375\n",
      "          vf_explained_var: 2.8840956289855058e-09\n",
      "          vf_loss: 1439.0338134765625\n",
      "    num_agent_steps_sampled: 863784\n",
      "    num_agent_steps_trained: 863784\n",
      "    num_steps_sampled: 431892\n",
      "    num_steps_trained: 431892\n",
      "  iterations_since_restore: 54\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.945454545454545\n",
      "    ram_util_percent: 98.8318181818182\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 321.0\n",
      "    agent-1: 219.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 255.12\n",
      "    agent-1: 151.47\n",
      "  policy_reward_min:\n",
      "    agent-0: 176.0\n",
      "    agent-1: 97.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11450839597693559\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10245666695572778\n",
      "    mean_inference_ms: 1.8262518023991716\n",
      "    mean_raw_obs_processing_ms: 0.1645795050431004\n",
      "  time_since_restore: 805.8366162776947\n",
      "  time_this_iter_s: 15.902567148208618\n",
      "  time_total_s: 805.8366162776947\n",
      "  timers:\n",
      "    learn_throughput: 871.0\n",
      "    learn_time_ms: 9182.546\n",
      "    load_throughput: 426810.193\n",
      "    load_time_ms: 18.739\n",
      "    sample_throughput: 1348.322\n",
      "    sample_time_ms: 5931.819\n",
      "    update_time_ms: 10.303\n",
      "  timestamp: 1627273932\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 431892\n",
      "  training_iteration: 54\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    54</td><td style=\"text-align: right;\">         805.837</td><td style=\"text-align: right;\">431892</td><td style=\"text-align: right;\">  406.59</td><td style=\"text-align: right;\">                 444</td><td style=\"text-align: right;\">                 355</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 879780\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-32-27\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 456.0\n",
      "  episode_reward_mean: 403.96\n",
      "  episode_reward_min: 327.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 4398\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.2661144733428955\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010548556223511696\n",
      "          model: {}\n",
      "          policy_loss: -0.01840236410498619\n",
      "          total_loss: 281.97808837890625\n",
      "          vf_explained_var: 0.932467520236969\n",
      "          vf_loss: 281.9858703613281\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3869972825050354\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006625581067055464\n",
      "          model: {}\n",
      "          policy_loss: -0.00926696415990591\n",
      "          total_loss: 1317.5291748046875\n",
      "          vf_explained_var: -5.7681912579710115e-09\n",
      "          vf_loss: 1317.53173828125\n",
      "    num_agent_steps_sampled: 879780\n",
      "    num_agent_steps_trained: 879780\n",
      "    num_steps_sampled: 439890\n",
      "    num_steps_trained: 439890\n",
      "  iterations_since_restore: 55\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.50434782608696\n",
      "    ram_util_percent: 98.65652173913045\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 325.0\n",
      "    agent-1: 219.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 254.48\n",
      "    agent-1: 149.48\n",
      "  policy_reward_min:\n",
      "    agent-0: 178.0\n",
      "    agent-1: 101.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11458181612795389\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10253169390419942\n",
      "    mean_inference_ms: 1.827444284553921\n",
      "    mean_raw_obs_processing_ms: 0.1646826023474138\n",
      "  time_since_restore: 821.3733882904053\n",
      "  time_this_iter_s: 15.536772012710571\n",
      "  time_total_s: 821.3733882904053\n",
      "  timers:\n",
      "    learn_throughput: 865.009\n",
      "    learn_time_ms: 9246.151\n",
      "    load_throughput: 398246.362\n",
      "    load_time_ms: 20.083\n",
      "    sample_throughput: 1331.29\n",
      "    sample_time_ms: 6007.708\n",
      "    update_time_ms: 10.434\n",
      "  timestamp: 1627273947\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 439890\n",
      "  training_iteration: 55\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    55</td><td style=\"text-align: right;\">         821.373</td><td style=\"text-align: right;\">439890</td><td style=\"text-align: right;\">  403.96</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">                 327</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 895776\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-32-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 456.0\n",
      "  episode_reward_mean: 401.35\n",
      "  episode_reward_min: 327.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 4476\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.24233928322792053\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009087618440389633\n",
      "          model: {}\n",
      "          policy_loss: -0.016299139708280563\n",
      "          total_loss: 284.0053405761719\n",
      "          vf_explained_var: 0.9306807518005371\n",
      "          vf_loss: 284.0124206542969\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3919508457183838\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008087019436061382\n",
      "          model: {}\n",
      "          policy_loss: -0.007970680482685566\n",
      "          total_loss: 1451.4219970703125\n",
      "          vf_explained_var: -9.613652096618353e-10\n",
      "          vf_loss: 1451.4219970703125\n",
      "    num_agent_steps_sampled: 895776\n",
      "    num_agent_steps_trained: 895776\n",
      "    num_steps_sampled: 447888\n",
      "    num_steps_trained: 447888\n",
      "  iterations_since_restore: 56\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.30952380952381\n",
      "    ram_util_percent: 98.76190476190476\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 327.0\n",
      "    agent-1: 234.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 256.35\n",
      "    agent-1: 145.0\n",
      "  policy_reward_min:\n",
      "    agent-0: 149.0\n",
      "    agent-1: 93.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11458308631791368\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10249657887606645\n",
      "    mean_inference_ms: 1.8266359900024178\n",
      "    mean_raw_obs_processing_ms: 0.16471274582871537\n",
      "  time_since_restore: 836.0133624076843\n",
      "  time_this_iter_s: 14.639974117279053\n",
      "  time_total_s: 836.0133624076843\n",
      "  timers:\n",
      "    learn_throughput: 872.002\n",
      "    learn_time_ms: 9171.998\n",
      "    load_throughput: 389799.679\n",
      "    load_time_ms: 20.518\n",
      "    sample_throughput: 1338.131\n",
      "    sample_time_ms: 5976.993\n",
      "    update_time_ms: 10.205\n",
      "  timestamp: 1627273962\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 447888\n",
      "  training_iteration: 56\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    56</td><td style=\"text-align: right;\">         836.013</td><td style=\"text-align: right;\">447888</td><td style=\"text-align: right;\">  401.35</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">                 327</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 911772\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-32-57\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 462.0\n",
      "  episode_reward_mean: 400.64\n",
      "  episode_reward_min: 331.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 4557\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.2442186325788498\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010107491165399551\n",
      "          model: {}\n",
      "          policy_loss: -0.019572613760828972\n",
      "          total_loss: 353.8544921875\n",
      "          vf_explained_var: 0.9216757416725159\n",
      "          vf_loss: 353.8638916015625\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.38319534063339233\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006299461238086224\n",
      "          model: {}\n",
      "          policy_loss: -0.00580514594912529\n",
      "          total_loss: 1342.385498046875\n",
      "          vf_explained_var: -3.845460838647341e-09\n",
      "          vf_loss: 1342.3846435546875\n",
      "    num_agent_steps_sampled: 911772\n",
      "    num_agent_steps_trained: 911772\n",
      "    num_steps_sampled: 455886\n",
      "    num_steps_trained: 455886\n",
      "  iterations_since_restore: 57\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.49047619047619\n",
      "    ram_util_percent: 98.78095238095239\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 325.0\n",
      "    agent-1: 245.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 253.42\n",
      "    agent-1: 147.22\n",
      "  policy_reward_min:\n",
      "    agent-0: 160.0\n",
      "    agent-1: 93.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11446809164630249\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1024469537714442\n",
      "    mean_inference_ms: 1.8256254856128484\n",
      "    mean_raw_obs_processing_ms: 0.1645369602062179\n",
      "  time_since_restore: 850.7461595535278\n",
      "  time_this_iter_s: 14.732797145843506\n",
      "  time_total_s: 850.7461595535278\n",
      "  timers:\n",
      "    learn_throughput: 876.037\n",
      "    learn_time_ms: 9129.748\n",
      "    load_throughput: 410142.453\n",
      "    load_time_ms: 19.501\n",
      "    sample_throughput: 1342.607\n",
      "    sample_time_ms: 5957.069\n",
      "    update_time_ms: 9.693\n",
      "  timestamp: 1627273977\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 455886\n",
      "  training_iteration: 57\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    57</td><td style=\"text-align: right;\">         850.746</td><td style=\"text-align: right;\">455886</td><td style=\"text-align: right;\">  400.64</td><td style=\"text-align: right;\">                 462</td><td style=\"text-align: right;\">                 331</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 927768\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-33-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 455.0\n",
      "  episode_reward_mean: 399.87\n",
      "  episode_reward_min: 349.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 4638\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.2337770015001297\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010617941617965698\n",
      "          model: {}\n",
      "          policy_loss: -0.014266278594732285\n",
      "          total_loss: 242.62081909179688\n",
      "          vf_explained_var: 0.9518111944198608\n",
      "          vf_loss: 242.6243438720703\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3906693756580353\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0075610363855957985\n",
      "          model: {}\n",
      "          policy_loss: -0.008972675539553165\n",
      "          total_loss: 1252.2784423828125\n",
      "          vf_explained_var: 6.729556467632847e-09\n",
      "          vf_loss: 1252.2801513671875\n",
      "    num_agent_steps_sampled: 927768\n",
      "    num_agent_steps_trained: 927768\n",
      "    num_steps_sampled: 463884\n",
      "    num_steps_trained: 463884\n",
      "  iterations_since_restore: 58\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 59.48181818181818\n",
      "    ram_util_percent: 98.64999999999999\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 335.0\n",
      "    agent-1: 220.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 256.31\n",
      "    agent-1: 143.56\n",
      "  policy_reward_min:\n",
      "    agent-0: 182.0\n",
      "    agent-1: 102.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11455549119189554\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1025016725517974\n",
      "    mean_inference_ms: 1.8268512685476264\n",
      "    mean_raw_obs_processing_ms: 0.16465157351273121\n",
      "  time_since_restore: 866.5917959213257\n",
      "  time_this_iter_s: 15.845636367797852\n",
      "  time_total_s: 866.5917959213257\n",
      "  timers:\n",
      "    learn_throughput: 877.246\n",
      "    learn_time_ms: 9117.172\n",
      "    load_throughput: 399115.814\n",
      "    load_time_ms: 20.039\n",
      "    sample_throughput: 1344.621\n",
      "    sample_time_ms: 5948.144\n",
      "    update_time_ms: 9.839\n",
      "  timestamp: 1627273993\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 463884\n",
      "  training_iteration: 58\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    58</td><td style=\"text-align: right;\">         866.592</td><td style=\"text-align: right;\">463884</td><td style=\"text-align: right;\">  399.87</td><td style=\"text-align: right;\">                 455</td><td style=\"text-align: right;\">                 349</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 943764\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-33-28\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 481.0\n",
      "  episode_reward_mean: 403.79\n",
      "  episode_reward_min: 335.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 4716\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.2568591833114624\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007509557064622641\n",
      "          model: {}\n",
      "          policy_loss: -0.014792942441999912\n",
      "          total_loss: 322.1475830078125\n",
      "          vf_explained_var: 0.9176406860351562\n",
      "          vf_loss: 322.1546936035156\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.38141345977783203\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007553455885499716\n",
      "          model: {}\n",
      "          policy_loss: -0.008779671974480152\n",
      "          total_loss: 1402.0155029296875\n",
      "          vf_explained_var: -9.613652096618353e-10\n",
      "          vf_loss: 1402.016357421875\n",
      "    num_agent_steps_sampled: 943764\n",
      "    num_agent_steps_trained: 943764\n",
      "    num_steps_sampled: 471882\n",
      "    num_steps_trained: 471882\n",
      "  iterations_since_restore: 59\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.1409090909091\n",
      "    ram_util_percent: 98.7818181818182\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 334.0\n",
      "    agent-1: 228.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 255.22\n",
      "    agent-1: 148.57\n",
      "  policy_reward_min:\n",
      "    agent-0: 173.0\n",
      "    agent-1: 103.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11461511271085292\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10252034025919254\n",
      "    mean_inference_ms: 1.8269728792648985\n",
      "    mean_raw_obs_processing_ms: 0.16482130822606159\n",
      "  time_since_restore: 882.0388078689575\n",
      "  time_this_iter_s: 15.447011947631836\n",
      "  time_total_s: 882.0388078689575\n",
      "  timers:\n",
      "    learn_throughput: 873.416\n",
      "    learn_time_ms: 9157.149\n",
      "    load_throughput: 395852.013\n",
      "    load_time_ms: 20.205\n",
      "    sample_throughput: 1346.28\n",
      "    sample_time_ms: 5940.813\n",
      "    update_time_ms: 10.603\n",
      "  timestamp: 1627274008\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 471882\n",
      "  training_iteration: 59\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    59</td><td style=\"text-align: right;\">         882.039</td><td style=\"text-align: right;\">471882</td><td style=\"text-align: right;\">  403.79</td><td style=\"text-align: right;\">                 481</td><td style=\"text-align: right;\">                 335</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 959760\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-33-48\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 476.0\n",
      "  episode_reward_mean: 404.67\n",
      "  episode_reward_min: 335.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 4797\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.25212061405181885\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009571892209351063\n",
      "          model: {}\n",
      "          policy_loss: -0.015501260757446289\n",
      "          total_loss: 284.307373046875\n",
      "          vf_explained_var: 0.9222297668457031\n",
      "          vf_loss: 284.3131408691406\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.37355515360832214\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006337779574096203\n",
      "          model: {}\n",
      "          policy_loss: -0.007743724621832371\n",
      "          total_loss: 1431.872314453125\n",
      "          vf_explained_var: 9.613652096618353e-09\n",
      "          vf_loss: 1431.8739013671875\n",
      "    num_agent_steps_sampled: 959760\n",
      "    num_agent_steps_trained: 959760\n",
      "    num_steps_sampled: 479880\n",
      "    num_steps_trained: 479880\n",
      "  iterations_since_restore: 60\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 71.63571428571429\n",
      "    ram_util_percent: 99.07142857142858\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 345.0\n",
      "    agent-1: 225.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 251.46\n",
      "    agent-1: 153.21\n",
      "  policy_reward_min:\n",
      "    agent-0: 185.0\n",
      "    agent-1: 95.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11475154670153792\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10272558179187556\n",
      "    mean_inference_ms: 1.8314539556056644\n",
      "    mean_raw_obs_processing_ms: 0.16498561153146518\n",
      "  time_since_restore: 901.5726430416107\n",
      "  time_this_iter_s: 19.5338351726532\n",
      "  time_total_s: 901.5726430416107\n",
      "  timers:\n",
      "    learn_throughput: 842.222\n",
      "    learn_time_ms: 9496.312\n",
      "    load_throughput: 403439.119\n",
      "    load_time_ms: 19.825\n",
      "    sample_throughput: 1320.764\n",
      "    sample_time_ms: 6055.584\n",
      "    update_time_ms: 11.372\n",
      "  timestamp: 1627274028\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 479880\n",
      "  training_iteration: 60\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 15.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    60</td><td style=\"text-align: right;\">         901.573</td><td style=\"text-align: right;\">479880</td><td style=\"text-align: right;\">  404.67</td><td style=\"text-align: right;\">                 476</td><td style=\"text-align: right;\">                 335</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 975756\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-34-12\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 477.0\n",
      "  episode_reward_mean: 411.35\n",
      "  episode_reward_min: 355.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 4878\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.24294498562812805\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008959508500993252\n",
      "          model: {}\n",
      "          policy_loss: -0.014439842663705349\n",
      "          total_loss: 319.3625793457031\n",
      "          vf_explained_var: 0.9256961941719055\n",
      "          vf_loss: 319.3678894042969\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.39172205328941345\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008506511338055134\n",
      "          model: {}\n",
      "          policy_loss: -0.011169546283781528\n",
      "          total_loss: 1490.968994140625\n",
      "          vf_explained_var: -1.9227304193236705e-09\n",
      "          vf_loss: 1490.9715576171875\n",
      "    num_agent_steps_sampled: 975756\n",
      "    num_agent_steps_trained: 975756\n",
      "    num_steps_sampled: 487878\n",
      "    num_steps_trained: 487878\n",
      "  iterations_since_restore: 61\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 80.84\n",
      "    ram_util_percent: 98.25714285714285\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 351.0\n",
      "    agent-1: 231.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 258.0\n",
      "    agent-1: 153.35\n",
      "  policy_reward_min:\n",
      "    agent-0: 181.0\n",
      "    agent-1: 107.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1157747046791257\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10371919064004498\n",
      "    mean_inference_ms: 1.8552148165592097\n",
      "    mean_raw_obs_processing_ms: 0.16661938188775127\n",
      "  time_since_restore: 925.5173518657684\n",
      "  time_this_iter_s: 23.944708824157715\n",
      "  time_total_s: 925.5173518657684\n",
      "  timers:\n",
      "    learn_throughput: 811.495\n",
      "    learn_time_ms: 9855.879\n",
      "    load_throughput: 291350.291\n",
      "    load_time_ms: 27.451\n",
      "    sample_throughput: 1197.831\n",
      "    sample_time_ms: 6677.071\n",
      "    update_time_ms: 11.915\n",
      "  timestamp: 1627274052\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 487878\n",
      "  training_iteration: 61\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.8/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    61</td><td style=\"text-align: right;\">         925.517</td><td style=\"text-align: right;\">487878</td><td style=\"text-align: right;\">  411.35</td><td style=\"text-align: right;\">                 477</td><td style=\"text-align: right;\">                 355</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 991752\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-34-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 473.0\n",
      "  episode_reward_mean: 412.39\n",
      "  episode_reward_min: 307.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 4956\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.254840224981308\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009856765158474445\n",
      "          model: {}\n",
      "          policy_loss: -0.012761233374476433\n",
      "          total_loss: 325.7589111328125\n",
      "          vf_explained_var: 0.9239456057548523\n",
      "          vf_loss: 325.7616882324219\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3834461569786072\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005974136758595705\n",
      "          model: {}\n",
      "          policy_loss: -0.007822338491678238\n",
      "          total_loss: 1605.7540283203125\n",
      "          vf_explained_var: -4.806826048309176e-09\n",
      "          vf_loss: 1605.7557373046875\n",
      "    num_agent_steps_sampled: 991752\n",
      "    num_agent_steps_trained: 991752\n",
      "    num_steps_sampled: 495876\n",
      "    num_steps_trained: 495876\n",
      "  iterations_since_restore: 62\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 63.87307692307692\n",
      "    ram_util_percent: 96.19615384615382\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 349.0\n",
      "    agent-1: 218.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 253.47\n",
      "    agent-1: 158.92\n",
      "  policy_reward_min:\n",
      "    agent-0: 186.0\n",
      "    agent-1: 107.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11624540149923834\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10407342192817279\n",
      "    mean_inference_ms: 1.8640854604510537\n",
      "    mean_raw_obs_processing_ms: 0.1672621377508596\n",
      "  time_since_restore: 943.7942452430725\n",
      "  time_this_iter_s: 18.276893377304077\n",
      "  time_total_s: 943.7942452430725\n",
      "  timers:\n",
      "    learn_throughput: 796.403\n",
      "    learn_time_ms: 10042.649\n",
      "    load_throughput: 292906.661\n",
      "    load_time_ms: 27.306\n",
      "    sample_throughput: 1177.712\n",
      "    sample_time_ms: 6791.132\n",
      "    update_time_ms: 11.066\n",
      "  timestamp: 1627274070\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 495876\n",
      "  training_iteration: 62\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    62</td><td style=\"text-align: right;\">         943.794</td><td style=\"text-align: right;\">495876</td><td style=\"text-align: right;\">  412.39</td><td style=\"text-align: right;\">                 473</td><td style=\"text-align: right;\">                 307</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1007748\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-34-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 481.0\n",
      "  episode_reward_mean: 412.94\n",
      "  episode_reward_min: 307.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 5037\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.256808340549469\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00848357006907463\n",
      "          model: {}\n",
      "          policy_loss: -0.013282516039907932\n",
      "          total_loss: 334.9039611816406\n",
      "          vf_explained_var: 0.9145994782447815\n",
      "          vf_loss: 334.90863037109375\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3774399161338806\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005682874005287886\n",
      "          model: {}\n",
      "          policy_loss: -0.006277299020439386\n",
      "          total_loss: 1544.002685546875\n",
      "          vf_explained_var: -1.9227304193236705e-09\n",
      "          vf_loss: 1544.0029296875\n",
      "    num_agent_steps_sampled: 1007748\n",
      "    num_agent_steps_trained: 1007748\n",
      "    num_steps_sampled: 503874\n",
      "    num_steps_trained: 503874\n",
      "  iterations_since_restore: 63\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.324999999999996\n",
      "    ram_util_percent: 94.49166666666667\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 348.0\n",
      "    agent-1: 234.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 248.92\n",
      "    agent-1: 164.02\n",
      "  policy_reward_min:\n",
      "    agent-0: 194.0\n",
      "    agent-1: 109.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11650272613565622\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10439883294250261\n",
      "    mean_inference_ms: 1.8706963104204783\n",
      "    mean_raw_obs_processing_ms: 0.16751392454252945\n",
      "  time_since_restore: 960.8760812282562\n",
      "  time_this_iter_s: 17.081835985183716\n",
      "  time_total_s: 960.8760812282562\n",
      "  timers:\n",
      "    learn_throughput: 789.897\n",
      "    learn_time_ms: 10125.375\n",
      "    load_throughput: 283762.101\n",
      "    load_time_ms: 28.186\n",
      "    sample_throughput: 1156.43\n",
      "    sample_time_ms: 6916.112\n",
      "    update_time_ms: 10.224\n",
      "  timestamp: 1627274087\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 503874\n",
      "  training_iteration: 63\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    63</td><td style=\"text-align: right;\">         960.876</td><td style=\"text-align: right;\">503874</td><td style=\"text-align: right;\">  412.94</td><td style=\"text-align: right;\">                 481</td><td style=\"text-align: right;\">                 307</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1023744\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-35-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 481.0\n",
      "  episode_reward_mean: 408.25\n",
      "  episode_reward_min: 358.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 5118\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.21913447976112366\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00814506784081459\n",
      "          model: {}\n",
      "          policy_loss: -0.016855478286743164\n",
      "          total_loss: 281.4005432128906\n",
      "          vf_explained_var: 0.9407631158828735\n",
      "          vf_loss: 281.40911865234375\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3942965269088745\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006225092336535454\n",
      "          model: {}\n",
      "          policy_loss: -0.008387021720409393\n",
      "          total_loss: 1357.5321044921875\n",
      "          vf_explained_var: 7.690921677294682e-09\n",
      "          vf_loss: 1357.5340576171875\n",
      "    num_agent_steps_sampled: 1023744\n",
      "    num_agent_steps_trained: 1023744\n",
      "    num_steps_sampled: 511872\n",
      "    num_steps_trained: 511872\n",
      "  iterations_since_restore: 64\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 60.36086956521739\n",
      "    ram_util_percent: 94.1304347826087\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 348.0\n",
      "    agent-1: 211.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 260.1\n",
      "    agent-1: 148.15\n",
      "  policy_reward_min:\n",
      "    agent-0: 191.0\n",
      "    agent-1: 95.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1167250838313136\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10466569900133753\n",
      "    mean_inference_ms: 1.8761035366242251\n",
      "    mean_raw_obs_processing_ms: 0.16777754232097541\n",
      "  time_since_restore: 977.2495896816254\n",
      "  time_this_iter_s: 16.37350845336914\n",
      "  time_total_s: 977.2495896816254\n",
      "  timers:\n",
      "    learn_throughput: 795.017\n",
      "    learn_time_ms: 10060.159\n",
      "    load_throughput: 291921.255\n",
      "    load_time_ms: 27.398\n",
      "    sample_throughput: 1137.624\n",
      "    sample_time_ms: 7030.44\n",
      "    update_time_ms: 9.453\n",
      "  timestamp: 1627274104\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 511872\n",
      "  training_iteration: 64\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    64</td><td style=\"text-align: right;\">          977.25</td><td style=\"text-align: right;\">511872</td><td style=\"text-align: right;\">  408.25</td><td style=\"text-align: right;\">                 481</td><td style=\"text-align: right;\">                 358</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1039740\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-35-19\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 451.0\n",
      "  episode_reward_mean: 407.35\n",
      "  episode_reward_min: 344.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 5196\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.22450049221515656\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007593050599098206\n",
      "          model: {}\n",
      "          policy_loss: -0.01342164445668459\n",
      "          total_loss: 322.3479919433594\n",
      "          vf_explained_var: 0.9346719980239868\n",
      "          vf_loss: 322.3537902832031\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3883882761001587\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006302353460341692\n",
      "          model: {}\n",
      "          policy_loss: -0.007466447073966265\n",
      "          total_loss: 1305.5107421875\n",
      "          vf_explained_var: 6.729556467632847e-09\n",
      "          vf_loss: 1305.5118408203125\n",
      "    num_agent_steps_sampled: 1039740\n",
      "    num_agent_steps_trained: 1039740\n",
      "    num_steps_sampled: 519870\n",
      "    num_steps_trained: 519870\n",
      "  iterations_since_restore: 65\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.17391304347825\n",
      "    ram_util_percent: 93.84347826086959\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 329.0\n",
      "    agent-1: 195.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 261.45\n",
      "    agent-1: 145.9\n",
      "  policy_reward_min:\n",
      "    agent-0: 202.0\n",
      "    agent-1: 108.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11682934507720084\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10473776386568968\n",
      "    mean_inference_ms: 1.8778532652159448\n",
      "    mean_raw_obs_processing_ms: 0.16795533172452376\n",
      "  time_since_restore: 993.0836417675018\n",
      "  time_this_iter_s: 15.834052085876465\n",
      "  time_total_s: 993.0836417675018\n",
      "  timers:\n",
      "    learn_throughput: 793.754\n",
      "    learn_time_ms: 10076.169\n",
      "    load_throughput: 297337.855\n",
      "    load_time_ms: 26.899\n",
      "    sample_throughput: 1135.08\n",
      "    sample_time_ms: 7046.198\n",
      "    update_time_ms: 8.527\n",
      "  timestamp: 1627274119\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 519870\n",
      "  training_iteration: 65\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.5/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    65</td><td style=\"text-align: right;\">         993.084</td><td style=\"text-align: right;\">519870</td><td style=\"text-align: right;\">  407.35</td><td style=\"text-align: right;\">                 451</td><td style=\"text-align: right;\">                 344</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1055736\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-35-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 456.0\n",
      "  episode_reward_mean: 410.78\n",
      "  episode_reward_min: 366.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 5277\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.23773528635501862\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008679226972162724\n",
      "          model: {}\n",
      "          policy_loss: -0.01379489153623581\n",
      "          total_loss: 239.9990234375\n",
      "          vf_explained_var: 0.940788984298706\n",
      "          vf_loss: 240.00401306152344\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3772125840187073\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005637780297547579\n",
      "          model: {}\n",
      "          policy_loss: -0.008027107454836369\n",
      "          total_loss: 1315.631103515625\n",
      "          vf_explained_var: 9.613652096618353e-10\n",
      "          vf_loss: 1315.63330078125\n",
      "    num_agent_steps_sampled: 1055736\n",
      "    num_agent_steps_trained: 1055736\n",
      "    num_steps_sampled: 527868\n",
      "    num_steps_trained: 527868\n",
      "  iterations_since_restore: 66\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 56.15909090909092\n",
      "    ram_util_percent: 93.91363636363639\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 329.0\n",
      "    agent-1: 229.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 265.49\n",
      "    agent-1: 145.29\n",
      "  policy_reward_min:\n",
      "    agent-0: 196.0\n",
      "    agent-1: 104.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11684755132977116\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10480439125220652\n",
      "    mean_inference_ms: 1.8792706483573958\n",
      "    mean_raw_obs_processing_ms: 0.1679235819347746\n",
      "  time_since_restore: 1008.3303682804108\n",
      "  time_this_iter_s: 15.246726512908936\n",
      "  time_total_s: 1008.3303682804108\n",
      "  timers:\n",
      "    learn_throughput: 793.363\n",
      "    learn_time_ms: 10081.142\n",
      "    load_throughput: 304783.654\n",
      "    load_time_ms: 26.242\n",
      "    sample_throughput: 1125.966\n",
      "    sample_time_ms: 7103.233\n",
      "    update_time_ms: 7.668\n",
      "  timestamp: 1627274135\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 527868\n",
      "  training_iteration: 66\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    66</td><td style=\"text-align: right;\">         1008.33</td><td style=\"text-align: right;\">527868</td><td style=\"text-align: right;\">  410.78</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">                 366</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1071732\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-35-52\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 459.0\n",
      "  episode_reward_mean: 409.1\n",
      "  episode_reward_min: 343.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 5358\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.22605714201927185\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009058385156095028\n",
      "          model: {}\n",
      "          policy_loss: -0.010979539714753628\n",
      "          total_loss: 272.61749267578125\n",
      "          vf_explained_var: 0.9374305605888367\n",
      "          vf_loss: 272.6192932128906\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.38988062739372253\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006500351242721081\n",
      "          model: {}\n",
      "          policy_loss: -0.005868753883987665\n",
      "          total_loss: 1274.75244140625\n",
      "          vf_explained_var: -2.8840956289855058e-09\n",
      "          vf_loss: 1274.75146484375\n",
      "    num_agent_steps_sampled: 1071732\n",
      "    num_agent_steps_trained: 1071732\n",
      "    num_steps_sampled: 535866\n",
      "    num_steps_trained: 535866\n",
      "  iterations_since_restore: 67\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 61.376000000000005\n",
      "    ram_util_percent: 94.116\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 347.0\n",
      "    agent-1: 210.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 263.65\n",
      "    agent-1: 145.45\n",
      "  policy_reward_min:\n",
      "    agent-0: 204.0\n",
      "    agent-1: 104.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11693154054683795\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10491452127046717\n",
      "    mean_inference_ms: 1.8813629489799646\n",
      "    mean_raw_obs_processing_ms: 0.16803297957448873\n",
      "  time_since_restore: 1025.5883448123932\n",
      "  time_this_iter_s: 17.257976531982422\n",
      "  time_total_s: 1025.5883448123932\n",
      "  timers:\n",
      "    learn_throughput: 780.923\n",
      "    learn_time_ms: 10241.729\n",
      "    load_throughput: 301913.606\n",
      "    load_time_ms: 26.491\n",
      "    sample_throughput: 1111.515\n",
      "    sample_time_ms: 7195.585\n",
      "    update_time_ms: 7.225\n",
      "  timestamp: 1627274152\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 535866\n",
      "  training_iteration: 67\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    67</td><td style=\"text-align: right;\">         1025.59</td><td style=\"text-align: right;\">535866</td><td style=\"text-align: right;\">   409.1</td><td style=\"text-align: right;\">                 459</td><td style=\"text-align: right;\">                 343</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1087728\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-36-13\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 454.0\n",
      "  episode_reward_mean: 403.63\n",
      "  episode_reward_min: 343.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 5436\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.21080978214740753\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008737009949982166\n",
      "          model: {}\n",
      "          policy_loss: -0.012997649610042572\n",
      "          total_loss: 255.27430725097656\n",
      "          vf_explained_var: 0.9406144022941589\n",
      "          vf_loss: 255.2784423828125\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.38731491565704346\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.004902175161987543\n",
      "          model: {}\n",
      "          policy_loss: -0.006861207541078329\n",
      "          total_loss: 1282.6334228515625\n",
      "          vf_explained_var: -7.690921677294682e-09\n",
      "          vf_loss: 1282.6353759765625\n",
      "    num_agent_steps_sampled: 1087728\n",
      "    num_agent_steps_trained: 1087728\n",
      "    num_steps_sampled: 543864\n",
      "    num_steps_trained: 543864\n",
      "  iterations_since_restore: 68\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 73.02333333333333\n",
      "    ram_util_percent: 94.61999999999999\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 347.0\n",
      "    agent-1: 210.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 258.54\n",
      "    agent-1: 145.09\n",
      "  policy_reward_min:\n",
      "    agent-0: 204.0\n",
      "    agent-1: 101.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1174851608193529\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10540515481822114\n",
      "    mean_inference_ms: 1.8934548521671086\n",
      "    mean_raw_obs_processing_ms: 0.16876703261329992\n",
      "  time_since_restore: 1046.6991701126099\n",
      "  time_this_iter_s: 21.110825300216675\n",
      "  time_total_s: 1046.6991701126099\n",
      "  timers:\n",
      "    learn_throughput: 764.126\n",
      "    learn_time_ms: 10466.865\n",
      "    load_throughput: 307553.068\n",
      "    load_time_ms: 26.005\n",
      "    sample_throughput: 1066.568\n",
      "    sample_time_ms: 7498.816\n",
      "    update_time_ms: 6.32\n",
      "  timestamp: 1627274173\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 543864\n",
      "  training_iteration: 68\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    68</td><td style=\"text-align: right;\">          1046.7</td><td style=\"text-align: right;\">543864</td><td style=\"text-align: right;\">  403.63</td><td style=\"text-align: right;\">                 454</td><td style=\"text-align: right;\">                 343</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1103724\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-36-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 458.0\n",
      "  episode_reward_mean: 404.19\n",
      "  episode_reward_min: 348.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 5517\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.2237769514322281\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008343319408595562\n",
      "          model: {}\n",
      "          policy_loss: -0.012482939288020134\n",
      "          total_loss: 165.97698974609375\n",
      "          vf_explained_var: 0.9636139869689941\n",
      "          vf_loss: 165.98098754882812\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.37503042817115784\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01162716280668974\n",
      "          model: {}\n",
      "          policy_loss: -0.011549455113708973\n",
      "          total_loss: 1270.005126953125\n",
      "          vf_explained_var: -3.845460838647341e-09\n",
      "          vf_loss: 1270.010986328125\n",
      "    num_agent_steps_sampled: 1103724\n",
      "    num_agent_steps_trained: 1103724\n",
      "    num_steps_sampled: 551862\n",
      "    num_steps_trained: 551862\n",
      "  iterations_since_restore: 69\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 62.39999999999999\n",
      "    ram_util_percent: 95.28260869565217\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 319.0\n",
      "    agent-1: 203.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 256.02\n",
      "    agent-1: 148.17\n",
      "  policy_reward_min:\n",
      "    agent-0: 217.0\n",
      "    agent-1: 103.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11783162649966446\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10578243456170416\n",
      "    mean_inference_ms: 1.9031270348404785\n",
      "    mean_raw_obs_processing_ms: 0.16917668030782873\n",
      "  time_since_restore: 1063.2869262695312\n",
      "  time_this_iter_s: 16.587756156921387\n",
      "  time_total_s: 1063.2869262695312\n",
      "  timers:\n",
      "    learn_throughput: 768.61\n",
      "    learn_time_ms: 10405.803\n",
      "    load_throughput: 291878.837\n",
      "    load_time_ms: 27.402\n",
      "    sample_throughput: 1042.073\n",
      "    sample_time_ms: 7675.088\n",
      "    update_time_ms: 5.237\n",
      "  timestamp: 1627274190\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 551862\n",
      "  training_iteration: 69\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    69</td><td style=\"text-align: right;\">         1063.29</td><td style=\"text-align: right;\">551862</td><td style=\"text-align: right;\">  404.19</td><td style=\"text-align: right;\">                 458</td><td style=\"text-align: right;\">                 348</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1119720\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-36-45\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 453.0\n",
      "  episode_reward_mean: 403.53\n",
      "  episode_reward_min: 353.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 5598\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.22130627930164337\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008648939430713654\n",
      "          model: {}\n",
      "          policy_loss: -0.01712965965270996\n",
      "          total_loss: 223.8227996826172\n",
      "          vf_explained_var: 0.9505885243415833\n",
      "          vf_loss: 223.83116149902344\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3570132851600647\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009383369237184525\n",
      "          model: {}\n",
      "          policy_loss: -0.01027295645326376\n",
      "          total_loss: 1328.9539794921875\n",
      "          vf_explained_var: -9.613652096618353e-10\n",
      "          vf_loss: 1328.95947265625\n",
      "    num_agent_steps_sampled: 1119720\n",
      "    num_agent_steps_trained: 1119720\n",
      "    num_steps_sampled: 559860\n",
      "    num_steps_trained: 559860\n",
      "  iterations_since_restore: 70\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.76190476190476\n",
      "    ram_util_percent: 95.33333333333333\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 322.0\n",
      "    agent-1: 225.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 253.24\n",
      "    agent-1: 150.29\n",
      "  policy_reward_min:\n",
      "    agent-0: 192.0\n",
      "    agent-1: 110.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11784580007133293\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10580143891652412\n",
      "    mean_inference_ms: 1.903461739166208\n",
      "    mean_raw_obs_processing_ms: 0.16919937721919395\n",
      "  time_since_restore: 1077.9848141670227\n",
      "  time_this_iter_s: 14.697887897491455\n",
      "  time_total_s: 1077.9848141670227\n",
      "  timers:\n",
      "    learn_throughput: 798.008\n",
      "    learn_time_ms: 10022.46\n",
      "    load_throughput: 275524.716\n",
      "    load_time_ms: 29.028\n",
      "    sample_throughput: 1055.79\n",
      "    sample_time_ms: 7575.368\n",
      "    update_time_ms: 4.039\n",
      "  timestamp: 1627274205\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 559860\n",
      "  training_iteration: 70\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    70</td><td style=\"text-align: right;\">         1077.98</td><td style=\"text-align: right;\">559860</td><td style=\"text-align: right;\">  403.53</td><td style=\"text-align: right;\">                 453</td><td style=\"text-align: right;\">                 353</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1135716\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-37-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 453.0\n",
      "  episode_reward_mean: 397.21\n",
      "  episode_reward_min: 353.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 5676\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.2261674553155899\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008258604444563389\n",
      "          model: {}\n",
      "          policy_loss: -0.013557943515479565\n",
      "          total_loss: 188.52090454101562\n",
      "          vf_explained_var: 0.9470757246017456\n",
      "          vf_loss: 188.52606201171875\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.36757364869117737\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007868926972150803\n",
      "          model: {}\n",
      "          policy_loss: -0.007285761181265116\n",
      "          total_loss: 1311.07470703125\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 1311.077880859375\n",
      "    num_agent_steps_sampled: 1135716\n",
      "    num_agent_steps_trained: 1135716\n",
      "    num_steps_sampled: 567858\n",
      "    num_steps_trained: 567858\n",
      "  iterations_since_restore: 71\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.2142857142857\n",
      "    ram_util_percent: 95.32857142857145\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 322.0\n",
      "    agent-1: 193.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 247.48\n",
      "    agent-1: 149.73\n",
      "  policy_reward_min:\n",
      "    agent-0: 196.0\n",
      "    agent-1: 109.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11779529714529925\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10571967264598975\n",
      "    mean_inference_ms: 1.9014343794779434\n",
      "    mean_raw_obs_processing_ms: 0.1691652731923626\n",
      "  time_since_restore: 1092.883995294571\n",
      "  time_this_iter_s: 14.899181127548218\n",
      "  time_total_s: 1092.883995294571\n",
      "  timers:\n",
      "    learn_throughput: 822.529\n",
      "    learn_time_ms: 9723.667\n",
      "    load_throughput: 375584.922\n",
      "    load_time_ms: 21.295\n",
      "    sample_throughput: 1146.048\n",
      "    sample_time_ms: 6978.767\n",
      "    update_time_ms: 3.082\n",
      "  timestamp: 1627274220\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 567858\n",
      "  training_iteration: 71\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.6/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    71</td><td style=\"text-align: right;\">         1092.88</td><td style=\"text-align: right;\">567858</td><td style=\"text-align: right;\">  397.21</td><td style=\"text-align: right;\">                 453</td><td style=\"text-align: right;\">                 353</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1151712\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-37-15\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 453.0\n",
      "  episode_reward_mean: 393.87\n",
      "  episode_reward_min: 348.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 5757\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.21824930608272552\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0074771917425096035\n",
      "          model: {}\n",
      "          policy_loss: -0.013683434575796127\n",
      "          total_loss: 137.37155151367188\n",
      "          vf_explained_var: 0.9616237878799438\n",
      "          vf_loss: 137.37765502929688\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3603613078594208\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007100180257111788\n",
      "          model: {}\n",
      "          policy_loss: -0.007212744560092688\n",
      "          total_loss: 1251.3216552734375\n",
      "          vf_explained_var: -1.0575017306280188e-08\n",
      "          vf_loss: 1251.3250732421875\n",
      "    num_agent_steps_sampled: 1151712\n",
      "    num_agent_steps_trained: 1151712\n",
      "    num_steps_sampled: 575856\n",
      "    num_steps_trained: 575856\n",
      "  iterations_since_restore: 72\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.795652173913034\n",
      "    ram_util_percent: 95.22608695652175\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 310.0\n",
      "    agent-1: 206.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 246.36\n",
      "    agent-1: 147.51\n",
      "  policy_reward_min:\n",
      "    agent-0: 192.0\n",
      "    agent-1: 107.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11775419892732175\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1057564762013725\n",
      "    mean_inference_ms: 1.901819380443165\n",
      "    mean_raw_obs_processing_ms: 0.16905234873550104\n",
      "  time_since_restore: 1108.7179687023163\n",
      "  time_this_iter_s: 15.833973407745361\n",
      "  time_total_s: 1108.7179687023163\n",
      "  timers:\n",
      "    learn_throughput: 837.309\n",
      "    learn_time_ms: 9552.034\n",
      "    load_throughput: 401507.157\n",
      "    load_time_ms: 19.92\n",
      "    sample_throughput: 1157.857\n",
      "    sample_time_ms: 6907.586\n",
      "    update_time_ms: 3.099\n",
      "  timestamp: 1627274235\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 575856\n",
      "  training_iteration: 72\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    72</td><td style=\"text-align: right;\">         1108.72</td><td style=\"text-align: right;\">575856</td><td style=\"text-align: right;\">  393.87</td><td style=\"text-align: right;\">                 453</td><td style=\"text-align: right;\">                 348</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1167708\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-37-30\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 462.0\n",
      "  episode_reward_mean: 395.47\n",
      "  episode_reward_min: 357.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 5838\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.21521374583244324\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008109299466013908\n",
      "          model: {}\n",
      "          policy_loss: -0.011786166578531265\n",
      "          total_loss: 177.97830200195312\n",
      "          vf_explained_var: 0.9496250748634338\n",
      "          vf_loss: 177.98187255859375\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3555956482887268\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010127558372914791\n",
      "          model: {}\n",
      "          policy_loss: -0.008780930191278458\n",
      "          total_loss: 1304.407958984375\n",
      "          vf_explained_var: 1.1536382515942023e-08\n",
      "          vf_loss: 1304.4117431640625\n",
      "    num_agent_steps_sampled: 1167708\n",
      "    num_agent_steps_trained: 1167708\n",
      "    num_steps_sampled: 583854\n",
      "    num_steps_trained: 583854\n",
      "  iterations_since_restore: 73\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.07142857142857\n",
      "    ram_util_percent: 95.27142857142856\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 339.0\n",
      "    agent-1: 199.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 246.91\n",
      "    agent-1: 148.56\n",
      "  policy_reward_min:\n",
      "    agent-0: 186.0\n",
      "    agent-1: 104.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11776603181392725\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10576237357616913\n",
      "    mean_inference_ms: 1.9015947489869758\n",
      "    mean_raw_obs_processing_ms: 0.16906394927837412\n",
      "  time_since_restore: 1123.6086654663086\n",
      "  time_this_iter_s: 14.89069676399231\n",
      "  time_total_s: 1123.6086654663086\n",
      "  timers:\n",
      "    learn_throughput: 845.809\n",
      "    learn_time_ms: 9456.037\n",
      "    load_throughput: 409343.696\n",
      "    load_time_ms: 19.539\n",
      "    sample_throughput: 1178.754\n",
      "    sample_time_ms: 6785.132\n",
      "    update_time_ms: 3.031\n",
      "  timestamp: 1627274250\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 583854\n",
      "  training_iteration: 73\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    73</td><td style=\"text-align: right;\">         1123.61</td><td style=\"text-align: right;\">583854</td><td style=\"text-align: right;\">  395.47</td><td style=\"text-align: right;\">                 462</td><td style=\"text-align: right;\">                 357</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1183704\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-37-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 462.0\n",
      "  episode_reward_mean: 393.38\n",
      "  episode_reward_min: 356.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 5916\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.2174830436706543\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007178018800914288\n",
      "          model: {}\n",
      "          policy_loss: -0.014257552102208138\n",
      "          total_loss: 153.66136169433594\n",
      "          vf_explained_var: 0.9524946808815002\n",
      "          vf_loss: 153.6683349609375\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.34816214442253113\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008357376791536808\n",
      "          model: {}\n",
      "          policy_loss: -0.005369727034121752\n",
      "          total_loss: 1259.9105224609375\n",
      "          vf_explained_var: 1.9227304193236705e-09\n",
      "          vf_loss: 1259.91162109375\n",
      "    num_agent_steps_sampled: 1183704\n",
      "    num_agent_steps_trained: 1183704\n",
      "    num_steps_sampled: 591852\n",
      "    num_steps_trained: 591852\n",
      "  iterations_since_restore: 74\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 58.02272727272726\n",
      "    ram_util_percent: 95.37272727272729\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 339.0\n",
      "    agent-1: 199.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 245.34\n",
      "    agent-1: 148.04\n",
      "  policy_reward_min:\n",
      "    agent-0: 186.0\n",
      "    agent-1: 104.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1178253483561166\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10575188802151544\n",
      "    mean_inference_ms: 1.9015588550239613\n",
      "    mean_raw_obs_processing_ms: 0.16916356680108657\n",
      "  time_since_restore: 1139.01744556427\n",
      "  time_this_iter_s: 15.408780097961426\n",
      "  time_total_s: 1139.01744556427\n",
      "  timers:\n",
      "    learn_throughput: 847.248\n",
      "    learn_time_ms: 9439.97\n",
      "    load_throughput: 400633.965\n",
      "    load_time_ms: 19.963\n",
      "    sample_throughput: 1192.94\n",
      "    sample_time_ms: 6704.443\n",
      "    update_time_ms: 2.913\n",
      "  timestamp: 1627274266\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 591852\n",
      "  training_iteration: 74\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    74</td><td style=\"text-align: right;\">         1139.02</td><td style=\"text-align: right;\">591852</td><td style=\"text-align: right;\">  393.38</td><td style=\"text-align: right;\">                 462</td><td style=\"text-align: right;\">                 356</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1199700\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-38-01\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 438.0\n",
      "  episode_reward_mean: 391.16\n",
      "  episode_reward_min: 335.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 5997\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.19314740598201752\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007340146228671074\n",
      "          model: {}\n",
      "          policy_loss: -0.008881542831659317\n",
      "          total_loss: 162.77044677734375\n",
      "          vf_explained_var: 0.9631467461585999\n",
      "          vf_loss: 162.77191162109375\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.34314361214637756\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007959908805787563\n",
      "          model: {}\n",
      "          policy_loss: -0.007275136653333902\n",
      "          total_loss: 1172.923095703125\n",
      "          vf_explained_var: 2.8840956289855058e-09\n",
      "          vf_loss: 1172.9263916015625\n",
      "    num_agent_steps_sampled: 1199700\n",
      "    num_agent_steps_trained: 1199700\n",
      "    num_steps_sampled: 599850\n",
      "    num_steps_trained: 599850\n",
      "  iterations_since_restore: 75\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.00869565217391\n",
      "    ram_util_percent: 95.29565217391306\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 301.0\n",
      "    agent-1: 197.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 245.73\n",
      "    agent-1: 145.43\n",
      "  policy_reward_min:\n",
      "    agent-0: 201.0\n",
      "    agent-1: 111.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11787049403759202\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10584745797453873\n",
      "    mean_inference_ms: 1.9036828061765163\n",
      "    mean_raw_obs_processing_ms: 0.16915610866069275\n",
      "  time_since_restore: 1154.656236410141\n",
      "  time_this_iter_s: 15.638790845870972\n",
      "  time_total_s: 1154.656236410141\n",
      "  timers:\n",
      "    learn_throughput: 851.232\n",
      "    learn_time_ms: 9395.789\n",
      "    load_throughput: 413147.439\n",
      "    load_time_ms: 19.359\n",
      "    sample_throughput: 1188.433\n",
      "    sample_time_ms: 6729.873\n",
      "    update_time_ms: 2.898\n",
      "  timestamp: 1627274281\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 599850\n",
      "  training_iteration: 75\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.7/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    75</td><td style=\"text-align: right;\">         1154.66</td><td style=\"text-align: right;\">599850</td><td style=\"text-align: right;\">  391.16</td><td style=\"text-align: right;\">                 438</td><td style=\"text-align: right;\">                 335</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1215696\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-38-17\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 456.0\n",
      "  episode_reward_mean: 389.69\n",
      "  episode_reward_min: 332.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 6078\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.18508246541023254\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007466468960046768\n",
      "          model: {}\n",
      "          policy_loss: -0.014211460016667843\n",
      "          total_loss: 234.74261474609375\n",
      "          vf_explained_var: 0.9341297149658203\n",
      "          vf_loss: 234.74925231933594\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3438134789466858\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01092876773327589\n",
      "          model: {}\n",
      "          policy_loss: -0.006608286406844854\n",
      "          total_loss: 1195.5748291015625\n",
      "          vf_explained_var: -2.8840956289855058e-09\n",
      "          vf_loss: 1195.5755615234375\n",
      "    num_agent_steps_sampled: 1215696\n",
      "    num_agent_steps_trained: 1215696\n",
      "    num_steps_sampled: 607848\n",
      "    num_steps_trained: 607848\n",
      "  iterations_since_restore: 76\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 57.2142857142857\n",
      "    ram_util_percent: 93.43333333333332\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 338.0\n",
      "    agent-1: 180.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 247.67\n",
      "    agent-1: 142.02\n",
      "  policy_reward_min:\n",
      "    agent-0: 196.0\n",
      "    agent-1: 111.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11792166633733003\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10590280086380656\n",
      "    mean_inference_ms: 1.9049624237546356\n",
      "    mean_raw_obs_processing_ms: 0.1691941323519691\n",
      "  time_since_restore: 1169.8265364170074\n",
      "  time_this_iter_s: 15.170300006866455\n",
      "  time_total_s: 1169.8265364170074\n",
      "  timers:\n",
      "    learn_throughput: 853.198\n",
      "    learn_time_ms: 9374.141\n",
      "    load_throughput: 405559.338\n",
      "    load_time_ms: 19.721\n",
      "    sample_throughput: 1185.91\n",
      "    sample_time_ms: 6744.186\n",
      "    update_time_ms: 2.948\n",
      "  timestamp: 1627274297\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 607848\n",
      "  training_iteration: 76\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    76</td><td style=\"text-align: right;\">         1169.83</td><td style=\"text-align: right;\">607848</td><td style=\"text-align: right;\">  389.69</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">                 332</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1231692\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-38-31\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 456.0\n",
      "  episode_reward_mean: 388.06\n",
      "  episode_reward_min: 347.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6156\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.18638938665390015\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007208842318505049\n",
      "          model: {}\n",
      "          policy_loss: -0.011877807788550854\n",
      "          total_loss: 131.11129760742188\n",
      "          vf_explained_var: 0.9580504894256592\n",
      "          vf_loss: 131.1158905029297\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.32913312315940857\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0073147122748196125\n",
      "          model: {}\n",
      "          policy_loss: -0.006899221334606409\n",
      "          total_loss: 1186.114990234375\n",
      "          vf_explained_var: -1.1536382515942023e-08\n",
      "          vf_loss: 1186.1181640625\n",
      "    num_agent_steps_sampled: 1231692\n",
      "    num_agent_steps_trained: 1231692\n",
      "    num_steps_sampled: 615846\n",
      "    num_steps_trained: 615846\n",
      "  iterations_since_restore: 77\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.38571428571428\n",
      "    ram_util_percent: 92.76190476190474\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 318.0\n",
      "    agent-1: 189.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 242.33\n",
      "    agent-1: 145.73\n",
      "  policy_reward_min:\n",
      "    agent-0: 195.0\n",
      "    agent-1: 107.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11797782547751551\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10587883449466866\n",
      "    mean_inference_ms: 1.904344869042334\n",
      "    mean_raw_obs_processing_ms: 0.1692941544508026\n",
      "  time_since_restore: 1184.5591912269592\n",
      "  time_this_iter_s: 14.732654809951782\n",
      "  time_total_s: 1184.5591912269592\n",
      "  timers:\n",
      "    learn_throughput: 871.798\n",
      "    learn_time_ms: 9174.139\n",
      "    load_throughput: 394033.053\n",
      "    load_time_ms: 20.298\n",
      "    sample_throughput: 1195.223\n",
      "    sample_time_ms: 6691.641\n",
      "    update_time_ms: 2.713\n",
      "  timestamp: 1627274311\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 615846\n",
      "  training_iteration: 77\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    77</td><td style=\"text-align: right;\">         1184.56</td><td style=\"text-align: right;\">615846</td><td style=\"text-align: right;\">  388.06</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">                 347</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1247688\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-38-46\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 458.0\n",
      "  episode_reward_mean: 385.54\n",
      "  episode_reward_min: 333.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 6237\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.19268780946731567\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006365378387272358\n",
      "          model: {}\n",
      "          policy_loss: -0.011625024490058422\n",
      "          total_loss: 228.68162536621094\n",
      "          vf_explained_var: 0.9424388408660889\n",
      "          vf_loss: 228.68678283691406\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.30645880103111267\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008750597015023232\n",
      "          model: {}\n",
      "          policy_loss: -0.005175802856683731\n",
      "          total_loss: 1218.7540283203125\n",
      "          vf_explained_var: -4.806826048309176e-09\n",
      "          vf_loss: 1218.754638671875\n",
      "    num_agent_steps_sampled: 1247688\n",
      "    num_agent_steps_trained: 1247688\n",
      "    num_steps_sampled: 623844\n",
      "    num_steps_trained: 623844\n",
      "  iterations_since_restore: 78\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.57142857142857\n",
      "    ram_util_percent: 92.72857142857144\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 313.0\n",
      "    agent-1: 197.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 237.82\n",
      "    agent-1: 147.72\n",
      "  policy_reward_min:\n",
      "    agent-0: 197.0\n",
      "    agent-1: 105.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11784754597989484\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10580302689346943\n",
      "    mean_inference_ms: 1.9021927443833868\n",
      "    mean_raw_obs_processing_ms: 0.16906148804568744\n",
      "  time_since_restore: 1199.1142642498016\n",
      "  time_this_iter_s: 14.555073022842407\n",
      "  time_total_s: 1199.1142642498016\n",
      "  timers:\n",
      "    learn_throughput: 898.221\n",
      "    learn_time_ms: 8904.267\n",
      "    load_throughput: 400620.568\n",
      "    load_time_ms: 19.964\n",
      "    sample_throughput: 1268.234\n",
      "    sample_time_ms: 6306.405\n",
      "    update_time_ms: 2.533\n",
      "  timestamp: 1627274326\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 623844\n",
      "  training_iteration: 78\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    78</td><td style=\"text-align: right;\">         1199.11</td><td style=\"text-align: right;\">623844</td><td style=\"text-align: right;\">  385.54</td><td style=\"text-align: right;\">                 458</td><td style=\"text-align: right;\">                 333</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1263684\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-39-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 441.0\n",
      "  episode_reward_mean: 384.48\n",
      "  episode_reward_min: 340.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 6318\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.18874792754650116\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0072004045359790325\n",
      "          model: {}\n",
      "          policy_loss: -0.012590460479259491\n",
      "          total_loss: 161.13934326171875\n",
      "          vf_explained_var: 0.9491716623306274\n",
      "          vf_loss: 161.1446533203125\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.33306923508644104\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00887164194136858\n",
      "          model: {}\n",
      "          policy_loss: -0.007909141480922699\n",
      "          total_loss: 1229.1573486328125\n",
      "          vf_explained_var: -7.690921677294682e-09\n",
      "          vf_loss: 1229.16064453125\n",
      "    num_agent_steps_sampled: 1263684\n",
      "    num_agent_steps_trained: 1263684\n",
      "    num_steps_sampled: 631842\n",
      "    num_steps_trained: 631842\n",
      "  iterations_since_restore: 79\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.85\n",
      "    ram_util_percent: 92.85000000000001\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 305.0\n",
      "    agent-1: 208.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 238.39\n",
      "    agent-1: 146.09\n",
      "  policy_reward_min:\n",
      "    agent-0: 193.0\n",
      "    agent-1: 107.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11773839102537863\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10570632080087254\n",
      "    mean_inference_ms: 1.8997068687798337\n",
      "    mean_raw_obs_processing_ms: 0.16891803373326073\n",
      "  time_since_restore: 1212.9095318317413\n",
      "  time_this_iter_s: 13.795267581939697\n",
      "  time_total_s: 1212.9095318317413\n",
      "  timers:\n",
      "    learn_throughput: 904.711\n",
      "    learn_time_ms: 8840.388\n",
      "    load_throughput: 447308.61\n",
      "    load_time_ms: 17.88\n",
      "    sample_throughput: 1312.624\n",
      "    sample_time_ms: 6093.138\n",
      "    update_time_ms: 2.522\n",
      "  timestamp: 1627274340\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 631842\n",
      "  training_iteration: 79\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    79</td><td style=\"text-align: right;\">         1212.91</td><td style=\"text-align: right;\">631842</td><td style=\"text-align: right;\">  384.48</td><td style=\"text-align: right;\">                 441</td><td style=\"text-align: right;\">                 340</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1279680\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-39-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 438.0\n",
      "  episode_reward_mean: 385.72\n",
      "  episode_reward_min: 340.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6396\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.18967969715595245\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006678653880953789\n",
      "          model: {}\n",
      "          policy_loss: -0.008573032915592194\n",
      "          total_loss: 105.11003875732422\n",
      "          vf_explained_var: 0.9676216244697571\n",
      "          vf_loss: 105.11185455322266\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.328370064496994\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006730645429342985\n",
      "          model: {}\n",
      "          policy_loss: -0.005988960154354572\n",
      "          total_loss: 1264.57080078125\n",
      "          vf_explained_var: 1.9227304193236705e-09\n",
      "          vf_loss: 1264.5736083984375\n",
      "    num_agent_steps_sampled: 1279680\n",
      "    num_agent_steps_trained: 1279680\n",
      "    num_steps_sampled: 639840\n",
      "    num_steps_trained: 639840\n",
      "  iterations_since_restore: 80\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.142857142857146\n",
      "    ram_util_percent: 92.53809523809521\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 305.0\n",
      "    agent-1: 204.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 236.81\n",
      "    agent-1: 148.91\n",
      "  policy_reward_min:\n",
      "    agent-0: 199.0\n",
      "    agent-1: 110.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11764992498475828\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10556152083403719\n",
      "    mean_inference_ms: 1.896683340132542\n",
      "    mean_raw_obs_processing_ms: 0.16883104704447582\n",
      "  time_since_restore: 1227.4358949661255\n",
      "  time_this_iter_s: 14.526363134384155\n",
      "  time_total_s: 1227.4358949661255\n",
      "  timers:\n",
      "    learn_throughput: 901.603\n",
      "    learn_time_ms: 8870.869\n",
      "    load_throughput: 487825.371\n",
      "    load_time_ms: 16.395\n",
      "    sample_throughput: 1322.625\n",
      "    sample_time_ms: 6047.068\n",
      "    update_time_ms: 2.528\n",
      "  timestamp: 1627274354\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 639840\n",
      "  training_iteration: 80\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    80</td><td style=\"text-align: right;\">         1227.44</td><td style=\"text-align: right;\">639840</td><td style=\"text-align: right;\">  385.72</td><td style=\"text-align: right;\">                 438</td><td style=\"text-align: right;\">                 340</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1295676\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-39-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 427.0\n",
      "  episode_reward_mean: 384.0\n",
      "  episode_reward_min: 347.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 6477\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.21083444356918335\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007866072468459606\n",
      "          model: {}\n",
      "          policy_loss: -0.014118032529950142\n",
      "          total_loss: 199.45018005371094\n",
      "          vf_explained_var: 0.9504311680793762\n",
      "          vf_loss: 199.45632934570312\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3014853298664093\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008552772924304008\n",
      "          model: {}\n",
      "          policy_loss: -0.00799654982984066\n",
      "          total_loss: 1272.019775390625\n",
      "          vf_explained_var: -2.8840956289855058e-09\n",
      "          vf_loss: 1272.023193359375\n",
      "    num_agent_steps_sampled: 1295676\n",
      "    num_agent_steps_trained: 1295676\n",
      "    num_steps_sampled: 647838\n",
      "    num_steps_trained: 647838\n",
      "  iterations_since_restore: 81\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 54.39\n",
      "    ram_util_percent: 92.60999999999999\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 307.0\n",
      "    agent-1: 212.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 233.7\n",
      "    agent-1: 150.3\n",
      "  policy_reward_min:\n",
      "    agent-0: 188.0\n",
      "    agent-1: 102.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11755874535747952\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10553084807240301\n",
      "    mean_inference_ms: 1.8956892022422769\n",
      "    mean_raw_obs_processing_ms: 0.16865184889155885\n",
      "  time_since_restore: 1241.624763250351\n",
      "  time_this_iter_s: 14.188868284225464\n",
      "  time_total_s: 1241.624763250351\n",
      "  timers:\n",
      "    learn_throughput: 910.787\n",
      "    learn_time_ms: 8781.414\n",
      "    load_throughput: 490199.148\n",
      "    load_time_ms: 16.316\n",
      "    sample_throughput: 1318.558\n",
      "    sample_time_ms: 6065.717\n",
      "    update_time_ms: 2.533\n",
      "  timestamp: 1627274369\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 647838\n",
      "  training_iteration: 81\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    81</td><td style=\"text-align: right;\">         1241.62</td><td style=\"text-align: right;\">647838</td><td style=\"text-align: right;\">     384</td><td style=\"text-align: right;\">                 427</td><td style=\"text-align: right;\">                 347</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1311672\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-39-43\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 438.0\n",
      "  episode_reward_mean: 385.19\n",
      "  episode_reward_min: 348.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 6558\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.20569045841693878\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005740252323448658\n",
      "          model: {}\n",
      "          policy_loss: -0.009977834299206734\n",
      "          total_loss: 166.03468322753906\n",
      "          vf_explained_var: 0.9478086233139038\n",
      "          vf_loss: 166.03883361816406\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.31988704204559326\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.014227853156626225\n",
      "          model: {}\n",
      "          policy_loss: -0.01477471087127924\n",
      "          total_loss: 1304.12255859375\n",
      "          vf_explained_var: -1.9227304193236705e-09\n",
      "          vf_loss: 1304.1300048828125\n",
      "    num_agent_steps_sampled: 1311672\n",
      "    num_agent_steps_trained: 1311672\n",
      "    num_steps_sampled: 655836\n",
      "    num_steps_trained: 655836\n",
      "  iterations_since_restore: 82\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.510000000000005\n",
      "    ram_util_percent: 92.58500000000001\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 334.0\n",
      "    agent-1: 236.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 231.32\n",
      "    agent-1: 153.87\n",
      "  policy_reward_min:\n",
      "    agent-0: 181.0\n",
      "    agent-1: 99.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11742433128960496\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10540542640179479\n",
      "    mean_inference_ms: 1.8927915790716954\n",
      "    mean_raw_obs_processing_ms: 0.16846950633724855\n",
      "  time_since_restore: 1255.673288345337\n",
      "  time_this_iter_s: 14.048525094985962\n",
      "  time_total_s: 1255.673288345337\n",
      "  timers:\n",
      "    learn_throughput: 919.36\n",
      "    learn_time_ms: 8699.529\n",
      "    load_throughput: 485355.066\n",
      "    load_time_ms: 16.479\n",
      "    sample_throughput: 1339.91\n",
      "    sample_time_ms: 5969.06\n",
      "    update_time_ms: 2.466\n",
      "  timestamp: 1627274383\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 655836\n",
      "  training_iteration: 82\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    82</td><td style=\"text-align: right;\">         1255.67</td><td style=\"text-align: right;\">655836</td><td style=\"text-align: right;\">  385.19</td><td style=\"text-align: right;\">                 438</td><td style=\"text-align: right;\">                 348</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1327668\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-39-56\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 435.0\n",
      "  episode_reward_mean: 391.38\n",
      "  episode_reward_min: 340.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6636\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.18983130156993866\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.015707677230238914\n",
      "          model: {}\n",
      "          policy_loss: -0.0040581487119197845\n",
      "          total_loss: 199.2979736328125\n",
      "          vf_explained_var: 0.9425944089889526\n",
      "          vf_loss: 199.28611755371094\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.34248262643814087\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009543460793793201\n",
      "          model: {}\n",
      "          policy_loss: -0.011629887856543064\n",
      "          total_loss: 1282.70166015625\n",
      "          vf_explained_var: 9.613652096618353e-10\n",
      "          vf_loss: 1282.7081298828125\n",
      "    num_agent_steps_sampled: 1327668\n",
      "    num_agent_steps_trained: 1327668\n",
      "    num_steps_sampled: 663834\n",
      "    num_steps_trained: 663834\n",
      "  iterations_since_restore: 83\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.76315789473685\n",
      "    ram_util_percent: 92.64736842105265\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 302.0\n",
      "    agent-1: 236.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 241.34\n",
      "    agent-1: 150.04\n",
      "  policy_reward_min:\n",
      "    agent-0: 181.0\n",
      "    agent-1: 107.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11732722584225269\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10525828809061459\n",
      "    mean_inference_ms: 1.889551573930083\n",
      "    mean_raw_obs_processing_ms: 0.16837177541154652\n",
      "  time_since_restore: 1269.2602314949036\n",
      "  time_this_iter_s: 13.58694314956665\n",
      "  time_total_s: 1269.2602314949036\n",
      "  timers:\n",
      "    learn_throughput: 925.704\n",
      "    learn_time_ms: 8639.909\n",
      "    load_throughput: 503878.95\n",
      "    load_time_ms: 15.873\n",
      "    sample_throughput: 1355.839\n",
      "    sample_time_ms: 5898.932\n",
      "    update_time_ms: 2.496\n",
      "  timestamp: 1627274396\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 663834\n",
      "  training_iteration: 83\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    83</td><td style=\"text-align: right;\">         1269.26</td><td style=\"text-align: right;\">663834</td><td style=\"text-align: right;\">  391.38</td><td style=\"text-align: right;\">                 435</td><td style=\"text-align: right;\">                 340</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1343664\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-40-11\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 453.0\n",
      "  episode_reward_mean: 400.26\n",
      "  episode_reward_min: 350.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 6717\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.18342237174510956\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005932702217251062\n",
      "          model: {}\n",
      "          policy_loss: -0.011731968261301517\n",
      "          total_loss: 359.8079528808594\n",
      "          vf_explained_var: 0.9245317578315735\n",
      "          vf_loss: 359.8136901855469\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3376181721687317\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.01038801483809948\n",
      "          model: {}\n",
      "          policy_loss: -0.006766797974705696\n",
      "          total_loss: 1159.9189453125\n",
      "          vf_explained_var: -9.613652096618353e-10\n",
      "          vf_loss: 1159.9202880859375\n",
      "    num_agent_steps_sampled: 1343664\n",
      "    num_agent_steps_trained: 1343664\n",
      "    num_steps_sampled: 671832\n",
      "    num_steps_trained: 671832\n",
      "  iterations_since_restore: 84\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.67619047619048\n",
      "    ram_util_percent: 92.59047619047618\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 327.0\n",
      "    agent-1: 187.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 259.08\n",
      "    agent-1: 141.18\n",
      "  policy_reward_min:\n",
      "    agent-0: 196.0\n",
      "    agent-1: 92.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11728430863791743\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10527816651112273\n",
      "    mean_inference_ms: 1.889372346926379\n",
      "    mean_raw_obs_processing_ms: 0.1682568886081919\n",
      "  time_since_restore: 1283.6309788227081\n",
      "  time_this_iter_s: 14.370747327804565\n",
      "  time_total_s: 1283.6309788227081\n",
      "  timers:\n",
      "    learn_throughput: 935.972\n",
      "    learn_time_ms: 8545.127\n",
      "    load_throughput: 507721.814\n",
      "    load_time_ms: 15.753\n",
      "    sample_throughput: 1357.909\n",
      "    sample_time_ms: 5889.937\n",
      "    update_time_ms: 2.515\n",
      "  timestamp: 1627274411\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 671832\n",
      "  training_iteration: 84\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    84</td><td style=\"text-align: right;\">         1283.63</td><td style=\"text-align: right;\">671832</td><td style=\"text-align: right;\">  400.26</td><td style=\"text-align: right;\">                 453</td><td style=\"text-align: right;\">                 350</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1359660\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-40-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 452.0\n",
      "  episode_reward_mean: 400.37\n",
      "  episode_reward_min: 336.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 6798\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.19256429374217987\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006227440666407347\n",
      "          model: {}\n",
      "          policy_loss: -0.01044674962759018\n",
      "          total_loss: 310.50701904296875\n",
      "          vf_explained_var: 0.9226920008659363\n",
      "          vf_loss: 310.51116943359375\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3428267240524292\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013894950971007347\n",
      "          model: {}\n",
      "          policy_loss: -0.00932086817920208\n",
      "          total_loss: 1316.433837890625\n",
      "          vf_explained_var: 4.806826048309176e-09\n",
      "          vf_loss: 1316.43603515625\n",
      "    num_agent_steps_sampled: 1359660\n",
      "    num_agent_steps_trained: 1359660\n",
      "    num_steps_sampled: 679830\n",
      "    num_steps_trained: 679830\n",
      "  iterations_since_restore: 85\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.71052631578949\n",
      "    ram_util_percent: 92.60526315789471\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 321.0\n",
      "    agent-1: 202.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 253.96\n",
      "    agent-1: 146.41\n",
      "  policy_reward_min:\n",
      "    agent-0: 186.0\n",
      "    agent-1: 92.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11718886003610912\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10518351079285\n",
      "    mean_inference_ms: 1.8870467167041869\n",
      "    mean_raw_obs_processing_ms: 0.16813248049089133\n",
      "  time_since_restore: 1297.1506655216217\n",
      "  time_this_iter_s: 13.519686698913574\n",
      "  time_total_s: 1297.1506655216217\n",
      "  timers:\n",
      "    learn_throughput: 945.749\n",
      "    learn_time_ms: 8456.789\n",
      "    load_throughput: 511212.08\n",
      "    load_time_ms: 15.645\n",
      "    sample_throughput: 1386.966\n",
      "    sample_time_ms: 5766.545\n",
      "    update_time_ms: 2.474\n",
      "  timestamp: 1627274424\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 679830\n",
      "  training_iteration: 85\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    85</td><td style=\"text-align: right;\">         1297.15</td><td style=\"text-align: right;\">679830</td><td style=\"text-align: right;\">  400.37</td><td style=\"text-align: right;\">                 452</td><td style=\"text-align: right;\">                 336</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1375656\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-40-39\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 456.0\n",
      "  episode_reward_mean: 404.39\n",
      "  episode_reward_min: 343.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 6876\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.1745738983154297\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006919703911989927\n",
      "          model: {}\n",
      "          policy_loss: -0.006926167756319046\n",
      "          total_loss: 538.5416870117188\n",
      "          vf_explained_var: 0.8760116100311279\n",
      "          vf_loss: 538.5415649414062\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.34402745962142944\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.011256600730121136\n",
      "          model: {}\n",
      "          policy_loss: -0.008557308465242386\n",
      "          total_loss: 1292.21533203125\n",
      "          vf_explained_var: -1.9227304193236705e-09\n",
      "          vf_loss: 1292.21826171875\n",
      "    num_agent_steps_sampled: 1375656\n",
      "    num_agent_steps_trained: 1375656\n",
      "    num_steps_sampled: 687828\n",
      "    num_steps_trained: 687828\n",
      "  iterations_since_restore: 86\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.466666666666676\n",
      "    ram_util_percent: 92.55714285714284\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 328.0\n",
      "    agent-1: 202.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 260.37\n",
      "    agent-1: 144.02\n",
      "  policy_reward_min:\n",
      "    agent-0: 197.0\n",
      "    agent-1: 111.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11717733772723263\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10512756155675594\n",
      "    mean_inference_ms: 1.8856339854130852\n",
      "    mean_raw_obs_processing_ms: 0.1681692153350272\n",
      "  time_since_restore: 1311.448516368866\n",
      "  time_this_iter_s: 14.297850847244263\n",
      "  time_total_s: 1311.448516368866\n",
      "  timers:\n",
      "    learn_throughput: 949.831\n",
      "    learn_time_ms: 8420.447\n",
      "    load_throughput: 515189.412\n",
      "    load_time_ms: 15.524\n",
      "    sample_throughput: 1399.243\n",
      "    sample_time_ms: 5715.947\n",
      "    update_time_ms: 2.424\n",
      "  timestamp: 1627274439\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 687828\n",
      "  training_iteration: 86\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    86</td><td style=\"text-align: right;\">         1311.45</td><td style=\"text-align: right;\">687828</td><td style=\"text-align: right;\">  404.39</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">                 343</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1391652\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-40-53\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 456.0\n",
      "  episode_reward_mean: 412.04\n",
      "  episode_reward_min: 362.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 6957\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.17002327740192413\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007400567643344402\n",
      "          model: {}\n",
      "          policy_loss: -0.009287199005484581\n",
      "          total_loss: 471.2423400878906\n",
      "          vf_explained_var: 0.909981906414032\n",
      "          vf_loss: 471.24407958984375\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.36152204871177673\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009339685551822186\n",
      "          model: {}\n",
      "          policy_loss: -0.007079597096890211\n",
      "          total_loss: 1247.2047119140625\n",
      "          vf_explained_var: -6.729556467632847e-09\n",
      "          vf_loss: 1247.207275390625\n",
      "    num_agent_steps_sampled: 1391652\n",
      "    num_agent_steps_trained: 1391652\n",
      "    num_steps_sampled: 695826\n",
      "    num_steps_trained: 695826\n",
      "  iterations_since_restore: 87\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.864999999999995\n",
      "    ram_util_percent: 92.56999999999998\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 340.0\n",
      "    agent-1: 202.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 273.07\n",
      "    agent-1: 138.97\n",
      "  policy_reward_min:\n",
      "    agent-0: 185.0\n",
      "    agent-1: 105.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11706575853511986\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10508907793422591\n",
      "    mean_inference_ms: 1.8841700769192335\n",
      "    mean_raw_obs_processing_ms: 0.16798911712839498\n",
      "  time_since_restore: 1325.3640069961548\n",
      "  time_this_iter_s: 13.915490627288818\n",
      "  time_total_s: 1325.3640069961548\n",
      "  timers:\n",
      "    learn_throughput: 953.554\n",
      "    learn_time_ms: 8387.567\n",
      "    load_throughput: 546344.49\n",
      "    load_time_ms: 14.639\n",
      "    sample_throughput: 1411.083\n",
      "    sample_time_ms: 5667.986\n",
      "    update_time_ms: 2.447\n",
      "  timestamp: 1627274453\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 695826\n",
      "  training_iteration: 87\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    87</td><td style=\"text-align: right;\">         1325.36</td><td style=\"text-align: right;\">695826</td><td style=\"text-align: right;\">  412.04</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">                 362</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1407648\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-41-06\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 456.0\n",
      "  episode_reward_mean: 414.57\n",
      "  episode_reward_min: 353.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 7038\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.16374143958091736\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005894342437386513\n",
      "          model: {}\n",
      "          policy_loss: -0.011010376736521721\n",
      "          total_loss: 389.39202880859375\n",
      "          vf_explained_var: 0.9089522957801819\n",
      "          vf_loss: 389.3970947265625\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.36553773283958435\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010419492609798908\n",
      "          model: {}\n",
      "          policy_loss: -0.006856672931462526\n",
      "          total_loss: 1428.127197265625\n",
      "          vf_explained_var: -1.9227304193236705e-09\n",
      "          vf_loss: 1428.1287841796875\n",
      "    num_agent_steps_sampled: 1407648\n",
      "    num_agent_steps_trained: 1407648\n",
      "    num_steps_sampled: 703824\n",
      "    num_steps_trained: 703824\n",
      "  iterations_since_restore: 88\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.368421052631575\n",
      "    ram_util_percent: 92.46315789473682\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 331.0\n",
      "    agent-1: 264.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 270.61\n",
      "    agent-1: 143.96\n",
      "  policy_reward_min:\n",
      "    agent-0: 169.0\n",
      "    agent-1: 100.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11695657043120555\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10498661655744555\n",
      "    mean_inference_ms: 1.8816528779139423\n",
      "    mean_raw_obs_processing_ms: 0.16783154025493766\n",
      "  time_since_restore: 1339.000409603119\n",
      "  time_this_iter_s: 13.636402606964111\n",
      "  time_total_s: 1339.000409603119\n",
      "  timers:\n",
      "    learn_throughput: 963.241\n",
      "    learn_time_ms: 8303.217\n",
      "    load_throughput: 543145.815\n",
      "    load_time_ms: 14.725\n",
      "    sample_throughput: 1412.929\n",
      "    sample_time_ms: 5660.58\n",
      "    update_time_ms: 2.413\n",
      "  timestamp: 1627274466\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 703824\n",
      "  training_iteration: 88\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    88</td><td style=\"text-align: right;\">            1339</td><td style=\"text-align: right;\">703824</td><td style=\"text-align: right;\">  414.57</td><td style=\"text-align: right;\">                 456</td><td style=\"text-align: right;\">                 353</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1423644\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-41-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 465.0\n",
      "  episode_reward_mean: 414.7\n",
      "  episode_reward_min: 353.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 7116\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.1809963881969452\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007176894228905439\n",
      "          model: {}\n",
      "          policy_loss: -0.008660020306706429\n",
      "          total_loss: 427.41583251953125\n",
      "          vf_explained_var: 0.8932651281356812\n",
      "          vf_loss: 427.417236328125\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.36056041717529297\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009578526020050049\n",
      "          model: {}\n",
      "          policy_loss: -0.008729458786547184\n",
      "          total_loss: 1377.582275390625\n",
      "          vf_explained_var: -7.690921677294682e-09\n",
      "          vf_loss: 1377.5860595703125\n",
      "    num_agent_steps_sampled: 1423644\n",
      "    num_agent_steps_trained: 1423644\n",
      "    num_steps_sampled: 711822\n",
      "    num_steps_trained: 711822\n",
      "  iterations_since_restore: 89\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.05\n",
      "    ram_util_percent: 92.495\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 331.0\n",
      "    agent-1: 218.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 270.05\n",
      "    agent-1: 144.65\n",
      "  policy_reward_min:\n",
      "    agent-0: 194.0\n",
      "    agent-1: 109.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1168813621802871\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10486547657591441\n",
      "    mean_inference_ms: 1.8788793872308065\n",
      "    mean_raw_obs_processing_ms: 0.16778089303809438\n",
      "  time_since_restore: 1352.6318988800049\n",
      "  time_this_iter_s: 13.631489276885986\n",
      "  time_total_s: 1352.6318988800049\n",
      "  timers:\n",
      "    learn_throughput: 963.29\n",
      "    learn_time_ms: 8302.794\n",
      "    load_throughput: 538535.835\n",
      "    load_time_ms: 14.851\n",
      "    sample_throughput: 1416.941\n",
      "    sample_time_ms: 5644.554\n",
      "    update_time_ms: 2.417\n",
      "  timestamp: 1627274480\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 711822\n",
      "  training_iteration: 89\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    89</td><td style=\"text-align: right;\">         1352.63</td><td style=\"text-align: right;\">711822</td><td style=\"text-align: right;\">   414.7</td><td style=\"text-align: right;\">                 465</td><td style=\"text-align: right;\">                 353</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1439640\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-41-35\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 465.0\n",
      "  episode_reward_mean: 409.84\n",
      "  episode_reward_min: 358.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 7197\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.18436583876609802\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007575497962534428\n",
      "          model: {}\n",
      "          policy_loss: -0.009899790398776531\n",
      "          total_loss: 288.3080749511719\n",
      "          vf_explained_var: 0.9315858483314514\n",
      "          vf_loss: 288.310302734375\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.34809359908103943\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.013200116343796253\n",
      "          model: {}\n",
      "          policy_loss: -0.010275275446474552\n",
      "          total_loss: 1323.7767333984375\n",
      "          vf_explained_var: -5.7681912579710115e-09\n",
      "          vf_loss: 1323.7806396484375\n",
      "    num_agent_steps_sampled: 1439640\n",
      "    num_agent_steps_trained: 1439640\n",
      "    num_steps_sampled: 719820\n",
      "    num_steps_trained: 719820\n",
      "  iterations_since_restore: 90\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.504999999999995\n",
      "    ram_util_percent: 92.425\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 331.0\n",
      "    agent-1: 218.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 261.72\n",
      "    agent-1: 148.12\n",
      "  policy_reward_min:\n",
      "    agent-0: 198.0\n",
      "    agent-1: 108.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11676502688742241\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10481276157167634\n",
      "    mean_inference_ms: 1.8773071741252432\n",
      "    mean_raw_obs_processing_ms: 0.16758155431762276\n",
      "  time_since_restore: 1367.1652836799622\n",
      "  time_this_iter_s: 14.533384799957275\n",
      "  time_total_s: 1367.1652836799622\n",
      "  timers:\n",
      "    learn_throughput: 965.588\n",
      "    learn_time_ms: 8283.036\n",
      "    load_throughput: 546818.273\n",
      "    load_time_ms: 14.626\n",
      "    sample_throughput: 1411.785\n",
      "    sample_time_ms: 5665.169\n",
      "    update_time_ms: 2.443\n",
      "  timestamp: 1627274495\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 719820\n",
      "  training_iteration: 90\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    90</td><td style=\"text-align: right;\">         1367.17</td><td style=\"text-align: right;\">719820</td><td style=\"text-align: right;\">  409.84</td><td style=\"text-align: right;\">                 465</td><td style=\"text-align: right;\">                 358</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1455636\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-41-49\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 448.0\n",
      "  episode_reward_mean: 402.36\n",
      "  episode_reward_min: 356.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 7278\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.2055835872888565\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007257877849042416\n",
      "          model: {}\n",
      "          policy_loss: -0.008871902711689472\n",
      "          total_loss: 214.63816833496094\n",
      "          vf_explained_var: 0.9412497878074646\n",
      "          vf_loss: 214.6396942138672\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.35778453946113586\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.012199493125081062\n",
      "          model: {}\n",
      "          policy_loss: -0.015218434855341911\n",
      "          total_loss: 1386.2918701171875\n",
      "          vf_explained_var: -1.9227304193236705e-09\n",
      "          vf_loss: 1386.3009033203125\n",
      "    num_agent_steps_sampled: 1455636\n",
      "    num_agent_steps_trained: 1455636\n",
      "    num_steps_sampled: 727818\n",
      "    num_steps_trained: 727818\n",
      "  iterations_since_restore: 91\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.11428571428572\n",
      "    ram_util_percent: 92.61904761904763\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 314.0\n",
      "    agent-1: 191.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 252.13\n",
      "    agent-1: 150.23\n",
      "  policy_reward_min:\n",
      "    agent-0: 206.0\n",
      "    agent-1: 108.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.116751601330584\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10478612555052194\n",
      "    mean_inference_ms: 1.8767829127734217\n",
      "    mean_raw_obs_processing_ms: 0.16755749595348257\n",
      "  time_since_restore: 1381.7807796001434\n",
      "  time_this_iter_s: 14.615495920181274\n",
      "  time_total_s: 1381.7807796001434\n",
      "  timers:\n",
      "    learn_throughput: 961.571\n",
      "    learn_time_ms: 8317.638\n",
      "    load_throughput: 544174.912\n",
      "    load_time_ms: 14.697\n",
      "    sample_throughput: 1409.807\n",
      "    sample_time_ms: 5673.118\n",
      "    update_time_ms: 2.489\n",
      "  timestamp: 1627274509\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 727818\n",
      "  training_iteration: 91\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    91</td><td style=\"text-align: right;\">         1381.78</td><td style=\"text-align: right;\">727818</td><td style=\"text-align: right;\">  402.36</td><td style=\"text-align: right;\">                 448</td><td style=\"text-align: right;\">                 356</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1471632\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-42-04\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 430.0\n",
      "  episode_reward_mean: 396.75\n",
      "  episode_reward_min: 351.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 7356\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.1995619237422943\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.0073877316899597645\n",
      "          model: {}\n",
      "          policy_loss: -0.008442332036793232\n",
      "          total_loss: 144.68338012695312\n",
      "          vf_explained_var: 0.9536446332931519\n",
      "          vf_loss: 144.68434143066406\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3557206988334656\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010516746900975704\n",
      "          model: {}\n",
      "          policy_loss: -0.012134883552789688\n",
      "          total_loss: 1343.6964111328125\n",
      "          vf_explained_var: -5.7681912579710115e-09\n",
      "          vf_loss: 1343.7032470703125\n",
      "    num_agent_steps_sampled: 1471632\n",
      "    num_agent_steps_trained: 1471632\n",
      "    num_steps_sampled: 735816\n",
      "    num_steps_trained: 735816\n",
      "  iterations_since_restore: 92\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.42857142857143\n",
      "    ram_util_percent: 92.76190476190474\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 285.0\n",
      "    agent-1: 203.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 246.15\n",
      "    agent-1: 150.6\n",
      "  policy_reward_min:\n",
      "    agent-0: 198.0\n",
      "    agent-1: 108.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11672541523362145\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10470407654425612\n",
      "    mean_inference_ms: 1.8749210001525893\n",
      "    mean_raw_obs_processing_ms: 0.16755346296552226\n",
      "  time_since_restore: 1396.194790840149\n",
      "  time_this_iter_s: 14.414011240005493\n",
      "  time_total_s: 1396.194790840149\n",
      "  timers:\n",
      "    learn_throughput: 960.792\n",
      "    learn_time_ms: 8324.386\n",
      "    load_throughput: 540783.485\n",
      "    load_time_ms: 14.79\n",
      "    sample_throughput: 1402.498\n",
      "    sample_time_ms: 5702.683\n",
      "    update_time_ms: 2.552\n",
      "  timestamp: 1627274524\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 735816\n",
      "  training_iteration: 92\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    92</td><td style=\"text-align: right;\">         1396.19</td><td style=\"text-align: right;\">735816</td><td style=\"text-align: right;\">  396.75</td><td style=\"text-align: right;\">                 430</td><td style=\"text-align: right;\">                 351</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1487628\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-42-18\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 430.0\n",
      "  episode_reward_mean: 391.6\n",
      "  episode_reward_min: 342.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 7437\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.20529185235500336\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009585930965840816\n",
      "          model: {}\n",
      "          policy_loss: -0.010495685040950775\n",
      "          total_loss: 118.1724624633789\n",
      "          vf_explained_var: 0.9617153406143188\n",
      "          vf_loss: 118.17328643798828\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.33744725584983826\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.008341672830283642\n",
      "          model: {}\n",
      "          policy_loss: -0.010322635993361473\n",
      "          total_loss: 1384.7373046875\n",
      "          vf_explained_var: -3.845460838647341e-09\n",
      "          vf_loss: 1384.7432861328125\n",
      "    num_agent_steps_sampled: 1487628\n",
      "    num_agent_steps_trained: 1487628\n",
      "    num_steps_sampled: 743814\n",
      "    num_steps_trained: 743814\n",
      "  iterations_since_restore: 93\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.36\n",
      "    ram_util_percent: 92.72\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 285.0\n",
      "    agent-1: 205.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 236.3\n",
      "    agent-1: 155.3\n",
      "  policy_reward_min:\n",
      "    agent-0: 203.0\n",
      "    agent-1: 115.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11658817748098631\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10465428148687415\n",
      "    mean_inference_ms: 1.873071753873238\n",
      "    mean_raw_obs_processing_ms: 0.1673281912375738\n",
      "  time_since_restore: 1410.5304071903229\n",
      "  time_this_iter_s: 14.33561635017395\n",
      "  time_total_s: 1410.5304071903229\n",
      "  timers:\n",
      "    learn_throughput: 952.7\n",
      "    learn_time_ms: 8395.092\n",
      "    load_throughput: 550857.146\n",
      "    load_time_ms: 14.519\n",
      "    sample_throughput: 1401.41\n",
      "    sample_time_ms: 5707.111\n",
      "    update_time_ms: 2.566\n",
      "  timestamp: 1627274538\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 743814\n",
      "  training_iteration: 93\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    93</td><td style=\"text-align: right;\">         1410.53</td><td style=\"text-align: right;\">743814</td><td style=\"text-align: right;\">   391.6</td><td style=\"text-align: right;\">                 430</td><td style=\"text-align: right;\">                 342</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1503624\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-42-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 431.0\n",
      "  episode_reward_mean: 388.56\n",
      "  episode_reward_min: 348.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 7518\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.2193996012210846\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006257872562855482\n",
      "          model: {}\n",
      "          policy_loss: -0.011065023951232433\n",
      "          total_loss: 94.89165496826172\n",
      "          vf_explained_var: 0.9652630090713501\n",
      "          vf_loss: 94.89637756347656\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.33917683362960815\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010765311308205128\n",
      "          model: {}\n",
      "          policy_loss: -0.014236243441700935\n",
      "          total_loss: 1417.833251953125\n",
      "          vf_explained_var: -5.7681912579710115e-09\n",
      "          vf_loss: 1417.8424072265625\n",
      "    num_agent_steps_sampled: 1503624\n",
      "    num_agent_steps_trained: 1503624\n",
      "    num_steps_sampled: 751812\n",
      "    num_steps_trained: 751812\n",
      "  iterations_since_restore: 94\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.171428571428564\n",
      "    ram_util_percent: 92.55714285714286\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 280.0\n",
      "    agent-1: 208.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 234.18\n",
      "    agent-1: 154.38\n",
      "  policy_reward_min:\n",
      "    agent-0: 202.0\n",
      "    agent-1: 101.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11654354852644193\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10461198730774608\n",
      "    mean_inference_ms: 1.871940314314305\n",
      "    mean_raw_obs_processing_ms: 0.16724417815171447\n",
      "  time_since_restore: 1425.1542119979858\n",
      "  time_this_iter_s: 14.623804807662964\n",
      "  time_total_s: 1425.1542119979858\n",
      "  timers:\n",
      "    learn_throughput: 945.888\n",
      "    learn_time_ms: 8455.544\n",
      "    load_throughput: 543071.954\n",
      "    load_time_ms: 14.727\n",
      "    sample_throughput: 1410.164\n",
      "    sample_time_ms: 5671.681\n",
      "    update_time_ms: 2.592\n",
      "  timestamp: 1627274553\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 751812\n",
      "  training_iteration: 94\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    94</td><td style=\"text-align: right;\">         1425.15</td><td style=\"text-align: right;\">751812</td><td style=\"text-align: right;\">  388.56</td><td style=\"text-align: right;\">                 431</td><td style=\"text-align: right;\">                 348</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1519620\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-42-47\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 426.0\n",
      "  episode_reward_mean: 388.11\n",
      "  episode_reward_min: 355.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 7596\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.2096399962902069\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005589097272604704\n",
      "          model: {}\n",
      "          policy_loss: -0.005787092726677656\n",
      "          total_loss: 83.94373321533203\n",
      "          vf_explained_var: 0.9679002165794373\n",
      "          vf_loss: 83.9438705444336\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.32894742488861084\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.010158986784517765\n",
      "          model: {}\n",
      "          policy_loss: -0.012029701843857765\n",
      "          total_loss: 1403.47265625\n",
      "          vf_explained_var: -2.8840956289855058e-09\n",
      "          vf_loss: 1403.479248046875\n",
      "    num_agent_steps_sampled: 1519620\n",
      "    num_agent_steps_trained: 1519620\n",
      "    num_steps_sampled: 759810\n",
      "    num_steps_trained: 759810\n",
      "  iterations_since_restore: 95\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.30500000000001\n",
      "    ram_util_percent: 92.6\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 271.0\n",
      "    agent-1: 205.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 229.93\n",
      "    agent-1: 158.18\n",
      "  policy_reward_min:\n",
      "    agent-0: 197.0\n",
      "    agent-1: 118.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1165446204873437\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10456108997547352\n",
      "    mean_inference_ms: 1.8706693557220568\n",
      "    mean_raw_obs_processing_ms: 0.16728598154474164\n",
      "  time_since_restore: 1439.2661483287811\n",
      "  time_this_iter_s: 14.111936330795288\n",
      "  time_total_s: 1439.2661483287811\n",
      "  timers:\n",
      "    learn_throughput: 944.041\n",
      "    learn_time_ms: 8472.089\n",
      "    load_throughput: 536476.61\n",
      "    load_time_ms: 14.908\n",
      "    sample_throughput: 1399.678\n",
      "    sample_time_ms: 5714.17\n",
      "    update_time_ms: 2.623\n",
      "  timestamp: 1627274567\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 759810\n",
      "  training_iteration: 95\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    95</td><td style=\"text-align: right;\">         1439.27</td><td style=\"text-align: right;\">759810</td><td style=\"text-align: right;\">  388.11</td><td style=\"text-align: right;\">                 426</td><td style=\"text-align: right;\">                 355</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1535616\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-43-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 436.0\n",
      "  episode_reward_mean: 392.01\n",
      "  episode_reward_min: 336.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 7677\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.21648798882961273\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006992046255618334\n",
      "          model: {}\n",
      "          policy_loss: -0.006999028380960226\n",
      "          total_loss: 70.91690063476562\n",
      "          vf_explained_var: 0.9743664860725403\n",
      "          vf_loss: 70.91681671142578\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.34174323081970215\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007948988117277622\n",
      "          model: {}\n",
      "          policy_loss: -0.006172352470457554\n",
      "          total_loss: 1484.362548828125\n",
      "          vf_explained_var: -4.806826048309176e-09\n",
      "          vf_loss: 1484.3646240234375\n",
      "    num_agent_steps_sampled: 1535616\n",
      "    num_agent_steps_trained: 1535616\n",
      "    num_steps_sampled: 767808\n",
      "    num_steps_trained: 767808\n",
      "  iterations_since_restore: 96\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 51.964999999999996\n",
      "    ram_util_percent: 92.625\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 270.0\n",
      "    agent-1: 214.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 231.13\n",
      "    agent-1: 160.88\n",
      "  policy_reward_min:\n",
      "    agent-0: 197.0\n",
      "    agent-1: 122.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11643783156150153\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1045174611248084\n",
      "    mean_inference_ms: 1.8693472055681286\n",
      "    mean_raw_obs_processing_ms: 0.16711747490024084\n",
      "  time_since_restore: 1452.8116273880005\n",
      "  time_this_iter_s: 13.54547905921936\n",
      "  time_total_s: 1452.8116273880005\n",
      "  timers:\n",
      "    learn_throughput: 947.984\n",
      "    learn_time_ms: 8436.853\n",
      "    load_throughput: 530908.592\n",
      "    load_time_ms: 15.065\n",
      "    sample_throughput: 1409.632\n",
      "    sample_time_ms: 5673.823\n",
      "    update_time_ms: 2.617\n",
      "  timestamp: 1627274580\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 767808\n",
      "  training_iteration: 96\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    96</td><td style=\"text-align: right;\">         1452.81</td><td style=\"text-align: right;\">767808</td><td style=\"text-align: right;\">  392.01</td><td style=\"text-align: right;\">                 436</td><td style=\"text-align: right;\">                 336</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1551612\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-43-14\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 434.0\n",
      "  episode_reward_mean: 394.93\n",
      "  episode_reward_min: 359.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 7758\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.2158336192369461\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005273258313536644\n",
      "          model: {}\n",
      "          policy_loss: -0.009327440522611141\n",
      "          total_loss: 97.15096282958984\n",
      "          vf_explained_var: 0.9663954377174377\n",
      "          vf_loss: 97.15496063232422\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3455866277217865\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007819114252924919\n",
      "          model: {}\n",
      "          policy_loss: -0.005597372073680162\n",
      "          total_loss: 1441.73388671875\n",
      "          vf_explained_var: 4.806826048309176e-09\n",
      "          vf_loss: 1441.7353515625\n",
      "    num_agent_steps_sampled: 1551612\n",
      "    num_agent_steps_trained: 1551612\n",
      "    num_steps_sampled: 775806\n",
      "    num_steps_trained: 775806\n",
      "  iterations_since_restore: 97\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 52.27\n",
      "    ram_util_percent: 92.66000000000001\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 293.0\n",
      "    agent-1: 214.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 240.09\n",
      "    agent-1: 154.84\n",
      "  policy_reward_min:\n",
      "    agent-0: 204.0\n",
      "    agent-1: 111.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11633486222943751\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10442245917290524\n",
      "    mean_inference_ms: 1.8672118875561683\n",
      "    mean_raw_obs_processing_ms: 0.1669807920029107\n",
      "  time_since_restore: 1466.7751898765564\n",
      "  time_this_iter_s: 13.963562488555908\n",
      "  time_total_s: 1466.7751898765564\n",
      "  timers:\n",
      "    learn_throughput: 945.333\n",
      "    learn_time_ms: 8460.512\n",
      "    load_throughput: 532756.788\n",
      "    load_time_ms: 15.012\n",
      "    sample_throughput: 1414.302\n",
      "    sample_time_ms: 5655.087\n",
      "    update_time_ms: 2.57\n",
      "  timestamp: 1627274594\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 775806\n",
      "  training_iteration: 97\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.2/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    97</td><td style=\"text-align: right;\">         1466.78</td><td style=\"text-align: right;\">775806</td><td style=\"text-align: right;\">  394.93</td><td style=\"text-align: right;\">                 434</td><td style=\"text-align: right;\">                 359</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1567608\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-43-29\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 438.0\n",
      "  episode_reward_mean: 390.59\n",
      "  episode_reward_min: 355.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 7836\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.21546031534671783\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006571006961166859\n",
      "          model: {}\n",
      "          policy_loss: -0.009616047143936157\n",
      "          total_loss: 80.19050598144531\n",
      "          vf_explained_var: 0.9691066741943359\n",
      "          vf_loss: 80.19348907470703\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3351488709449768\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006392396055161953\n",
      "          model: {}\n",
      "          policy_loss: -0.006097888108342886\n",
      "          total_loss: 1395.3172607421875\n",
      "          vf_explained_var: 0.0\n",
      "          vf_loss: 1395.3199462890625\n",
      "    num_agent_steps_sampled: 1567608\n",
      "    num_agent_steps_trained: 1567608\n",
      "    num_steps_sampled: 783804\n",
      "    num_steps_trained: 783804\n",
      "  iterations_since_restore: 98\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.15500000000001\n",
      "    ram_util_percent: 92.54499999999999\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 293.0\n",
      "    agent-1: 212.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 236.62\n",
      "    agent-1: 153.97\n",
      "  policy_reward_min:\n",
      "    agent-0: 200.0\n",
      "    agent-1: 111.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.1163320745519495\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10436549496802422\n",
      "    mean_inference_ms: 1.8659153462479336\n",
      "    mean_raw_obs_processing_ms: 0.16699583018554276\n",
      "  time_since_restore: 1481.0173554420471\n",
      "  time_this_iter_s: 14.242165565490723\n",
      "  time_total_s: 1481.0173554420471\n",
      "  timers:\n",
      "    learn_throughput: 943.771\n",
      "    learn_time_ms: 8474.514\n",
      "    load_throughput: 537260.201\n",
      "    load_time_ms: 14.887\n",
      "    sample_throughput: 1402.737\n",
      "    sample_time_ms: 5701.71\n",
      "    update_time_ms: 2.657\n",
      "  timestamp: 1627274609\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 783804\n",
      "  training_iteration: 98\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    98</td><td style=\"text-align: right;\">         1481.02</td><td style=\"text-align: right;\">783804</td><td style=\"text-align: right;\">  390.59</td><td style=\"text-align: right;\">                 438</td><td style=\"text-align: right;\">                 355</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1583604\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-43-42\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 437.0\n",
      "  episode_reward_mean: 389.52\n",
      "  episode_reward_min: 355.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 7917\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.208974689245224\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005429182201623917\n",
      "          model: {}\n",
      "          policy_loss: -0.008406792767345905\n",
      "          total_loss: 61.02883529663086\n",
      "          vf_explained_var: 0.9736587405204773\n",
      "          vf_loss: 61.03174591064453\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.3224791884422302\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.005722557194530964\n",
      "          model: {}\n",
      "          policy_loss: -0.005945986602455378\n",
      "          total_loss: 1461.0670166015625\n",
      "          vf_explained_var: -9.613652096618353e-09\n",
      "          vf_loss: 1461.0701904296875\n",
      "    num_agent_steps_sampled: 1583604\n",
      "    num_agent_steps_trained: 1583604\n",
      "    num_steps_sampled: 791802\n",
      "    num_steps_trained: 791802\n",
      "  iterations_since_restore: 99\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 53.15\n",
      "    ram_util_percent: 92.66\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 267.0\n",
      "    agent-1: 213.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 235.26\n",
      "    agent-1: 154.26\n",
      "  policy_reward_min:\n",
      "    agent-0: 200.0\n",
      "    agent-1: 113.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11625394333096399\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.1043388970805124\n",
      "    mean_inference_ms: 1.8650478309180734\n",
      "    mean_raw_obs_processing_ms: 0.16683316538312415\n",
      "  time_since_restore: 1494.7919149398804\n",
      "  time_this_iter_s: 13.774559497833252\n",
      "  time_total_s: 1494.7919149398804\n",
      "  timers:\n",
      "    learn_throughput: 944.797\n",
      "    learn_time_ms: 8465.312\n",
      "    load_throughput: 479306.565\n",
      "    load_time_ms: 16.687\n",
      "    sample_throughput: 1397.41\n",
      "    sample_time_ms: 5723.445\n",
      "    update_time_ms: 2.648\n",
      "  timestamp: 1627274622\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 791802\n",
      "  training_iteration: 99\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">    99</td><td style=\"text-align: right;\">         1494.79</td><td style=\"text-align: right;\">791802</td><td style=\"text-align: right;\">  389.52</td><td style=\"text-align: right;\">                 437</td><td style=\"text-align: right;\">                 355</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_8803d_00000:\n",
      "  agent_timesteps_total: 1599600\n",
      "  custom_metrics: {}\n",
      "  date: 2021-07-26_16-43-57\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 438.0\n",
      "  episode_reward_mean: 392.04\n",
      "  episode_reward_min: 363.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 7998\n",
      "  experiment_id: b1bbca03a67a4d8b933fd86322895b9b\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 1.0125000476837158\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.2060941606760025\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.006433700677007437\n",
      "          model: {}\n",
      "          policy_loss: -0.008029637858271599\n",
      "          total_loss: 61.23645782470703\n",
      "          vf_explained_var: 0.9744699001312256\n",
      "          vf_loss: 61.23796463012695\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          cur_kl_coeff: 0.5062500238418579\n",
      "          cur_lr: 0.004999999888241291\n",
      "          entropy: 0.33216193318367004\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.007547454908490181\n",
      "          model: {}\n",
      "          policy_loss: -0.00801654439419508\n",
      "          total_loss: 1433.8848876953125\n",
      "          vf_explained_var: -1.7304573773913035e-08\n",
      "          vf_loss: 1433.8890380859375\n",
      "    num_agent_steps_sampled: 1599600\n",
      "    num_agent_steps_trained: 1599600\n",
      "    num_steps_sampled: 799800\n",
      "    num_steps_trained: 799800\n",
      "  iterations_since_restore: 100\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 55.040000000000006\n",
      "    ram_util_percent: 92.96000000000001\n",
      "  pid: 42447\n",
      "  policy_reward_max:\n",
      "    agent-0: 274.0\n",
      "    agent-1: 206.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 239.72\n",
      "    agent-1: 152.32\n",
      "  policy_reward_min:\n",
      "    agent-0: 207.0\n",
      "    agent-1: 109.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.11623828574269325\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.10431799008338057\n",
      "    mean_inference_ms: 1.8646281954109412\n",
      "    mean_raw_obs_processing_ms: 0.1668023297014319\n",
      "  time_since_restore: 1509.0342717170715\n",
      "  time_this_iter_s: 14.242356777191162\n",
      "  time_total_s: 1509.0342717170715\n",
      "  timers:\n",
      "    learn_throughput: 952.376\n",
      "    learn_time_ms: 8397.946\n",
      "    load_throughput: 476488.701\n",
      "    load_time_ms: 16.785\n",
      "    sample_throughput: 1388.107\n",
      "    sample_time_ms: 5761.802\n",
      "    update_time_ms: 2.64\n",
      "  timestamp: 1627274637\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 799800\n",
      "  training_iteration: 100\n",
      "  trial_id: 8803d_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc               </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>RUNNING </td><td>192.168.1.21:42447</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         1509.03</td><td style=\"text-align: right;\">799800</td><td style=\"text-align: right;\">  392.04</td><td style=\"text-align: right;\">                 438</td><td style=\"text-align: right;\">                 363</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/2.63 GiB heap, 0.0/1.32 GiB objects (0.0/1.0 CPU_group_2_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_0_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_1_4090f26a4f1665b6b38916e8da9cb345, 0.0/4.0 CPU_group_4090f26a4f1665b6b38916e8da9cb345, 0.0/1.0 CPU_group_3_4090f26a4f1665b6b38916e8da9cb345)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">    ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_8803d_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         1509.03</td><td style=\"text-align: right;\">799800</td><td style=\"text-align: right;\">  392.04</td><td style=\"text-align: right;\">                 438</td><td style=\"text-align: right;\">                 363</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-26 16:43:57,583\tINFO tune.py:549 -- Total run time: 1528.08 seconds (1527.69 seconds for the tuning loop).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7f3530481040>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-26 17:28:50,330\tWARNING worker.py:1123 -- The autoscaler failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/peter/.local/lib/python3.8/site-packages/ray/autoscaler/_private/monitor.py\", line 317, in run\n",
      "    self._run()\n",
      "  File \"/home/peter/.local/lib/python3.8/site-packages/ray/autoscaler/_private/monitor.py\", line 207, in _run\n",
      "    self.update_load_metrics()\n",
      "  File \"/home/peter/.local/lib/python3.8/site-packages/ray/autoscaler/_private/monitor.py\", line 169, in update_load_metrics\n",
      "    response = self.gcs_node_resources_stub.GetAllResourceUsage(\n",
      "  File \"/home/peter/.local/lib/python3.8/site-packages/grpc/_channel.py\", line 826, in __call__\n",
      "    return _end_unary_response_blocking(state, call, False, None)\n",
      "  File \"/home/peter/.local/lib/python3.8/site-packages/grpc/_channel.py\", line 729, in _end_unary_response_blocking\n",
      "    raise _InactiveRpcError(state)\n",
      "grpc._channel._InactiveRpcError: <_InactiveRpcError of RPC that terminated with:\n",
      "\tstatus = StatusCode.DEADLINE_EXCEEDED\n",
      "\tdetails = \"Deadline Exceeded\"\n",
      "\tdebug_error_string = \"{\"created\":\"@1627277303.737042842\",\"description\":\"Error received from peer ipv4:192.168.1.21:41305\",\"file\":\"src/core/lib/surface/call.cc\",\"file_line\":1061,\"grpc_message\":\"Deadline Exceeded\",\"grpc_status\":4}\"\n",
      ">\n",
      "\n"
     ]
    }
   ],
   "source": [
    "exp_name = 'TA_TEST1'\n",
    "exp_dict = {\n",
    "        'name': exp_name,\n",
    "        'run_or_experiment': 'PPO',\n",
    "        \"stop\": {\n",
    "            \"training_iteration\": 100\n",
    "        },\n",
    "        'checkpoint_freq': 20,\n",
    "        \"config\": config,\n",
    "}\n",
    "# ray.init()\n",
    "tune.run(**exp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
