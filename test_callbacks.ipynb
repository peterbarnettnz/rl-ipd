{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85e61aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/anaconda3/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "from gym import spaces\n",
    "\n",
    "import players\n",
    "import envs\n",
    "from evaluation import generate_history, is_t4t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0387a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-21 11:12:37,510\tINFO services.py:1245 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.21',\n",
       " 'raylet_ip_address': '192.168.1.21',\n",
       " 'redis_address': '192.168.1.21:6379',\n",
       " 'object_store_address': '/tmp/ray/session_2021-08-21_11-12-35_603149_139304/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-08-21_11-12-35_603149_139304/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8265',\n",
       " 'session_dir': '/tmp/ray/session_2021-08-21_11-12-35_603149_139304',\n",
       " 'metrics_export_port': 61335,\n",
       " 'node_id': '95efe6761ce1922a4428ee7acd11dffa3d4d05bd385dde22cd3b4b07'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.examples.env.multi_agent import MultiAgentCartPole\n",
    "from ray.rllib.examples.models.shared_weights_model import \\\n",
    "    SharedWeightsModel1, SharedWeightsModel2, TF2SharedWeightsModel, \\\n",
    "    TorchSharedWeightsModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "# from ray.rllib.policy import PolicySpec\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, PPOTorchPolicy, PPOTFPolicy\n",
    "from ray.rllib.agents.ppo import DEFAULT_CONFIG as DEFAULT_CONFIG_PPO\n",
    "\n",
    "from ray.rllib.agents.dqn import DQNTrainer, DQNTorchPolicy, DQNTFPolicy\n",
    "from ray.rllib.agents.dqn import  DEFAULT_CONFIG as DEFAULT_CONFIG_DQN\n",
    "\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "from ray.rllib.agents.callbacks import DefaultCallbacks\n",
    "from ray.rllib.env import BaseEnv\n",
    "from ray.rllib.evaluation import MultiAgentEpisode, RolloutWorker\n",
    "from ray.rllib.policy import Policy\n",
    "from ray.rllib.policy.sample_batch import SampleBatch\n",
    "\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7587172",
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env('two_agent_MG_env', lambda c: envs.TwoAgentMatrixGameEnv(c))\n",
    "\n",
    "register_env('two_agent_t4t_MG_env', lambda c: envs.TwoAgentSeparateMatrixGameEnv(c))\n",
    "\n",
    "register_env('MG_t4tTD_env', lambda c: envs.MatrixGameEnv(player2=players.TitForTatThenDefect()))\n",
    "# register_env('MG_t4t_env', lambda c: envs.MatrixGameEnv(player2=players.TitForTat()))\n",
    "register_env('MG_t4t_env', lambda c: envs.MatrixGameEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3ee71fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_dqn0 = DEFAULT_CONFIG_DQN.copy()\n",
    "# trainer_config_dqn0['num_workers'] = 3\n",
    "trainer_config_dqn0['n_step'] = 3\n",
    "trainer_config_dqn0['noisy'] = True\n",
    "trainer_config_dqn0['v_min'] = -10.\n",
    "trainer_config_dqn0['v_max'] = 10.\n",
    "trainer_config_dqn0['model']['fcnet_hiddens'] = [256,32,8]\n",
    "trainer_config_dqn0['framework'] = 'torch'\n",
    "# trainer_config_dqn0['framework'] = 'tf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "687700cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_dqn1 = DEFAULT_CONFIG_DQN.copy()\n",
    "# trainer_config_dqn1['num_workers'] = 3\n",
    "trainer_config_dqn1['n_step'] = 3\n",
    "trainer_config_dqn1['noisy'] = True\n",
    "trainer_config_dqn1['v_min'] = -10.\n",
    "trainer_config_dqn1['v_max'] = 10.\n",
    "trainer_config_dqn1['model']['fcnet_hiddens'] = [256,32,8]\n",
    "trainer_config_dqn1['framework'] = 'torch'\n",
    "# trainer_config_dqn1['framework'] = 'tf'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "994a4daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up stuff for pre-training with tit-for-tat\n",
    "\n",
    "def env_creator(_):\n",
    "    return envs.TwoAgentSeparateMatrixGameEnv()\n",
    "single_env = envs.TwoAgentSeparateMatrixGameEnv()\n",
    "env_name = 'two_agent_t4t_MG_env'\n",
    "register_env(env_name, env_creator)\n",
    "\n",
    "\n",
    "\n",
    "obs_space = single_env.observation_space\n",
    "act_space = single_env.action_space\n",
    "num_agents = single_env.num_agents\n",
    "# DQNTorchPolicy0 = DQNTorchPolicy.copy()\n",
    "\n",
    "def gen_policy(i):\n",
    "#     return (PPOTorchPolicy, obs_space, act_space, {})\n",
    "    if i == 0:\n",
    "        conf = trainer_config_dqn0\n",
    "#         conf['restore']='wtf_this better not work'#results.get_last_checkpoint()\n",
    "#         return (DQNTorchPolicy0, obs_space, act_space, conf)\n",
    "#         policy0 = copy.copy(agent0.get_policy())\n",
    "#         policy0 = DQNTorchPolicy(obs_space, act_space, conf)\n",
    "        policy0 = DQNTorchPolicy\n",
    "#         policy0 = agent0.get_policy\n",
    "#         policy0.set_weights(agent0.get_weights()['default_policy'])\n",
    "#         policy0 = lambda c: policy0\n",
    "#         return policy0\n",
    "        return (policy0, obs_space, act_space, conf)\n",
    "\n",
    "    elif i == 1:\n",
    "        conf = trainer_config_dqn1\n",
    "        return (DQNTorchPolicy, obs_space, act_space, conf)\n",
    "    else:\n",
    "        print('NO CONF!')\n",
    "    \n",
    "    return (DQNTorchPolicy, obs_space, act_space, conf)\n",
    "\n",
    "policy_graphs = {}\n",
    "\n",
    "# def restore_fn(agent_id):\n",
    "# #     if agent_id == 0:\n",
    "#     return results.get_last_checkpoint()\n",
    "# #     else:\n",
    "# #         return None\n",
    "    \n",
    "# restore_dict = {}\n",
    "# for i in range(num_agents):\n",
    "#     restore_dict['agent-' + str(i)] = restore_fn(i)\n",
    "\n",
    "for i in range(num_agents):\n",
    "    policy_graphs['agent-' + str(i)] = gen_policy(i)\n",
    "def policy_mapping_fn(agent_id):\n",
    "        return 'agent-' + str(agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a33fbe4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-49-c34c07422145>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-49-c34c07422145>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    assert episode.length == 0,\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "    def on_episode_start(self, *, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                         policies: Dict[str, Policy],\n",
    "                         episode: MultiAgentEpisode, env_index: int, **kwargs):\n",
    "        # Make sure this episode has just been started (only initial obs\n",
    "        # logged so far).\n",
    "        assert episode.length == 0, \n",
    "        episode.user_data[\"it\"] = []\n",
    "        episode.hist_data[\"it\"] = []\n",
    "\n",
    "#     def on_episode_step(self, *, worker: RolloutWorker, base_env: BaseEnv,\n",
    "#                         episode: MultiAgentEpisode, env_index: int, **kwargs):\n",
    "#         # Make sure this episode is ongoing.\n",
    "#         assert episode.length > 0, \\\n",
    "#             \"ERROR: `on_episode_step()` callback should not be called right \" \\\n",
    "#             \"after env reset!\"\n",
    "#         pole_angle = abs(episode.last_observation_for()[2])\n",
    "#         raw_angle = abs(episode.last_raw_obs_for()[2])\n",
    "#         assert pole_angle == raw_angle\n",
    "#         episode.user_data[\"pole_angles\"].append(pole_angle)\n",
    "\n",
    "    def on_episode_end(self, *, worker: RolloutWorker, base_env: BaseEnv,\n",
    "                       policies: Dict[str, Policy], episode: MultiAgentEpisode,\n",
    "                       env_index: int, **kwargs):\n",
    "        # Make sure this episode is really done.\n",
    "        assert episode.batch_builder.policy_collectors[\n",
    "            \"default_policy\"].buffers[\"dones\"][-1], \\\n",
    "            \"ERROR: `on_episode_end()` should only be called \" \\\n",
    "            \"after episode is done!\"\n",
    "        pole_angle = np.mean(episode.user_data[\"pole_angles\"])\n",
    "        print(\"episode {} (env-idx={}) ended with length {} and pole \"\n",
    "              \"angles {}\".format(episode.episode_id, env_index, episode.length,\n",
    "                                 pole_angle))\n",
    "        episode.custom_metrics[\"pole_angle\"] = pole_angle\n",
    "        episode.hist_data[\"pole_angles\"] = episode.user_data[\"pole_angles\"]\n",
    "\n",
    "    def on_sample_end(self, *, worker: RolloutWorker, samples: SampleBatch,\n",
    "                      **kwargs):\n",
    "        print(\"returned sample batch of size {}\".format(samples.count))\n",
    "\n",
    "    def on_train_result(self, *, trainer, result: dict, **kwargs):\n",
    "        print(\"trainer.train() result: {} -> {} episodes\".format(\n",
    "            trainer, result[\"episodes_this_iter\"]))\n",
    "        # you can mutate the result dict to add new fields to return\n",
    "        result[\"callback_ok\"] = True\n",
    "\n",
    "    def on_learn_on_batch(self, *, policy: Policy, train_batch: SampleBatch,\n",
    "                          result: dict, **kwargs) -> None:\n",
    "        result[\"sum_actions_in_train_batch\"] = np.sum(train_batch[\"actions\"])\n",
    "        print(\"policy.learn_on_batch() result: {} -> sum actions: {}\".format(\n",
    "            policy, result[\"sum_actions_in_train_batch\"]))\n",
    "\n",
    "    def on_postprocess_trajectory(\n",
    "            self, *, worker: RolloutWorker, episode: MultiAgentEpisode,\n",
    "            agent_id: str, policy_id: str, policies: Dict[str, Policy],\n",
    "            postprocessed_batch: SampleBatch,\n",
    "            original_batches: Dict[str, SampleBatch], **kwargs):\n",
    "        print(\"postprocessed {} steps\".format(postprocessed_batch.count))\n",
    "        if \"num_batches\" not in episode.custom_metrics:\n",
    "            episode.custom_metrics[\"num_batches\"] = 0\n",
    "        episode.custom_metrics[\"num_batches\"] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81372338",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyCallbacks(DefaultCallbacks):\n",
    "\n",
    "    def on_train_result(self, *, trainer, result: dict, **kwargs) -> None:\n",
    "        \"\"\"Called at the end of Trainable.train().\n",
    "\n",
    "        Args:\n",
    "            trainer (Trainer): Current trainer instance.\n",
    "            result (dict): Dict of results returned from trainer.train() call.\n",
    "                You can mutate this object to add additional metrics.\n",
    "            kwargs: Forward compatibility placeholder.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.legacy_callbacks.get(\"on_train_result\"):\n",
    "            self.legacy_callbacks[\"on_train_result\"]({\n",
    "                \"trainer\": trainer,\n",
    "                \"result\": result,\n",
    "            })\n",
    "        result['t4t_frac_agent-0'], result['coop_frac_agent-0'] = is_t4t(trainer,n_samples=3, policy_id='agent-0')\n",
    "        result['t4t_frac_agent-1'], result['coop_frac_agent-1'] = is_t4t(trainer,n_samples=3, policy_id='agent-1')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbbd97f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f21b7063",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-21 11:14:18,953\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "trainer = DQNTrainer(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf359a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbe032c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-21 11:14:38,483\tWARNING deprecation.py:33 -- DeprecationWarning: `compute_action` has been deprecated. Use `compute_single_action` instead. This will raise an error in the future!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'default_policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-7a841cceab44>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mcompute_action\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1007\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlog_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Trainer.compute_action\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"compute_action\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"compute_single_action\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_single_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mPublicAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36mcompute_single_action\u001b[0;34m(self, observation, state, prev_action, prev_reward, info, policy_id, full_fetch, explore, unsquash_actions, clip_actions)\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m         \u001b[0;31m# Check the preprocessor and preprocess, if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m         \u001b[0mpp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_worker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpolicy_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"NoPreprocessor\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m             \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'default_policy'"
     ]
    }
   ],
   "source": [
    "trainer.compute_action(np.random.rand(400), 'policy_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5a9310",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy_graphs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7134e73a2e10>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     \"model\":{\"fcnet_hiddens\": [1024, 512,256,32,8]},\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \"multiagent\": {\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;34m\"policies\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpolicy_graphs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0;34m\"policy_mapping_fn\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpolicy_mapping_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     },\n",
      "\u001b[0;31mNameError\u001b[0m: name 'policy_graphs' is not defined"
     ]
    }
   ],
   "source": [
    "# make config for pretraining with tit-for-tat\n",
    "\n",
    "config={\n",
    "    \"log_level\": \"WARN\",\n",
    "#     \"num_workers\": 3,\n",
    "#     \"num_cpus_for_driver\": 1,\n",
    "#     \"num_cpus_per_worker\": 1,\n",
    "#     \"lr\": 5e-3,\n",
    "#     \"model\":{\"fcnet_hiddens\": [1024, 512,256,32,8]},\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policy_graphs,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "    },\n",
    "    \"env\": \"two_agent_t4t_MG_env\",\n",
    "    'framework': 'torch',\n",
    "    \"callbacks\": MyCallbacks,\n",
    "#     'framework': 'tf',\n",
    "#             \"resume\":True,\n",
    "#         \"restore\": results.get_last_checkpoint()\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b18b00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/1.83 GiB heap, 0.0/0.92 GiB objects<br>Result logdir: /home/peter/ray_results/TA_TEST3<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_24a63_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_24a63_00000:\n",
      "  agent_timesteps_total: 2000\n",
      "  coop_frac_agent-0: 1.0\n",
      "  coop_frac_agent-1: 0.33333333333333337\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-21_11-13-00\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 461.0\n",
      "  episode_reward_mean: 446.5\n",
      "  episode_reward_min: 433.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 10\n",
      "  experiment_id: 70210c90d0e84263b7b3c510bb85bdef\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 1000\n",
      "    learner:\n",
      "      agent-0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.0005\n",
      "          grad_gnorm: 6.6504621505737305\n",
      "          max_q: 0.8930140733718872\n",
      "          mean_q: 0.40550529956817627\n",
      "          mean_td_error: -4.355594635009766\n",
      "          min_q: -0.1307121366262436\n",
      "        model: {}\n",
      "        td_error: \"[-4.3638225  -6.823642   -4.6358457   0.15035672 -2.7031853  -0.4177984\\n\\\n",
      "          \\ -4.919959   -8.91489    -2.1450436  -7.801814   -4.700367   -3.1714263\\n\\\n",
      "          \\ -3.364237   -1.4560381  -7.3422     -6.841537   -5.3440266  -6.8056216\\n\\\n",
      "          \\ -0.64855987 -8.650839    0.42044428 -0.4177984  -0.4619032  -8.781835\\n\\\n",
      "          \\ -6.0743737  -2.8113225  -3.1151123  -4.313072   -8.786708   -4.5366063\\n\\\n",
      "          \\ -4.829837   -4.7704163 ]\"\n",
      "      agent-1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.0005\n",
      "          grad_gnorm: 4.962874412536621\n",
      "          max_q: 0.0020421966910362244\n",
      "          mean_q: -0.30723780393600464\n",
      "          mean_td_error: -3.645698070526123\n",
      "          min_q: -1.0387606620788574\n",
      "        model: {}\n",
      "        td_error: \"[-3.2772682  -0.8606205  -7.3354354  -5.2606673  -4.8981943  -6.3358917\\n\\\n",
      "          \\ -1.1235144  -2.305695   -9.095274   -1.4143338  -7.12886    -3.2136276\\n\\\n",
      "          \\ -4.9391165  -0.42753845 -5.150629   -4.8103104  -1.1137863  -4.0743246\\n\\\n",
      "          \\ -1.3816295  -5.166976   -0.23185864 -0.9105212  -4.991352   -5.3374825\\n\\\n",
      "          \\ -0.06792077 -4.5377946  -2.998271   -3.4611225  -4.907603   -1.0083035\\n\\\n",
      "          \\ -7.763118   -1.1333053 ]\"\n",
      "    num_agent_steps_sampled: 2000\n",
      "    num_agent_steps_trained: 64\n",
      "    num_steps_sampled: 1000\n",
      "    num_steps_trained: 32\n",
      "    num_target_updates: 1\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 22.06\n",
      "    ram_util_percent: 93.32000000000001\n",
      "  pid: 139440\n",
      "  policy_reward_max:\n",
      "    agent-0: 237.0\n",
      "    agent-1: 236.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 225.7\n",
      "    agent-1: 220.8\n",
      "  policy_reward_min:\n",
      "    agent-0: 217.0\n",
      "    agent-1: 211.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06675196217966604\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06761512794456519\n",
      "    mean_inference_ms: 2.564666273591521\n",
      "    mean_raw_obs_processing_ms: 0.2022015822160018\n",
      "  t4t_frac_agent-0:\n",
      "  - 0.6666666666666666\n",
      "  t4t_frac_agent-1:\n",
      "  - 1.0\n",
      "  time_since_restore: 3.128908395767212\n",
      "  time_this_iter_s: 3.128908395767212\n",
      "  time_total_s: 3.128908395767212\n",
      "  timers:\n",
      "    learn_throughput: 1278.849\n",
      "    learn_time_ms: 25.023\n",
      "  timestamp: 1629501180\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 1000\n",
      "  training_iteration: 1\n",
      "  trial_id: 24a63_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/1.83 GiB heap, 0.0/0.92 GiB objects<br>Result logdir: /home/peter/ray_results/TA_TEST3<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_24a63_00000</td><td>RUNNING </td><td>192.168.1.21:139440</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         3.12891</td><td style=\"text-align: right;\">1000</td><td style=\"text-align: right;\">   446.5</td><td style=\"text-align: right;\">                 461</td><td style=\"text-align: right;\">                 433</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_24a63_00000:\n",
      "  agent_timesteps_total: 4000\n",
      "  coop_frac_agent-0: 1.0\n",
      "  coop_frac_agent-1: 1.0\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-21_11-13-08\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 513.0\n",
      "  episode_reward_mean: 458.65\n",
      "  episode_reward_min: 433.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 20\n",
      "  experiment_id: 70210c90d0e84263b7b3c510bb85bdef\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 1504\n",
      "    learner:\n",
      "      agent-0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.0005\n",
      "          grad_gnorm: 1.8771191835403442\n",
      "          max_q: 11.89836311340332\n",
      "          mean_q: 7.8602705001831055\n",
      "          mean_td_error: 0.35999757051467896\n",
      "          min_q: 3.2044410705566406\n",
      "        model: {}\n",
      "        td_error: \"[ 1.8426661   0.07402658 -5.0726495  -1.833005   -0.6509576   2.0006084\\n\\\n",
      "          \\  0.29657173  0.2269206   0.42355442 -3.276247   -2.8512645   0.63190556\\n\\\n",
      "          \\ -3.7565818  -4.9983506   6.5633307   1.1279068  -0.00825548  2.0006084\\n\\\n",
      "          \\  0.9612665  -2.6251574   2.4385998   6.446781    2.20728     3.9526005\\n\\\n",
      "          \\ -0.7618084   5.5226984  -1.4245405  -1.891809    0.20444107  0.9789057\\n\\\n",
      "          \\  0.9568548   1.8130217 ]\"\n",
      "      agent-1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.0005\n",
      "          grad_gnorm: 1.3541457653045654\n",
      "          max_q: 13.841849327087402\n",
      "          mean_q: 9.43459701538086\n",
      "          mean_td_error: -0.22591669857501984\n",
      "          min_q: 5.224828243255615\n",
      "        model: {}\n",
      "        td_error: \"[ 1.283185    1.3199229   1.0113187   1.3506737  -0.5593624   0.31599808\\n\\\n",
      "          \\  4.2745824  -0.7025156  -0.31456184 -0.08507919 -4.788738   -0.34018707\\n\\\n",
      "          \\ -2.1573124  -5.7259      1.0113187  -0.4344778   2.5652986  -5.3345366\\n\\\n",
      "          \\ -2.2032623  -2.6245232   4.7696576   6.315312   -1.9308033   2.9260674\\n\\\n",
      "          \\  1.4452257   2.6893678  -1.4053097   4.902562   -2.5762148  -4.9618034\\n\\\n",
      "          \\ -1.9307003  -5.334537  ]\"\n",
      "    num_agent_steps_sampled: 4000\n",
      "    num_agent_steps_trained: 16064\n",
      "    num_steps_sampled: 2000\n",
      "    num_steps_trained: 8032\n",
      "    num_target_updates: 2\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 21.55\n",
      "    ram_util_percent: 93.35833333333333\n",
      "  pid: 139440\n",
      "  policy_reward_max:\n",
      "    agent-0: 260.0\n",
      "    agent-1: 253.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 232.6\n",
      "    agent-1: 226.05\n",
      "  policy_reward_min:\n",
      "    agent-0: 217.0\n",
      "    agent-1: 202.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06693425993380947\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.0682225888397622\n",
      "    mean_inference_ms: 2.574279867726407\n",
      "    mean_raw_obs_processing_ms: 0.20275450212099413\n",
      "  t4t_frac_agent-0:\n",
      "  - 0.0\n",
      "  t4t_frac_agent-1:\n",
      "  - 0.6666666666666666\n",
      "  time_since_restore: 11.276414155960083\n",
      "  time_this_iter_s: 8.147505760192871\n",
      "  time_total_s: 11.276414155960083\n",
      "  timers:\n",
      "    learn_throughput: 2477.654\n",
      "    learn_time_ms: 12.915\n",
      "  timestamp: 1629501188\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 2000\n",
      "  training_iteration: 2\n",
      "  trial_id: 24a63_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/1.83 GiB heap, 0.0/0.92 GiB objects<br>Result logdir: /home/peter/ray_results/TA_TEST3<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_24a63_00000</td><td>RUNNING </td><td>192.168.1.21:139440</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">         11.2764</td><td style=\"text-align: right;\">2000</td><td style=\"text-align: right;\">  458.65</td><td style=\"text-align: right;\">                 513</td><td style=\"text-align: right;\">                 433</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_24a63_00000:\n",
      "  agent_timesteps_total: 6000\n",
      "  coop_frac_agent-0: 1.0\n",
      "  coop_frac_agent-1: 0.6666666666666667\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-21_11-13-16\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 513.0\n",
      "  episode_reward_mean: 467.7\n",
      "  episode_reward_min: 433.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 30\n",
      "  experiment_id: 70210c90d0e84263b7b3c510bb85bdef\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 2512\n",
      "    learner:\n",
      "      agent-0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.0005\n",
      "          grad_gnorm: 4.58929443359375\n",
      "          max_q: 22.024850845336914\n",
      "          mean_q: 16.553028106689453\n",
      "          mean_td_error: 2.266909599304199\n",
      "          min_q: 10.451704025268555\n",
      "        model: {}\n",
      "        td_error: \"[-0.7353096   1.8714256   1.8703775  -1.6181889   2.49024     1.3302097\\n\\\n",
      "          \\  1.1748562   0.70038223  5.885974    0.2295475   7.3936224   1.8703775\\n\\\n",
      "          \\  4.7554226   6.9578323  -4.885825    3.263461   -0.18783379 -9.866856\\n\\\n",
      "          \\  1.7699909   1.9379063   2.2544584   5.753578    6.0071936   5.5558157\\n\\\n",
      "          \\ -0.21139622  0.8364334   5.53508    -0.06986332  0.1337452   8.024815\\n\\\n",
      "          \\  5.5558004   6.9578404 ]\"\n",
      "      agent-1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.0005\n",
      "          grad_gnorm: 1.5082744359970093\n",
      "          max_q: 22.03331184387207\n",
      "          mean_q: 16.96892738342285\n",
      "          mean_td_error: -1.602020263671875\n",
      "          min_q: 4.220267295837402\n",
      "        model: {}\n",
      "        td_error: \"[ 1.9123144e+00 -2.7797327e+00  6.5978470e+00 -4.7247047e+00\\n -2.9300556e+00\\\n",
      "          \\  1.0518723e+00 -6.9904709e-01 -7.7478266e+00\\n -3.6252327e+00 -2.2967167e+00\\\n",
      "          \\ -4.1832504e+00  1.5116472e+00\\n -9.0502548e-01 -2.6327801e+00  1.5461189e+01\\\n",
      "          \\ -1.0392742e+00\\n -3.9060974e-01 -1.1277733e+00 -7.1109753e+00 -7.6247406e+00\\n\\\n",
      "          \\  2.5839043e-01 -7.7699080e+00  1.5057564e+00 -6.9208174e+00\\n  2.5749092e+00\\\n",
      "          \\  1.0228157e-03 -5.4384632e+00 -7.4716949e+00\\n  1.9633293e+00 -1.0589542e+00\\\n",
      "          \\ -5.9056816e+00  2.8034210e-01]\"\n",
      "    num_agent_steps_sampled: 6000\n",
      "    num_agent_steps_trained: 32064\n",
      "    num_steps_sampled: 3000\n",
      "    num_steps_trained: 16032\n",
      "    num_target_updates: 4\n",
      "  iterations_since_restore: 3\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.98181818181818\n",
      "    ram_util_percent: 93.4\n",
      "  pid: 139440\n",
      "  policy_reward_max:\n",
      "    agent-0: 260.0\n",
      "    agent-1: 259.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 234.56666666666666\n",
      "    agent-1: 233.13333333333333\n",
      "  policy_reward_min:\n",
      "    agent-0: 217.0\n",
      "    agent-1: 202.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06668120613585563\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06839400822145576\n",
      "    mean_inference_ms: 2.5651601028124253\n",
      "    mean_raw_obs_processing_ms: 0.20214243473358623\n",
      "  t4t_frac_agent-0:\n",
      "  - 0.6666666666666666\n",
      "  t4t_frac_agent-1:\n",
      "  - 0.6666666666666666\n",
      "  time_since_restore: 19.028299570083618\n",
      "  time_this_iter_s: 7.751885414123535\n",
      "  time_total_s: 19.028299570083618\n",
      "  timers:\n",
      "    learn_throughput: 2456.122\n",
      "    learn_time_ms: 13.029\n",
      "  timestamp: 1629501196\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 3000\n",
      "  training_iteration: 3\n",
      "  trial_id: 24a63_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/1.83 GiB heap, 0.0/0.92 GiB objects (0.0/1.0 CPU_group_6fea1609fbac878d7acbf150cdbfd8a2, 0.0/1.0 CPU_group_0_6fea1609fbac878d7acbf150cdbfd8a2)<br>Result logdir: /home/peter/ray_results/TA_TEST3<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_24a63_00000</td><td>RUNNING </td><td>192.168.1.21:139440</td><td style=\"text-align: right;\">     3</td><td style=\"text-align: right;\">         19.0283</td><td style=\"text-align: right;\">3000</td><td style=\"text-align: right;\">   467.7</td><td style=\"text-align: right;\">                 513</td><td style=\"text-align: right;\">                 433</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_24a63_00000:\n",
      "  agent_timesteps_total: 8000\n",
      "  coop_frac_agent-0: 0.33333333333333337\n",
      "  coop_frac_agent-1: 0.6666666666666667\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-21_11-13-24\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 513.0\n",
      "  episode_reward_mean: 474.3\n",
      "  episode_reward_min: 433.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 40\n",
      "  experiment_id: 70210c90d0e84263b7b3c510bb85bdef\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 3520\n",
      "    learner:\n",
      "      agent-0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.0005\n",
      "          grad_gnorm: 2.2042548656463623\n",
      "          max_q: 27.102201461791992\n",
      "          mean_q: 20.21448516845703\n",
      "          mean_td_error: -2.4119162559509277\n",
      "          min_q: 1.008014440536499\n",
      "        model: {}\n",
      "        td_error: \"[ -0.8525162   -3.9969025   -4.951109    -6.6085796    1.4292612\\n\\\n",
      "          \\  -3.8816051    0.28396797   5.546156     6.1859913   -4.528797\\n  -1.0111704\\\n",
      "          \\   -0.737627   -14.727056     8.9542675  -12.033192\\n   0.44884205  -0.50279427\\\n",
      "          \\   0.80319214  -4.132269    -3.4613361\\n  -4.3632545  -10.642659    -7.6069126\\\n",
      "          \\    0.43348694  -0.7568989\\n  -5.773546    -3.9919856   -5.095974    -2.591051\\\n",
      "          \\     0.12986374\\n   2.0947094   -1.2438221 ]\"\n",
      "      agent-1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.0005\n",
      "          grad_gnorm: 0.9573275446891785\n",
      "          max_q: 32.10089111328125\n",
      "          mean_q: 26.847536087036133\n",
      "          mean_td_error: 2.1038753986358643\n",
      "          min_q: 17.24315071105957\n",
      "        model: {}\n",
      "        td_error: \"[-1.0995026   5.2035847  -0.30018806  7.2906084   2.4751186   1.5662632\\n\\\n",
      "          \\ 12.911236   -2.6318703   8.184885    4.118088    1.0068169   5.313858\\n\\\n",
      "          \\ -1.5557175  -7.475868   -1.0049591   4.5533485  -1.9122295  -6.824087\\n\\\n",
      "          \\ -0.58187485 -1.1356983   2.052433    4.210169    3.1857376   0.44149017\\n\\\n",
      "          \\ -2.4612904   4.393013    3.1294498   7.6114807   8.184885    4.961382\\n\\\n",
      "          \\ -0.78165245  4.2950945 ]\"\n",
      "    num_agent_steps_sampled: 8000\n",
      "    num_agent_steps_trained: 48064\n",
      "    num_steps_sampled: 4000\n",
      "    num_steps_trained: 24032\n",
      "    num_target_updates: 6\n",
      "  iterations_since_restore: 4\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 19.072727272727274\n",
      "    ram_util_percent: 93.5\n",
      "  pid: 139440\n",
      "  policy_reward_max:\n",
      "    agent-0: 263.0\n",
      "    agent-1: 260.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 238.0\n",
      "    agent-1: 236.3\n",
      "  policy_reward_min:\n",
      "    agent-0: 217.0\n",
      "    agent-1: 202.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06642697125001698\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06841985655108317\n",
      "    mean_inference_ms: 2.5568539299305604\n",
      "    mean_raw_obs_processing_ms: 0.2017021240478567\n",
      "  t4t_frac_agent-0:\n",
      "  - 0.0\n",
      "  t4t_frac_agent-1:\n",
      "  - 1.0\n",
      "  time_since_restore: 26.858696460723877\n",
      "  time_this_iter_s: 7.830396890640259\n",
      "  time_total_s: 26.858696460723877\n",
      "  timers:\n",
      "    learn_throughput: 2488.86\n",
      "    learn_time_ms: 12.857\n",
      "  timestamp: 1629501204\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 4000\n",
      "  training_iteration: 4\n",
      "  trial_id: 24a63_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/1.83 GiB heap, 0.0/0.92 GiB objects (0.0/1.0 CPU_group_6fea1609fbac878d7acbf150cdbfd8a2, 0.0/1.0 CPU_group_0_6fea1609fbac878d7acbf150cdbfd8a2)<br>Result logdir: /home/peter/ray_results/TA_TEST3<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_24a63_00000</td><td>RUNNING </td><td>192.168.1.21:139440</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         26.8587</td><td style=\"text-align: right;\">4000</td><td style=\"text-align: right;\">   474.3</td><td style=\"text-align: right;\">                 513</td><td style=\"text-align: right;\">                 433</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for DQN_two_agent_t4t_MG_env_24a63_00000:\n",
      "  agent_timesteps_total: 10000\n",
      "  coop_frac_agent-0: 1.0\n",
      "  coop_frac_agent-1: 0.33333333333333337\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-21_11-13-32\n",
      "  done: true\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 531.0\n",
      "  episode_reward_mean: 481.9\n",
      "  episode_reward_min: 433.0\n",
      "  episodes_this_iter: 10\n",
      "  episodes_total: 50\n",
      "  experiment_id: 70210c90d0e84263b7b3c510bb85bdef\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    last_target_update_ts: 4528\n",
      "    learner:\n",
      "      agent-0:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.0005\n",
      "          grad_gnorm: 1.7683415412902832\n",
      "          max_q: 40.280521392822266\n",
      "          mean_q: 26.401500701904297\n",
      "          mean_td_error: 1.9872442483901978\n",
      "          min_q: 3.664611339569092\n",
      "        model: {}\n",
      "        td_error: \"[ 3.6835976  10.354925    2.4421482  -9.174738   -1.6979523   3.4056368\\n\\\n",
      "          \\  0.55655766  3.277443   17.560732    1.6883659  -3.5916252   5.8234196\\n\\\n",
      "          \\ -4.348776    5.8901863   4.4802856   3.2266006   5.6856823   4.552683\\n\\\n",
      "          \\ -2.966259   -1.5991058   8.732645    2.9343052  -5.2966366  -3.3155212\\n\\\n",
      "          \\ -1.3353887   5.025839    5.06791     5.1425858  -2.3730717  -1.9260445\\n\\\n",
      "          \\  3.0963764  -1.4109955 ]\"\n",
      "      agent-1:\n",
      "        custom_metrics: {}\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_lr: 0.0005\n",
      "          grad_gnorm: 0.6286078691482544\n",
      "          max_q: 39.40818786621094\n",
      "          mean_q: 31.94832992553711\n",
      "          mean_td_error: -0.2977098524570465\n",
      "          min_q: 3.353825807571411\n",
      "        model: {}\n",
      "        td_error: \"[  1.1508675   -8.452946    -2.3755112    4.3731346    0.5807648\\n\\\n",
      "          \\   2.705885    -3.7729015    5.37693      0.2591591   -1.2609711\\n   1.9970341\\\n",
      "          \\  -11.398102     8.177708    -9.23469     -1.8347015\\n   0.3538258   -1.3644905\\\n",
      "          \\   -1.4769611   -1.5893669   -4.108143\\n   4.5912304   -3.8479576    2.7306786\\\n",
      "          \\   -0.6926422   -2.0375786\\n   9.743486    -7.200981    -1.6856747    0.03247833\\\n",
      "          \\   1.5200768\\n   7.8652573    1.3483868 ]\"\n",
      "    num_agent_steps_sampled: 10000\n",
      "    num_agent_steps_trained: 64064\n",
      "    num_steps_sampled: 5000\n",
      "    num_steps_trained: 32032\n",
      "    num_target_updates: 8\n",
      "  iterations_since_restore: 5\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 0\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 20.56363636363636\n",
      "    ram_util_percent: 93.51818181818182\n",
      "  pid: 139440\n",
      "  policy_reward_max:\n",
      "    agent-0: 271.0\n",
      "    agent-1: 273.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 241.64\n",
      "    agent-1: 240.26\n",
      "  policy_reward_min:\n",
      "    agent-0: 217.0\n",
      "    agent-1: 202.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.06621154906517086\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.06841722146812793\n",
      "    mean_inference_ms: 2.54907597739497\n",
      "    mean_raw_obs_processing_ms: 0.20136776472135573\n",
      "  t4t_frac_agent-0:\n",
      "  - 0.3333333333333333\n",
      "  t4t_frac_agent-1:\n",
      "  - 0.0\n",
      "  time_since_restore: 34.65098333358765\n",
      "  time_this_iter_s: 7.7922868728637695\n",
      "  time_total_s: 34.65098333358765\n",
      "  timers:\n",
      "    learn_throughput: 2200.525\n",
      "    learn_time_ms: 14.542\n",
      "  timestamp: 1629501212\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 5000\n",
      "  training_iteration: 5\n",
      "  trial_id: 24a63_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/8 CPUs, 0/0 GPUs, 0.0/1.83 GiB heap, 0.0/0.92 GiB objects (0.0/1.0 CPU_group_0_6fea1609fbac878d7acbf150cdbfd8a2, 0.0/1.0 CPU_group_6fea1609fbac878d7acbf150cdbfd8a2)<br>Result logdir: /home/peter/ray_results/TA_TEST3<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_24a63_00000</td><td>RUNNING </td><td>192.168.1.21:139440</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          34.651</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">   481.9</td><td style=\"text-align: right;\">                 531</td><td style=\"text-align: right;\">                 433</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.1/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/1.83 GiB heap, 0.0/0.92 GiB objects (0.0/1.0 CPU_group_0_6fea1609fbac878d7acbf150cdbfd8a2, 0.0/1.0 CPU_group_6fea1609fbac878d7acbf150cdbfd8a2)<br>Result logdir: /home/peter/ray_results/TA_TEST3<br>Number of trials: 1/1 (1 TERMINATED)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                          </th><th>status    </th><th>loc  </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>DQN_two_agent_t4t_MG_env_24a63_00000</td><td>TERMINATED</td><td>     </td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">          34.651</td><td style=\"text-align: right;\">5000</td><td style=\"text-align: right;\">   481.9</td><td style=\"text-align: right;\">                 531</td><td style=\"text-align: right;\">                 433</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-21 11:13:32,529\tINFO tune.py:550 -- Total run time: 40.66 seconds (40.37 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# pretraining against tit-for-tat\n",
    "\n",
    "exp_name = 'TA_TEST3'\n",
    "exp_dict = {\n",
    "        'name': exp_name,\n",
    "        'run_or_experiment': 'DQN',\n",
    "        \"stop\": {\n",
    "            \"training_iteration\": 5\n",
    "        },\n",
    "        'checkpoint_freq': 0,\n",
    "        'checkpoint_at_end': True,\n",
    "        \"config\": config,\n",
    "}\n",
    "# ray.init()\n",
    "MA_result=tune.run(**exp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "26838861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_workers': 0,\n",
       " 'num_envs_per_worker': 1,\n",
       " 'create_env_on_driver': False,\n",
       " 'rollout_fragment_length': 4,\n",
       " 'batch_mode': 'truncate_episodes',\n",
       " 'gamma': 0.99,\n",
       " 'lr': 0.0005,\n",
       " 'train_batch_size': 32,\n",
       " 'model': {'_use_default_native_models': False,\n",
       "  'fcnet_hiddens': [256, 32, 8],\n",
       "  'fcnet_activation': 'tanh',\n",
       "  'conv_filters': None,\n",
       "  'conv_activation': 'relu',\n",
       "  'post_fcnet_hiddens': [],\n",
       "  'post_fcnet_activation': 'relu',\n",
       "  'free_log_std': False,\n",
       "  'no_final_linear': False,\n",
       "  'vf_share_layers': True,\n",
       "  'use_lstm': False,\n",
       "  'max_seq_len': 20,\n",
       "  'lstm_cell_size': 256,\n",
       "  'lstm_use_prev_action': False,\n",
       "  'lstm_use_prev_reward': False,\n",
       "  '_time_major': False,\n",
       "  'use_attention': False,\n",
       "  'attention_num_transformer_units': 1,\n",
       "  'attention_dim': 64,\n",
       "  'attention_num_heads': 1,\n",
       "  'attention_head_dim': 32,\n",
       "  'attention_memory_inference': 50,\n",
       "  'attention_memory_training': 50,\n",
       "  'attention_position_wise_mlp_dim': 32,\n",
       "  'attention_init_gru_gate_bias': 2.0,\n",
       "  'attention_use_n_prev_actions': 0,\n",
       "  'attention_use_n_prev_rewards': 0,\n",
       "  'num_framestacks': 'auto',\n",
       "  'dim': 84,\n",
       "  'grayscale': False,\n",
       "  'zero_mean': True,\n",
       "  'custom_model': None,\n",
       "  'custom_model_config': {},\n",
       "  'custom_action_dist': None,\n",
       "  'custom_preprocessor': None,\n",
       "  'lstm_use_prev_action_reward': -1,\n",
       "  'framestack': True},\n",
       " 'optimizer': {},\n",
       " 'horizon': None,\n",
       " 'soft_horizon': False,\n",
       " 'no_done_at_end': False,\n",
       " 'env': None,\n",
       " 'observation_space': None,\n",
       " 'action_space': None,\n",
       " 'env_config': {},\n",
       " 'env_task_fn': None,\n",
       " 'render_env': False,\n",
       " 'record_env': False,\n",
       " 'clip_rewards': None,\n",
       " 'normalize_actions': True,\n",
       " 'clip_actions': False,\n",
       " 'preprocessor_pref': 'deepmind',\n",
       " 'log_level': 'WARN',\n",
       " 'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       " 'ignore_worker_failures': False,\n",
       " 'log_sys_usage': True,\n",
       " 'fake_sampler': False,\n",
       " 'framework': 'torch',\n",
       " 'eager_tracing': False,\n",
       " 'explore': True,\n",
       " 'exploration_config': {'type': 'EpsilonGreedy',\n",
       "  'initial_epsilon': 1.0,\n",
       "  'final_epsilon': 0.02,\n",
       "  'epsilon_timesteps': 10000},\n",
       " 'evaluation_interval': None,\n",
       " 'evaluation_num_episodes': 10,\n",
       " 'evaluation_parallel_to_training': False,\n",
       " 'in_evaluation': False,\n",
       " 'evaluation_config': {'explore': False},\n",
       " 'evaluation_num_workers': 0,\n",
       " 'custom_eval_function': None,\n",
       " 'sample_async': False,\n",
       " 'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       " 'observation_filter': 'NoFilter',\n",
       " 'synchronize_filters': True,\n",
       " 'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "  'inter_op_parallelism_threads': 2,\n",
       "  'gpu_options': {'allow_growth': True},\n",
       "  'log_device_placement': False,\n",
       "  'device_count': {'CPU': 1},\n",
       "  'allow_soft_placement': True},\n",
       " 'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "  'inter_op_parallelism_threads': 8},\n",
       " 'compress_observations': False,\n",
       " 'collect_metrics_timeout': 180,\n",
       " 'metrics_smoothing_episodes': 100,\n",
       " 'remote_worker_envs': False,\n",
       " 'remote_env_batch_wait_ms': 0,\n",
       " 'min_iter_time_s': 1,\n",
       " 'timesteps_per_iteration': 1000,\n",
       " 'seed': None,\n",
       " 'extra_python_environs_for_driver': {},\n",
       " 'extra_python_environs_for_worker': {},\n",
       " 'num_gpus': 0,\n",
       " '_fake_gpus': False,\n",
       " 'num_cpus_per_worker': 1,\n",
       " 'num_gpus_per_worker': 0,\n",
       " 'custom_resources_per_worker': {},\n",
       " 'num_cpus_for_driver': 1,\n",
       " 'placement_strategy': 'PACK',\n",
       " 'input': 'sampler',\n",
       " 'input_config': {},\n",
       " 'actions_in_input_normalized': False,\n",
       " 'input_evaluation': ['is', 'wis'],\n",
       " 'postprocess_inputs': False,\n",
       " 'shuffle_buffer_size': 0,\n",
       " 'output': None,\n",
       " 'output_compress_columns': ['obs', 'new_obs'],\n",
       " 'output_max_file_size': 67108864,\n",
       " 'multiagent': {'policies': {},\n",
       "  'policy_mapping_fn': None,\n",
       "  'policies_to_train': None,\n",
       "  'observation_fn': None,\n",
       "  'replay_mode': 'independent',\n",
       "  'count_steps_by': 'env_steps'},\n",
       " 'logger_config': None,\n",
       " 'simple_optimizer': -1,\n",
       " 'monitor': -1,\n",
       " 'num_atoms': 1,\n",
       " 'v_min': -10.0,\n",
       " 'v_max': 10.0,\n",
       " 'noisy': True,\n",
       " 'sigma0': 0.5,\n",
       " 'dueling': True,\n",
       " 'hiddens': [256],\n",
       " 'double_q': True,\n",
       " 'n_step': 3,\n",
       " 'target_network_update_freq': 500,\n",
       " 'buffer_size': 50000,\n",
       " 'replay_sequence_length': 1,\n",
       " 'prioritized_replay': True,\n",
       " 'prioritized_replay_alpha': 0.6,\n",
       " 'prioritized_replay_beta': 0.4,\n",
       " 'final_prioritized_replay_beta': 0.4,\n",
       " 'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       " 'prioritized_replay_eps': 1e-06,\n",
       " 'before_learn_on_batch': None,\n",
       " 'training_intensity': None,\n",
       " 'lr_schedule': None,\n",
       " 'adam_epsilon': 1e-08,\n",
       " 'grad_clip': 40,\n",
       " 'learning_starts': 1000,\n",
       " 'worker_side_prioritization': False}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer_config_dqn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1a73490",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-21 11:14:57,672\tWARNING util.py:55 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "agent0 = DQNTrainer(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aeec7dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "p0=MA_result.get_best_trial(metric=\"episode_reward_mean\", mode=\"max\").config['multiagent']['policies']['agent-0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "09aeeb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = MA_result.get_best_trial(metric=\"episode_reward_mean\", mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b6430cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'policies': {'agent-0': (ray.rllib.policy.policy_template.DQNTorchPolicy,\n",
       "   Box(0.0, 1.0, (400,), float32),\n",
       "   Discrete(2),\n",
       "   {'num_workers': 0,\n",
       "    'num_envs_per_worker': 1,\n",
       "    'create_env_on_driver': False,\n",
       "    'rollout_fragment_length': 4,\n",
       "    'batch_mode': 'truncate_episodes',\n",
       "    'gamma': 0.99,\n",
       "    'lr': 0.0005,\n",
       "    'train_batch_size': 32,\n",
       "    'model': {'_use_default_native_models': False,\n",
       "     'fcnet_hiddens': [256, 32, 8],\n",
       "     'fcnet_activation': 'tanh',\n",
       "     'conv_filters': None,\n",
       "     'conv_activation': 'relu',\n",
       "     'post_fcnet_hiddens': [],\n",
       "     'post_fcnet_activation': 'relu',\n",
       "     'free_log_std': False,\n",
       "     'no_final_linear': False,\n",
       "     'vf_share_layers': True,\n",
       "     'use_lstm': False,\n",
       "     'max_seq_len': 20,\n",
       "     'lstm_cell_size': 256,\n",
       "     'lstm_use_prev_action': False,\n",
       "     'lstm_use_prev_reward': False,\n",
       "     '_time_major': False,\n",
       "     'use_attention': False,\n",
       "     'attention_num_transformer_units': 1,\n",
       "     'attention_dim': 64,\n",
       "     'attention_num_heads': 1,\n",
       "     'attention_head_dim': 32,\n",
       "     'attention_memory_inference': 50,\n",
       "     'attention_memory_training': 50,\n",
       "     'attention_position_wise_mlp_dim': 32,\n",
       "     'attention_init_gru_gate_bias': 2.0,\n",
       "     'attention_use_n_prev_actions': 0,\n",
       "     'attention_use_n_prev_rewards': 0,\n",
       "     'num_framestacks': 'auto',\n",
       "     'dim': 84,\n",
       "     'grayscale': False,\n",
       "     'zero_mean': True,\n",
       "     'custom_model': None,\n",
       "     'custom_model_config': {},\n",
       "     'custom_action_dist': None,\n",
       "     'custom_preprocessor': None,\n",
       "     'lstm_use_prev_action_reward': -1,\n",
       "     'framestack': True},\n",
       "    'optimizer': {},\n",
       "    'horizon': None,\n",
       "    'soft_horizon': False,\n",
       "    'no_done_at_end': False,\n",
       "    'env': 'MG_t4t_env',\n",
       "    'observation_space': None,\n",
       "    'action_space': None,\n",
       "    'env_config': {},\n",
       "    'env_task_fn': None,\n",
       "    'render_env': False,\n",
       "    'record_env': False,\n",
       "    'clip_rewards': None,\n",
       "    'normalize_actions': True,\n",
       "    'clip_actions': False,\n",
       "    'preprocessor_pref': 'deepmind',\n",
       "    'log_level': 'WARN',\n",
       "    'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "    'ignore_worker_failures': False,\n",
       "    'log_sys_usage': True,\n",
       "    'fake_sampler': False,\n",
       "    'framework': 'torch',\n",
       "    'eager_tracing': False,\n",
       "    'explore': True,\n",
       "    'exploration_config': {'type': 'EpsilonGreedy',\n",
       "     'initial_epsilon': 1.0,\n",
       "     'final_epsilon': 0.02,\n",
       "     'epsilon_timesteps': 10000},\n",
       "    'evaluation_interval': None,\n",
       "    'evaluation_num_episodes': 10,\n",
       "    'evaluation_parallel_to_training': False,\n",
       "    'in_evaluation': False,\n",
       "    'evaluation_config': {'explore': False},\n",
       "    'evaluation_num_workers': 0,\n",
       "    'custom_eval_function': None,\n",
       "    'sample_async': False,\n",
       "    'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "    'observation_filter': 'NoFilter',\n",
       "    'synchronize_filters': True,\n",
       "    'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "     'inter_op_parallelism_threads': 2,\n",
       "     'gpu_options': {'allow_growth': True},\n",
       "     'log_device_placement': False,\n",
       "     'device_count': {'CPU': 1},\n",
       "     'allow_soft_placement': True},\n",
       "    'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "     'inter_op_parallelism_threads': 8},\n",
       "    'compress_observations': False,\n",
       "    'collect_metrics_timeout': 180,\n",
       "    'metrics_smoothing_episodes': 100,\n",
       "    'remote_worker_envs': False,\n",
       "    'remote_env_batch_wait_ms': 0,\n",
       "    'min_iter_time_s': 1,\n",
       "    'timesteps_per_iteration': 1000,\n",
       "    'seed': None,\n",
       "    'extra_python_environs_for_driver': {},\n",
       "    'extra_python_environs_for_worker': {},\n",
       "    'num_gpus': 0,\n",
       "    '_fake_gpus': False,\n",
       "    'num_cpus_per_worker': 1,\n",
       "    'num_gpus_per_worker': 0,\n",
       "    'custom_resources_per_worker': {},\n",
       "    'num_cpus_for_driver': 1,\n",
       "    'placement_strategy': 'PACK',\n",
       "    'input': 'sampler',\n",
       "    'input_config': {},\n",
       "    'actions_in_input_normalized': False,\n",
       "    'input_evaluation': ['is', 'wis'],\n",
       "    'postprocess_inputs': False,\n",
       "    'shuffle_buffer_size': 0,\n",
       "    'output': None,\n",
       "    'output_compress_columns': ['obs', 'new_obs'],\n",
       "    'output_max_file_size': 67108864,\n",
       "    'multiagent': {'policies': {},\n",
       "     'policy_mapping_fn': None,\n",
       "     'policies_to_train': None,\n",
       "     'observation_fn': None,\n",
       "     'replay_mode': 'independent',\n",
       "     'count_steps_by': 'env_steps'},\n",
       "    'logger_config': None,\n",
       "    'simple_optimizer': -1,\n",
       "    'monitor': -1,\n",
       "    'num_atoms': 1,\n",
       "    'v_min': -10.0,\n",
       "    'v_max': 10.0,\n",
       "    'noisy': True,\n",
       "    'sigma0': 0.5,\n",
       "    'dueling': True,\n",
       "    'hiddens': [256],\n",
       "    'double_q': True,\n",
       "    'n_step': 3,\n",
       "    'target_network_update_freq': 500,\n",
       "    'buffer_size': 50000,\n",
       "    'replay_sequence_length': 1,\n",
       "    'prioritized_replay': True,\n",
       "    'prioritized_replay_alpha': 0.6,\n",
       "    'prioritized_replay_beta': 0.4,\n",
       "    'final_prioritized_replay_beta': 0.4,\n",
       "    'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       "    'prioritized_replay_eps': 1e-06,\n",
       "    'before_learn_on_batch': None,\n",
       "    'training_intensity': None,\n",
       "    'lr_schedule': None,\n",
       "    'adam_epsilon': 1e-08,\n",
       "    'grad_clip': 40,\n",
       "    'learning_starts': 1000,\n",
       "    'worker_side_prioritization': False}),\n",
       "  'agent-1': (ray.rllib.policy.policy_template.DQNTorchPolicy,\n",
       "   Box(0.0, 1.0, (400,), float32),\n",
       "   Discrete(2),\n",
       "   {'num_workers': 0,\n",
       "    'num_envs_per_worker': 1,\n",
       "    'create_env_on_driver': False,\n",
       "    'rollout_fragment_length': 4,\n",
       "    'batch_mode': 'truncate_episodes',\n",
       "    'gamma': 0.99,\n",
       "    'lr': 0.0005,\n",
       "    'train_batch_size': 32,\n",
       "    'model': {'_use_default_native_models': False,\n",
       "     'fcnet_hiddens': [256, 32, 8],\n",
       "     'fcnet_activation': 'tanh',\n",
       "     'conv_filters': None,\n",
       "     'conv_activation': 'relu',\n",
       "     'post_fcnet_hiddens': [],\n",
       "     'post_fcnet_activation': 'relu',\n",
       "     'free_log_std': False,\n",
       "     'no_final_linear': False,\n",
       "     'vf_share_layers': True,\n",
       "     'use_lstm': False,\n",
       "     'max_seq_len': 20,\n",
       "     'lstm_cell_size': 256,\n",
       "     'lstm_use_prev_action': False,\n",
       "     'lstm_use_prev_reward': False,\n",
       "     '_time_major': False,\n",
       "     'use_attention': False,\n",
       "     'attention_num_transformer_units': 1,\n",
       "     'attention_dim': 64,\n",
       "     'attention_num_heads': 1,\n",
       "     'attention_head_dim': 32,\n",
       "     'attention_memory_inference': 50,\n",
       "     'attention_memory_training': 50,\n",
       "     'attention_position_wise_mlp_dim': 32,\n",
       "     'attention_init_gru_gate_bias': 2.0,\n",
       "     'attention_use_n_prev_actions': 0,\n",
       "     'attention_use_n_prev_rewards': 0,\n",
       "     'num_framestacks': 'auto',\n",
       "     'dim': 84,\n",
       "     'grayscale': False,\n",
       "     'zero_mean': True,\n",
       "     'custom_model': None,\n",
       "     'custom_model_config': {},\n",
       "     'custom_action_dist': None,\n",
       "     'custom_preprocessor': None,\n",
       "     'lstm_use_prev_action_reward': -1,\n",
       "     'framestack': True},\n",
       "    'optimizer': {},\n",
       "    'horizon': None,\n",
       "    'soft_horizon': False,\n",
       "    'no_done_at_end': False,\n",
       "    'env': None,\n",
       "    'observation_space': None,\n",
       "    'action_space': None,\n",
       "    'env_config': {},\n",
       "    'env_task_fn': None,\n",
       "    'render_env': False,\n",
       "    'record_env': False,\n",
       "    'clip_rewards': None,\n",
       "    'normalize_actions': True,\n",
       "    'clip_actions': False,\n",
       "    'preprocessor_pref': 'deepmind',\n",
       "    'log_level': 'WARN',\n",
       "    'callbacks': ray.rllib.agents.callbacks.DefaultCallbacks,\n",
       "    'ignore_worker_failures': False,\n",
       "    'log_sys_usage': True,\n",
       "    'fake_sampler': False,\n",
       "    'framework': 'torch',\n",
       "    'eager_tracing': False,\n",
       "    'explore': True,\n",
       "    'exploration_config': {'type': 'EpsilonGreedy',\n",
       "     'initial_epsilon': 1.0,\n",
       "     'final_epsilon': 0.02,\n",
       "     'epsilon_timesteps': 10000},\n",
       "    'evaluation_interval': None,\n",
       "    'evaluation_num_episodes': 10,\n",
       "    'evaluation_parallel_to_training': False,\n",
       "    'in_evaluation': False,\n",
       "    'evaluation_config': {'explore': False},\n",
       "    'evaluation_num_workers': 0,\n",
       "    'custom_eval_function': None,\n",
       "    'sample_async': False,\n",
       "    'sample_collector': ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector,\n",
       "    'observation_filter': 'NoFilter',\n",
       "    'synchronize_filters': True,\n",
       "    'tf_session_args': {'intra_op_parallelism_threads': 2,\n",
       "     'inter_op_parallelism_threads': 2,\n",
       "     'gpu_options': {'allow_growth': True},\n",
       "     'log_device_placement': False,\n",
       "     'device_count': {'CPU': 1},\n",
       "     'allow_soft_placement': True},\n",
       "    'local_tf_session_args': {'intra_op_parallelism_threads': 8,\n",
       "     'inter_op_parallelism_threads': 8},\n",
       "    'compress_observations': False,\n",
       "    'collect_metrics_timeout': 180,\n",
       "    'metrics_smoothing_episodes': 100,\n",
       "    'remote_worker_envs': False,\n",
       "    'remote_env_batch_wait_ms': 0,\n",
       "    'min_iter_time_s': 1,\n",
       "    'timesteps_per_iteration': 1000,\n",
       "    'seed': None,\n",
       "    'extra_python_environs_for_driver': {},\n",
       "    'extra_python_environs_for_worker': {},\n",
       "    'num_gpus': 0,\n",
       "    '_fake_gpus': False,\n",
       "    'num_cpus_per_worker': 1,\n",
       "    'num_gpus_per_worker': 0,\n",
       "    'custom_resources_per_worker': {},\n",
       "    'num_cpus_for_driver': 1,\n",
       "    'placement_strategy': 'PACK',\n",
       "    'input': 'sampler',\n",
       "    'input_config': {},\n",
       "    'actions_in_input_normalized': False,\n",
       "    'input_evaluation': ['is', 'wis'],\n",
       "    'postprocess_inputs': False,\n",
       "    'shuffle_buffer_size': 0,\n",
       "    'output': None,\n",
       "    'output_compress_columns': ['obs', 'new_obs'],\n",
       "    'output_max_file_size': 67108864,\n",
       "    'multiagent': {'policies': {},\n",
       "     'policy_mapping_fn': None,\n",
       "     'policies_to_train': None,\n",
       "     'observation_fn': None,\n",
       "     'replay_mode': 'independent',\n",
       "     'count_steps_by': 'env_steps'},\n",
       "    'logger_config': None,\n",
       "    'simple_optimizer': -1,\n",
       "    'monitor': -1,\n",
       "    'num_atoms': 1,\n",
       "    'v_min': -10.0,\n",
       "    'v_max': 10.0,\n",
       "    'noisy': True,\n",
       "    'sigma0': 0.5,\n",
       "    'dueling': True,\n",
       "    'hiddens': [256],\n",
       "    'double_q': True,\n",
       "    'n_step': 3,\n",
       "    'target_network_update_freq': 500,\n",
       "    'buffer_size': 50000,\n",
       "    'replay_sequence_length': 1,\n",
       "    'prioritized_replay': True,\n",
       "    'prioritized_replay_alpha': 0.6,\n",
       "    'prioritized_replay_beta': 0.4,\n",
       "    'final_prioritized_replay_beta': 0.4,\n",
       "    'prioritized_replay_beta_annealing_timesteps': 20000,\n",
       "    'prioritized_replay_eps': 1e-06,\n",
       "    'before_learn_on_batch': None,\n",
       "    'training_intensity': None,\n",
       "    'lr_schedule': None,\n",
       "    'adam_epsilon': 1e-08,\n",
       "    'grad_clip': 40,\n",
       "    'learning_starts': 1000,\n",
       "    'worker_side_prioritization': False})},\n",
       " 'policy_mapping_fn': <function __main__.policy_mapping_fn(agent_id)>}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1.config['multiagent']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f32549b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc = MA_result.get_last_checkpoint(metric=\"episode_reward_mean\", mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbd4b79c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/peter/ray_results/TA_TEST3/DQN_two_agent_t4t_MG_env_24a63_00000_0_2021-08-21_11-12-52/checkpoint_000005/checkpoint-5'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fedd6eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-21 11:15:13,870\tINFO trainable.py:382 -- Restored on 192.168.1.21 from checkpoint: /home/peter/ray_results/TA_TEST3/DQN_two_agent_t4t_MG_env_24a63_00000_0_2021-08-21_11-12-52/checkpoint_000005/checkpoint-5\n",
      "2021-08-21 11:15:13,871\tINFO trainable.py:390 -- Current state after restoring: {'_iteration': 5, '_timesteps_total': None, '_time_total': 34.65098333358765, '_episodes_total': 50}\n"
     ]
    }
   ],
   "source": [
    "agent0.restore(lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c72d0099",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent0.compute_action(np.random.randn(400), policy_id='agent-0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5531c325",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
