{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gym\n",
    "\n",
    "from gym import spaces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/.local/lib/python3.8/site-packages/ray/autoscaler/_private/cli_logger.py:57: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  warnings.warn(\n",
      "2021-08-10 14:47:36,342\tINFO services.py:1272 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8266\u001b[39m\u001b[22m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'node_ip_address': '192.168.1.21',\n",
       " 'raylet_ip_address': '192.168.1.21',\n",
       " 'redis_address': '192.168.1.21:64875',\n",
       " 'object_store_address': '/tmp/ray/session_2021-08-10_14-47-34_494692_140440/sockets/plasma_store',\n",
       " 'raylet_socket_name': '/tmp/ray/session_2021-08-10_14-47-34_494692_140440/sockets/raylet',\n",
       " 'webui_url': '127.0.0.1:8266',\n",
       " 'session_dir': '/tmp/ray/session_2021-08-10_14-47-34_494692_140440',\n",
       " 'metrics_export_port': 59734,\n",
       " 'node_id': '872ee5db35d6e39ca1fd3849a77853082d898391200b061fe1ac3a15'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ray\n",
    "from ray import tune\n",
    "from ray.rllib.examples.env.multi_agent import MultiAgentCartPole\n",
    "from ray.rllib.examples.models.shared_weights_model import \\\n",
    "    SharedWeightsModel1, SharedWeightsModel2, TF2SharedWeightsModel, \\\n",
    "    TorchSharedWeightsModel\n",
    "from ray.rllib.models import ModelCatalog\n",
    "# from ray.rllib.policy import PolicySpec\n",
    "from ray.rllib.utils.framework import try_import_tf\n",
    "from ray.rllib.utils.test_utils import check_learning_achieved\n",
    "from ray.tune.registry import register_env\n",
    "\n",
    "from ray.rllib.env.multi_agent_env import MultiAgentEnv\n",
    "\n",
    "\n",
    "import ray\n",
    "from ray.rllib.agents.ppo import PPOTrainer, PPOTorchPolicy\n",
    "from ray.rllib.agents.ppo import DEFAULT_CONFIG as DEFAULT_CONFIG_PPO\n",
    "\n",
    "from ray.rllib.agents.dqn import DQNTrainer, DQNTorchPolicy\n",
    "from ray.rllib.agents.dqn import  DEFAULT_CONFIG as DEFAULT_CONFIG_DQN\n",
    "\n",
    "\n",
    "from ray.tune.registry import register_env\n",
    "from ray.tune.logger import pretty_print\n",
    "\n",
    "ray.init(ignore_reinit_error=True, log_to_driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixGame():\n",
    "    \n",
    "    def __init__(self, RPST=(3,1,0,5)):\n",
    "        \n",
    "        self.RPST = RPST\n",
    "        \n",
    "        self.payoff_mat = np.empty((2,2), dtype=np.object)\n",
    "        \n",
    "        self.payoff_mat[0, 0] = (RPST[0], RPST[0])\n",
    "        self.payoff_mat[1, 1] = (RPST[1], RPST[1])\n",
    "        self.payoff_mat[0, 1] = (RPST[2], RPST[3])\n",
    "        self.payoff_mat[1, 0] = (RPST[3], RPST[2])\n",
    "        \n",
    "    def play(self, a_row, a_col):\n",
    "        # for ease of things 0 is coooperate\n",
    "#                            1 is defect\n",
    "        \n",
    "        \n",
    "#         if a_row == 'c':\n",
    "#             row = 0\n",
    "#         else:\n",
    "#             row = 1\n",
    "            \n",
    "#         if a_col == 'c':\n",
    "#             col = 0\n",
    "#         else:\n",
    "#             col = 1\n",
    "            \n",
    "        return self.payoff_mat[a_row, a_col]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoAgentMatrixGameEnv(MultiAgentEnv):\n",
    "    \n",
    "    def __init__(self, RPST=(3,1,0,5), history_n=100):\n",
    "        \n",
    "        self.num_agents = 2\n",
    "        \n",
    "        self.RPST = RPST\n",
    "        self.history_n = history_n\n",
    "        self.history = np.zeros((2,2,self.history_n))\n",
    "        \n",
    "        self._counter = 0\n",
    "        self._setup_spaces()\n",
    "        self.game = MatrixGame(RPST=self.RPST)\n",
    "    \n",
    "    \n",
    "    def _setup_spaces(self):\n",
    "        \n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        \n",
    "        self.observation_space = spaces.Box(0, 1,\n",
    "                                           shape=(self.history_n * 4,))\n",
    "        \n",
    "        \n",
    "    def history_to_states(self, history=None):\n",
    "        \n",
    "        if history is None:\n",
    "            history = self.history\n",
    "            \n",
    "        state1 = history.flatten()\n",
    "        state2 = history[::-1,:,:].flatten()\n",
    "        \n",
    "        states = {0: state1, 1:state2}\n",
    "        \n",
    "        return states\n",
    "            \n",
    "        \n",
    "        \n",
    "    def step(self, action_dict):\n",
    "        \n",
    "        print((action_dict[0], action_dict[1]))\n",
    "        rewards = self.game.play(action_dict[0], action_dict[1])\n",
    "        rew = {i: rewards[i] for i in [0, 1]}\n",
    "        \n",
    "        self.history[0, action_dict[0], self._counter] = 1\n",
    "        self.history[1, action_dict[1], self._counter] = 1\n",
    "        \n",
    "        obs = self.history_to_states(self.history)\n",
    "        \n",
    "        self._counter += 1\n",
    "        \n",
    "        is_done = self._counter >= self.history_n\n",
    "        done = {i: is_done for i in [0, 1, \"__all__\"]}\n",
    "        \n",
    "        info = {0:{}, 1:{}}\n",
    "        \n",
    "        return obs, rew, done, info\n",
    "        \n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        \n",
    "        self.history = np.zeros((2,2,self.history_n))\n",
    "        obs = self.history_to_states(self.history)\n",
    "\n",
    "        self._counter = 0\n",
    "        \n",
    "        return obs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TwoAgentMatrixGameEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 4, 1: 3}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = (4,3)\n",
    "{i:aa[i] for i in [0,1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({0: array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       "  1: array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.])},\n",
       " {0: 3, 1: 3},\n",
       " {0: False, 1: False, '__all__': False},\n",
       " {0: {}, 1: {}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step({0:0,1:0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "info ={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[0] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "info[1] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {}, 1: {}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_env('twoagent_PD', lambda c: TwoAgentMatrixGameEnv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_config_ppo = DEFAULT_CONFIG_PPO.copy()\n",
    "trainer_config_ppo['num_workers'] = 1\n",
    "trainer_config_ppo['num_sgd_iter'] = 20\n",
    "trainer_config_ppo['sgd_minibatch_size'] = 32\n",
    "# trainer_config_ppo['model']['fcnet_hiddens'] = [1024, 512,512, 256,256,32,8]\n",
    "trainer_config_ppo['model']['fcnet_hiddens'] = [256,256,32,8]\n",
    "\n",
    "trainer_config_ppo['num_cpus_per_worker'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 14:44:16,235\tINFO trainer.py:671 -- Tip: set framework=tfe or the --eager flag to enable TensorFlow eager execution\n",
      "2021-08-10 14:44:16,237\tINFO trainer.py:696 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-8099c2b1a098>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPOTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer_config_ppo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"twoagent_PD\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, env, logger_creator)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0mTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         def _init(self, config: TrainerConfigDict,\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, env, logger_creator)\u001b[0m\n\u001b[1;32m    546\u001b[0m             \u001b[0mlogger_creator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_logger_creator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/tune/trainable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, config, logger_creator)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0msetup_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msetup_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mSETUP_TIME_THRESHOLD\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, config)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mget_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 709\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    710\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[0;31m# Evaluation setup.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer_template.py\u001b[0m in \u001b[0;36m_init\u001b[0;34m(self, config, env_creator)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m             \u001b[0;31m# Creating all workers (excluding evaluation workers).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             self.workers = self._make_workers(\n\u001b[0m\u001b[1;32m    151\u001b[0m                 \u001b[0menv_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 \u001b[0mvalidate_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/agents/trainer.py\u001b[0m in \u001b[0;36m_make_workers\u001b[0;34m(self, env_creator, validate_env, policy_class, config, num_workers)\u001b[0m\n\u001b[1;32m    789\u001b[0m             \u001b[0mWorkerSet\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mcreated\u001b[0m \u001b[0mWorkerSet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \"\"\"\n\u001b[0;32m--> 791\u001b[0;31m         return WorkerSet(\n\u001b[0m\u001b[1;32m    792\u001b[0m             \u001b[0menv_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m             \u001b[0mvalidate_env\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/rllib/evaluation/worker_set.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env_creator, validate_env, policy_class, trainer_config, num_workers, logdir, _setup)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;31m# to not be forced to create an Env on the local worker.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remote_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 remote_spaces = ray.get(self.remote_workers(\n\u001b[0m\u001b[1;32m     82\u001b[0m                 \u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeach_policy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                     lambda p, pid: (pid, p.observation_space, p.action_space)))\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclient_mode_should_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m         \u001b[0;31m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1488\u001b[0;31m         values, debugger_breakpoint = worker.get_objects(\n\u001b[0m\u001b[1;32m   1489\u001b[0m             object_refs, timeout=timeout)\n\u001b[1;32m   1490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0mtimeout_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         data_metadata_pairs = self.core_worker.get_objects(\n\u001b[0m\u001b[1;32m    338\u001b[0m             object_refs, self.current_task_id, timeout_ms)\n\u001b[1;32m    339\u001b[0m         \u001b[0mdebugger_breakpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = PPOTrainer(trainer_config_ppo, env=\"twoagent_PD\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2):\n",
    "    print(\"Training iteration {}...\".format(i))\n",
    "    result=trainer.train()\n",
    "    print(pretty_print(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TwoAgentMatrixGameEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-93e06fe5a326>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtotal_defect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "n_samples = 50\n",
    "\n",
    "\n",
    "defects = []\n",
    "rewards = []\n",
    "for i in range(n_samples):\n",
    "    state = env.reset()\n",
    "    \n",
    "    total_defect = 0\n",
    "    cum_reward = 0\n",
    "    done = False\n",
    "    i = 0\n",
    "    while not done:\n",
    "        print('===========')\n",
    "        print(i)\n",
    "        print('===========')\n",
    "        \n",
    "        i+=1\n",
    "        action = trainer.compute_actions(state)\n",
    "        print(action)\n",
    "#         total_defect += action\n",
    "        state, reward, done, results = env.step(action)\n",
    "#         cum_reward += reward\n",
    "    defects.append(total_defect)\n",
    "    rewards.append(cum_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def env_creator(_):\n",
    "    return TwoAgentMatrixGameEnv()\n",
    "single_env = TwoAgentMatrixGameEnv()\n",
    "env_name = \"TwoAgent_PD\"\n",
    "register_env(env_name, env_creator)\n",
    "\n",
    "\n",
    "obs_space = single_env.observation_space\n",
    "act_space = single_env.action_space\n",
    "num_agents = single_env.num_agents\n",
    "\n",
    "def gen_policy():\n",
    "    return (PPOTorchPolicy, obs_space, act_space, {})\n",
    "policy_graphs = {}\n",
    "\n",
    "for i in range(num_agents):\n",
    "    policy_graphs['agent-' + str(i)] = gen_policy()\n",
    "def policy_mapping_fn(agent_id):\n",
    "        return 'agent-' + str(agent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\n",
    "    \"log_level\": \"WARN\",\n",
    "    \"num_workers\": 3,\n",
    "    \"num_cpus_for_driver\": 1,\n",
    "    \"num_cpus_per_worker\": 1,\n",
    "    \"lr\": 5e-3,\n",
    "    \"model\":{\"fcnet_hiddens\": [1024, 512,256,32,8]},\n",
    "    \"multiagent\": {\n",
    "        \"policies\": policy_graphs,\n",
    "        \"policy_mapping_fn\": policy_mapping_fn,\n",
    "    },\n",
    "    \"env\": \"TwoAgent_PD\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 13.3/15.4 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/8 CPUs, 0/0 GPUs, 0.0/2.21 GiB heap, 0.0/1.1 GiB objects<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 PENDING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_57fc6_00000</td><td>PENDING </td><td>     </td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_57fc6_00000:\n",
      "  agent_timesteps_total: 15996\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-10_14-52-20\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 481.0\n",
      "  episode_reward_mean: 449.79487179487177\n",
      "  episode_reward_min: 417.0\n",
      "  episodes_this_iter: 78\n",
      "  episodes_total: 78\n",
      "  experiment_id: b8bdad5b0cf241c18137debbeebc8113\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1999999999999999\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.67753576570087\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.016009656919373408\n",
      "          policy_loss: -0.005544625104419769\n",
      "          total_loss: 6696.291372147817\n",
      "          vf_explained_var: -8.514949634275126e-09\n",
      "          vf_loss: 6696.293759300595\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1999999999999999\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.6906502123862978\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.002518519402580447\n",
      "          policy_loss: -0.0015185060097821176\n",
      "          total_loss: 7144.881208147322\n",
      "          vf_explained_var: -1.419158213167293e-08\n",
      "          vf_loss: 7144.882277715774\n",
      "    num_agent_steps_sampled: 15996\n",
      "    num_agent_steps_trained: 15996\n",
      "    num_steps_sampled: 7998\n",
      "    num_steps_trained: 7998\n",
      "  iterations_since_restore: 1\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 36.92553191489362\n",
      "    ram_util_percent: 92.8308510638298\n",
      "  pid: 140555\n",
      "  policy_reward_max:\n",
      "    agent-0: 259.0\n",
      "    agent-1: 273.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 223.6153846153846\n",
      "    agent-1: 226.17948717948718\n",
      "  policy_reward_min:\n",
      "    agent-0: 193.0\n",
      "    agent-1: 186.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.19430935405191965\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.21713469240340935\n",
      "    mean_inference_ms: 12.091691725523141\n",
      "    mean_raw_obs_processing_ms: 0.3245747278607081\n",
      "  time_since_restore: 263.04703664779663\n",
      "  time_this_iter_s: 263.04703664779663\n",
      "  time_total_s: 263.04703664779663\n",
      "  timers:\n",
      "    learn_throughput: 35.057\n",
      "    learn_time_ms: 228145.329\n",
      "    sample_throughput: 229.348\n",
      "    sample_time_ms: 34872.723\n",
      "    update_time_ms: 6.208\n",
      "  timestamp: 1628563940\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 7998\n",
      "  training_iteration: 1\n",
      "  trial_id: 57fc6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.3/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.21 GiB heap, 0.0/1.1 GiB objects<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_57fc6_00000</td><td>RUNNING </td><td>192.168.1.21:140555</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         263.047</td><td style=\"text-align: right;\">7998</td><td style=\"text-align: right;\"> 449.795</td><td style=\"text-align: right;\">                 481</td><td style=\"text-align: right;\">                 417</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 14:54:58,545\tWARNING tune.py:506 -- SIGINT received (e.g. via Ctrl+C), ending Ray Tune run. This will try to checkpoint the experiment state one last time. Press CTRL+C one more time (or send SIGINT/SIGKILL/SIGTERM) to skip. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result for PPO_TwoAgent_PD_57fc6_00000:\n",
      "  agent_timesteps_total: 31992\n",
      "  custom_metrics: {}\n",
      "  date: 2021-08-10_14-56-33\n",
      "  done: false\n",
      "  episode_len_mean: 100.0\n",
      "  episode_media: {}\n",
      "  episode_reward_max: 482.0\n",
      "  episode_reward_mean: 447.57\n",
      "  episode_reward_min: 422.0\n",
      "  episodes_this_iter: 81\n",
      "  episodes_total: 159\n",
      "  experiment_id: b8bdad5b0cf241c18137debbeebc8113\n",
      "  hostname: coolo-computer\n",
      "  info:\n",
      "    learner:\n",
      "      agent-0:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.1999999999999999\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.6699515495981488\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.00733576962988179\n",
      "          policy_loss: 0.0003143421684702237\n",
      "          total_loss: 7493.894639756944\n",
      "          vf_explained_var: -4.730527525254047e-09\n",
      "          vf_loss: 7493.892973400298\n",
      "      agent-1:\n",
      "        learner_stats:\n",
      "          allreduce_latency: 0.0\n",
      "          cur_kl_coeff: 0.09999999999999995\n",
      "          cur_lr: 0.005000000000000001\n",
      "          entropy: 0.6910140419763232\n",
      "          entropy_coeff: 0.0\n",
      "          kl: 0.009981220328925355\n",
      "          policy_loss: -7.914924728018897e-05\n",
      "          total_loss: 4041.3856569320437\n",
      "          vf_explained_var: -1.419158213167293e-08\n",
      "          vf_loss: 4041.3846881200398\n",
      "    num_agent_steps_sampled: 31992\n",
      "    num_agent_steps_trained: 31992\n",
      "    num_steps_sampled: 15996\n",
      "    num_steps_trained: 15996\n",
      "  iterations_since_restore: 2\n",
      "  node_ip: 192.168.1.21\n",
      "  num_healthy_workers: 3\n",
      "  off_policy_estimator: {}\n",
      "  perf:\n",
      "    cpu_util_percent: 37.638781163434906\n",
      "    ram_util_percent: 93.71634349030472\n",
      "  pid: 140555\n",
      "  policy_reward_max:\n",
      "    agent-0: 294.0\n",
      "    agent-1: 273.0\n",
      "  policy_reward_mean:\n",
      "    agent-0: 247.01\n",
      "    agent-1: 200.56\n",
      "  policy_reward_min:\n",
      "    agent-0: 193.0\n",
      "    agent-1: 157.0\n",
      "  sampler_perf:\n",
      "    mean_action_processing_ms: 0.16874280023187904\n",
      "    mean_env_render_ms: 0.0\n",
      "    mean_env_wait_ms: 0.18651305205339597\n",
      "    mean_inference_ms: 10.374601670697569\n",
      "    mean_raw_obs_processing_ms: 0.2782135470022035\n",
      "  time_since_restore: 516.1898214817047\n",
      "  time_this_iter_s: 253.14278483390808\n",
      "  time_total_s: 516.1898214817047\n",
      "  timers:\n",
      "    learn_throughput: 34.872\n",
      "    learn_time_ms: 229353.884\n",
      "    sample_throughput: 278.576\n",
      "    sample_time_ms: 28710.346\n",
      "    update_time_ms: 9.068\n",
      "  timestamp: 1628564193\n",
      "  timesteps_since_restore: 0\n",
      "  timesteps_total: 15996\n",
      "  training_iteration: 2\n",
      "  trial_id: 57fc6_00000\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.21 GiB heap, 0.0/1.1 GiB objects (0.0/4.0 CPU_group_1e2bb0c819a3b0cf079acdabc0d3a194, 0.0/1.0 CPU_group_0_1e2bb0c819a3b0cf079acdabc0d3a194, 0.0/1.0 CPU_group_1_1e2bb0c819a3b0cf079acdabc0d3a194, 0.0/1.0 CPU_group_3_1e2bb0c819a3b0cf079acdabc0d3a194, 0.0/1.0 CPU_group_2_1e2bb0c819a3b0cf079acdabc0d3a194)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_57fc6_00000</td><td>RUNNING </td><td>192.168.1.21:140555</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">          516.19</td><td style=\"text-align: right;\">15996</td><td style=\"text-align: right;\">  447.57</td><td style=\"text-align: right;\">                 482</td><td style=\"text-align: right;\">                 422</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Memory usage on this node: 14.4/15.4 GiB: ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.<br>Using FIFO scheduling algorithm.<br>Resources requested: 4.0/8 CPUs, 0/0 GPUs, 0.0/2.21 GiB heap, 0.0/1.1 GiB objects (0.0/4.0 CPU_group_1e2bb0c819a3b0cf079acdabc0d3a194, 0.0/1.0 CPU_group_0_1e2bb0c819a3b0cf079acdabc0d3a194, 0.0/1.0 CPU_group_1_1e2bb0c819a3b0cf079acdabc0d3a194, 0.0/1.0 CPU_group_3_1e2bb0c819a3b0cf079acdabc0d3a194, 0.0/1.0 CPU_group_2_1e2bb0c819a3b0cf079acdabc0d3a194)<br>Result logdir: /home/peter/ray_results/TA_TEST1<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name                 </th><th>status  </th><th>loc                </th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">   ts</th><th style=\"text-align: right;\">  reward</th><th style=\"text-align: right;\">  episode_reward_max</th><th style=\"text-align: right;\">  episode_reward_min</th><th style=\"text-align: right;\">  episode_len_mean</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>PPO_TwoAgent_PD_57fc6_00000</td><td>RUNNING </td><td>192.168.1.21:140555</td><td style=\"text-align: right;\">     2</td><td style=\"text-align: right;\">          516.19</td><td style=\"text-align: right;\">15996</td><td style=\"text-align: right;\">  447.57</td><td style=\"text-align: right;\">                 482</td><td style=\"text-align: right;\">                 422</td><td style=\"text-align: right;\">               100</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-10 14:56:33,874\tERROR tune.py:545 -- Trials did not complete: [PPO_TwoAgent_PD_57fc6_00000]\n",
      "2021-08-10 14:56:33,875\tINFO tune.py:549 -- Total run time: 527.22 seconds (526.85 seconds for the tuning loop).\n",
      "2021-08-10 14:56:33,876\tWARNING tune.py:553 -- Experiment has been interrupted, but the most recent state was saved. You can continue running this experiment by passing `resume=True` to `tune.run()`\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ray.tune.analysis.experiment_analysis.ExperimentAnalysis at 0x7fdf84133e50>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = 'TA_TEST1'\n",
    "exp_dict = {\n",
    "        'name': exp_name,\n",
    "        'run_or_experiment': 'PPO',\n",
    "        \"stop\": {\n",
    "            \"training_iteration\": 20\n",
    "        },\n",
    "        'checkpoint_freq': 20,\n",
    "        \"config\": config,\n",
    "}\n",
    "# ray.init()\n",
    "tune.run(**exp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
